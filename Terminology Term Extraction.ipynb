{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a rule-based system to extract term candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os #readfile\n",
    "import textract #pdftotext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertpdfstotext(path):\n",
    "\n",
    "    filelist = []\n",
    "    \n",
    "    for fp in os.listdir(path):\n",
    "        allfiles = filelist.append(os.path.join(path,fp))\n",
    "        \n",
    "    for f in filelist:\n",
    "        doc = textract.process(f)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Neural Text Generation from Structured Data\\r\\nwith Application to the Biography Domain\\r\\nRe\\xcc\\x81mi Lebret\\xe2\\x88\\x97\\r\\nEPFL, Switzerland\\r\\n\\r\\nDavid Grangier\\r\\nFacebook AI Research\\r\\n\\r\\nAbstract\\r\\nThis paper introduces a neural model for\\r\\nconcept-to-text generation that scales to large,\\r\\nrich domains. It generates biographical sentences from fact tables on a new dataset of\\r\\nbiographies from Wikipedia. This set is an\\r\\norder of magnitude larger than existing resources with over 700k samples and a 400k\\r\\nvocabulary. Our model builds on conditional\\r\\nneural language models for text generation.\\r\\nTo deal with the large vocabulary, we extend these models to mix a fixed vocabulary\\r\\nwith copy actions that transfer sample-specific\\r\\nwords from the input database to the generated output sentence. To deal with structured\\r\\ndata, we allow the model to embed words\\r\\ndifferently depending on the data fields in\\r\\nwhich they occur. Our neural model significantly outperforms a Templated Kneser-Ney\\r\\nlanguage model by nearly 15 BLEU.\\r\\n\\r\\n1\\r\\n\\r\\nIntroduction\\r\\n\\r\\nConcept-to-text generation renders structured\\r\\nrecords into natural language (Reiter et al., 2000). A\\r\\ntypical application is to generate a weather forecast\\r\\nbased on a set of structured meteorological measurements. In contrast to previous work, we scale\\r\\nto the large and very diverse problem of generating\\r\\nbiographies based on Wikipedia infoboxes. An\\r\\ninfobox is a fact table describing a person, similar to\\r\\na person subgraph in a knowledge base (Bollacker\\r\\net al., 2008; Ferrucci, 2012). Similar generation\\r\\napplications include the generation of product\\r\\ndescriptions based on a catalog of millions of items\\r\\nwith dozens of attributes each.\\r\\nPrevious work experimented with datasets that\\r\\ncontain only a few tens of thousands of records such\\r\\nas W EATHERGOV or the ROBOCUP dataset, while\\r\\nour dataset contains over 700k biographies from\\r\\n\\xe2\\x88\\x97\\r\\n\\r\\nRe\\xcc\\x81mi performed this work while interning at Facebook.\\r\\n\\r\\nMichael Auli\\r\\nFacebook AI Research\\r\\n\\r\\nWikipedia. Furthermore, these datasets have a limited vocabulary of only about 350 words each, compared to over 400k words in our dataset.\\r\\nTo tackle this problem we introduce a statistical\\r\\ngeneration model conditioned on a Wikipedia infobox. We focus on the generation of the first sentence of a biography which requires the model to\\r\\nselect among a large number of possible fields to\\r\\ngenerate an adequate output. Such diversity makes\\r\\nit difficult for classical count-based models to estimate probabilities of rare events due to data sparsity.\\r\\nWe address this issue by parameterizing words and\\r\\nfields as embeddings, along with a neural language\\r\\nmodel operating on them (Bengio et al., 2003). This\\r\\nfactorization allows us to scale to a larger number of\\r\\nwords and fields than Liang et al. (2009), or Kim\\r\\nand Mooney (2010) where the number of parameters grows as the product of the number of words\\r\\nand fields.\\r\\nMoreover, our approach does not restrict the relations between the field contents and the generated text. This contrasts with less flexible strategies\\r\\nthat assume the generation to follow either a hybrid\\r\\nalignment tree (Kim and Mooney, 2010), a probabilistic context-free grammar (Konstas and Lapata,\\r\\n2013), or a tree adjoining grammar (Gyawali and\\r\\nGardent, 2014).\\r\\nOur model exploits structured data both globally\\r\\nand locally. Global conditioning summarizes all information about a personality to understand highlevel themes such as that the biography is about a\\r\\nscientist or an artist, while as local conditioning describes the previously generated tokens in terms of\\r\\nthe their relationship to the infobox. We analyze the\\r\\neffectiveness of each and demonstrate their complementarity.\\r\\n\\r\\n2\\r\\n\\r\\nRelated Work\\r\\n\\r\\nTraditionally, generation systems relied on rules and\\r\\nhand-crafted specifications (Dale et al., 2003; Reiter et al., 2005; Green, 2006; Galanis and Androut-\\r\\n\\r\\n1203\\r\\nProceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1203\\xe2\\x80\\x931213,\\r\\nAustin, Texas, November 1-5, 2016. c 2016 Association for Computational Linguistics\\r\\n\\r\\n\\x0csopoulos, 2007; Turner et al., 2010). Generation is\\r\\ndivided into modular, yet highly interdependent, decisions: (1) content planning defines which parts of\\r\\nthe input fields or meaning representations should\\r\\nbe selected; (2) sentence planning determines which\\r\\nselected fields are to be dealt with in each output\\r\\nsentence; and (3) surface realization generates those\\r\\nsentences.\\r\\nData-driven approaches have been proposed to\\r\\nautomatically learn the individual modules. One approach first aligns records and sentences and then\\r\\nlearns a content selection model (Duboue and McKeown, 2002; Barzilay and Lapata, 2005). Hierarchical hidden semi-Markov generative models have\\r\\nalso been used to first determine which facts to discuss and then to generate words from the predicates and arguments of the chosen facts (Liang et al.,\\r\\n2009). Sentence planning has been formulated as a\\r\\nsupervised set partitioning problem over facts where\\r\\neach partition corresponds to a sentence (Barzilay\\r\\nand Lapata, 2006). End-to-end approaches have\\r\\ncombined sentence planning and surface realization by using explicitly aligned sentence/meaning\\r\\npairs as training data (Ratnaparkhi, 2002; Wong and\\r\\nMooney, 2007; Belz, 2008; Lu and Ng, 2011). More\\r\\nrecently, content selection and surface realization\\r\\nhave been combined (Angeli et al., 2010; Kim and\\r\\nMooney, 2010; Konstas and Lapata, 2013).\\r\\nAt the intersection of rule-based and statistical methods, hybrid systems aim at leveraging human contributed rules and corpus statistics (Langkilde and Knight, 1998; Soricut and Marcu, 2006;\\r\\nMairesse and Walker, 2011).\\r\\nOur approach is inspired by the recent success of\\r\\nneural language models for image captioning (Kiros\\r\\net al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et\\r\\nal., 2015; Fang et al., 2015; Xu et al., 2015), machine translation (Devlin et al., 2014; Bahdanau et\\r\\nal., 2015; Luong et al., 2015), and modeling conversations and dialogues (Shang et al., 2015; Wen et al.,\\r\\n2015; Yao et al., 2015).\\r\\nOur model is most similar to Mei et al. (2016)\\r\\nwho use an encoder-decoder style neural network\\r\\nmodel to tackle the W EATHERGOV and ROBOCUP\\r\\ntasks. Their architecture relies on LSTM units and\\r\\nan attention mechanism which reduces scalability\\r\\ncompared to our simpler design.\\r\\n\\r\\n1204\\r\\n\\r\\nFigure 1: Wikipedia infobox of Frederick Parker-Rhodes. The\\r\\nintroduction of his article reads: \\xe2\\x80\\x9cFrederick Parker-Rhodes (21\\r\\nMarch 1914 \\xe2\\x80\\x93 21 November 1987) was an English linguist,\\r\\nplant pathologist, computer scientist, mathematician, mystic,\\r\\nand mycologist.\\xe2\\x80\\x9d.\\r\\n\\r\\n3\\r\\n\\r\\nLanguage Modeling for Constrained\\r\\nSentence generation\\r\\n\\r\\nConditional language models are a popular choice\\r\\nto generate sentences.\\r\\nWe introduce a tableconditioned language model for constraining text\\r\\ngeneration to include elements from fact tables.\\r\\n3.1\\r\\n\\r\\nLanguage model\\r\\n\\r\\nGiven a sentence s = w1 , . . . , wT with T words\\r\\nfrom vocabulary W, a language model estimates:\\r\\nP (s) =\\r\\n\\r\\nT\\r\\nY\\r\\nt=1\\r\\n\\r\\nP (wt |w1 , . . . , wt\\xe2\\x88\\x921 ) .\\r\\n\\r\\n(1)\\r\\n\\r\\nLet ct = wt\\xe2\\x88\\x92(n\\xe2\\x88\\x921) , . . . , wt\\xe2\\x88\\x921 be the sequence of\\r\\nn \\xe2\\x88\\x92 1 context words preceding wt . An n-gram language model makes an order n Markov assumption,\\r\\nP (s) \\xe2\\x89\\x88\\r\\n3.2\\r\\n\\r\\nT\\r\\nY\\r\\nt=1\\r\\n\\r\\nP (wt |ct ) .\\r\\n\\r\\n(2)\\r\\n\\r\\nLanguage model conditioned on tables\\r\\n\\r\\nA table is a set of field/value pairs, where values are\\r\\nsequences of words. We therefore propose language\\r\\nmodels that are conditioned on these pairs.\\r\\nLocal conditioning refers to the information\\r\\nfrom the table that is applied to the description of the\\r\\nwords which have already generated, i.e. the previous words that constitute the context of the language\\r\\n\\r\\n\\x0cinput text (ct , zct )\\r\\nct\\r\\n\\r\\nDoe\\r\\n\\r\\n(\\r\\n\\r\\n18\\r\\n\\r\\nApril\\r\\n\\r\\n1352\\r\\n\\r\\n13944\\r\\n\\r\\nunk\\r\\n\\r\\n17\\r\\n\\r\\n37\\r\\n\\r\\n92\\r\\n\\r\\n25\\r\\n\\r\\n(name,1,2)\\r\\n\\r\\nTable (gf , gw )\\r\\nname\\r\\nbirthdate\\r\\nbirthplace\\r\\noccupation\\r\\nspouse\\r\\nchildren\\r\\n\\r\\nJohn\\r\\n\\r\\nJohn Doe\\r\\n18 April 1352\\r\\nOxford UK\\r\\nplaceholder\\r\\nJane Doe\\r\\nJohnnie Doe\\r\\n\\r\\nzct\\r\\n\\r\\n(name,2,1) \\xe2\\x88\\x85\\r\\n(spouse,2,1)\\r\\n(children,2,1)\\r\\n\\r\\nzw\\r\\n\\r\\nis\\r\\n\\r\\na\\r\\n\\r\\n18 12\\r\\n\\r\\n4\\r\\n\\r\\n(birthd.,1,3) (birthd.,2,2) (birthd.,3,1) \\xe2\\x88\\x85\\r\\n\\r\\noutput candidates (w \\xe2\\x88\\x88 W \\xe2\\x88\\xaa Q)\\r\\nw\\r\\n\\r\\n)\\r\\n\\r\\n\\xe2\\x88\\x85\\r\\n\\r\\nthe\\r\\n\\r\\n...\\r\\n\\r\\napril\\r\\n\\r\\n...\\r\\n\\r\\nplaceholder\\r\\n\\r\\n...\\r\\n\\r\\njohn\\r\\n\\r\\n...\\r\\n\\r\\ndoe\\r\\n\\r\\n1\\r\\n\\r\\n...\\r\\n\\r\\n92\\r\\n\\r\\n...\\r\\n\\r\\n5302\\r\\n\\r\\n...\\r\\n\\r\\n13944\\r\\n\\r\\n...\\r\\n\\r\\nunk\\r\\n\\r\\n\\xe2\\x88\\x85\\r\\n\\r\\n(birthd.,2,2)\\r\\n\\r\\n(occupation,1,1)\\r\\n\\r\\n(name,1,2)\\r\\n\\r\\n\\xe2\\x88\\x85\\r\\n\\r\\n(name,2,1)\\r\\n(spouse,2,1)\\r\\n(children,2,1)\\r\\n\\r\\nFigure 2: Table features (right) for an example table (left); W \\xe2\\x88\\xaa Q is the set of all output words as defined in Section 3.3.\\r\\n\\r\\nmodel. The table allows us to describe each word\\r\\nnot only by its string (or index in the vocabulary)\\r\\nbut also by a descriptor of its occurrence in the table. Let F define the set of all possible fields f . The\\r\\noccurrence of a word w in the table is described by\\r\\na set of (field, position) pairs.\\r\\n\\x08\\r\\nm\\r\\nzw = (fi , pi ) i=1 ,\\r\\n(3)\\r\\n\\r\\nwhere m is the number of occurrences of w. Each\\r\\npair (f, p) indicates that w occurs in field f at position p. In this scheme, most words are described by\\r\\nthe empty set as they do not occur in the table. For\\r\\nexample, the word linguistics in the table of Figure 1\\r\\nis described as follows:\\r\\nzlinguistics = {(fields, 8); (known for, 4)},\\r\\n\\r\\n(4)\\r\\n\\r\\nassuming words are lower-cased and commas are\\r\\ntreated as separate tokens.\\r\\nConditioning both on the field type and the position within the field allows the model to encode\\r\\nfield-specific regularities, e.g., a number token in a\\r\\ndate field is likely followed by a month token; knowing that the number is the first token in the date field\\r\\nmakes this even more likely.\\r\\nThe (field, position) description scheme of the table does not allow to express that a token terminates\\r\\na field which can be useful to capture field transitions. For biographies, the last token of the name\\r\\nfield is often followed by an introduction of the birth\\r\\ndate like \\xe2\\x80\\x98(\\xe2\\x80\\x99 or \\xe2\\x80\\x98was born\\xe2\\x80\\x99. We hence extend our descriptor to a triplet that includes the position of the\\r\\n\\r\\n1205\\r\\n\\r\\ntoken counted from the end of the field:\\r\\n\\x08\\r\\n\\xe2\\x88\\x92 m\\r\\nzw = (fi , p+\\r\\ni , pi ) i=1 ,\\r\\n\\r\\n(5)\\r\\n\\r\\nwhere our example becomes:\\r\\n\\r\\nzlinguistics = {(fields, 8, 4); (known for, 4, 13)}.\\r\\nWe extend Equation 2 to use the above information as additional conditioning context when generating a sentence s:\\r\\nP (s|z) =\\r\\n\\r\\nT\\r\\nY\\r\\nt=1\\r\\n\\r\\nP (wt |ct , zct ) ,\\r\\n\\r\\n(6)\\r\\n\\r\\nwhere zct = zwt\\xe2\\x88\\x92(n\\xe2\\x88\\x921) , . . . , zwt\\xe2\\x88\\x921 are referred to as\\r\\nthe local conditioning variables since they describe\\r\\nthe local context (previous word) relations with the\\r\\ntable.\\r\\nGlobal conditioning refers to information from\\r\\nall tokens and fields of the table, regardless whether\\r\\nthey appear in the previous generated words or not.\\r\\nThe set of fields available in a table often impacts\\r\\nthe structure of the generation. For biographies, the\\r\\nfields used to describe a politician are different from\\r\\nthe ones for an actor or an athlete. We introduce\\r\\nglobal conditioning on the available fields gf as\\r\\nP (s|z, gf ) =\\r\\n\\r\\nT\\r\\nY\\r\\nt=1\\r\\n\\r\\nP (wt |ct , zct , gf ).\\r\\n\\r\\n(7)\\r\\n\\r\\nSimilarly, global conditioning gw on the available\\r\\n\\r\\n\\x0cwords occurring in the table is introduced:\\r\\nP (s|z, gf , gw ) =\\r\\n\\r\\nT\\r\\nY\\r\\nt=1\\r\\n\\r\\nP (wt |ct , zct , gf , gw ).\\r\\n\\r\\n(8)\\r\\n\\r\\nTokens provide information complementary to\\r\\nfields. For example, it may be hard to distinguish a\\r\\nbasketball player from a hockey player by looking\\r\\nonly at the field names, e.g. teams, league, position,\\r\\nweight and height, etc. However the actual field\\r\\ntokens such as team names, league name, player\\xe2\\x80\\x99s\\r\\nposition can help the model to give a better prediction. Here, gf \\xe2\\x88\\x88 {0, 1}F and gw \\xe2\\x88\\x88 {0, 1}W\\r\\nare binary indicators over fixed field and word\\r\\nvocabularies.\\r\\nFigure 2 illustrates the model with a schematic example. For predicting the next word wt after a given\\r\\ncontext ct , the language model is conditioned on sets\\r\\nof triplets for each word occurring in the table zct ,\\r\\nalong with all fields and words from this table.\\r\\n3.3 Copy actions\\r\\nSo far we extended the model conditioning with features derived from the fact table. We now turn to\\r\\nusing table information when scoring output words.\\r\\nIn particular, sentences which express facts from a\\r\\ngiven table often copy words from the table. We\\r\\ntherefore extend our model to also score special field\\r\\ntokens such as name 1 or name 2 which are subsequently added to the score of the corresponding\\r\\nwords from the field value.\\r\\nOur model reads a table and defines an output domain W \\xe2\\x88\\xaaQ. Q defines all tokens in the table, which\\r\\nmight include out of vocabulary words (\\xe2\\x88\\x88\\r\\n/ W). For\\r\\ninstance Park-Rhodes in Figure 1 is not in W. However, Park-Rhodes will be included in Q as name 2\\r\\n(since it is the second token of the name field) which\\r\\nallows our model to generate it. This mechanism\\r\\nis inspired by recent work on attention based word\\r\\ncopying for neural machine translation (Luong et al.,\\r\\n2015) as well as delexicalization for neural dialog\\r\\nsystems (Wen et al., 2015). It also builds upon older\\r\\nwork such as class-based language models for dialog\\r\\nsystems (Oh and Rudnicky, 2000).\\r\\n\\r\\n4\\r\\n\\r\\nA Neural Language Model Approach\\r\\n\\r\\nA feed-forward neural language model (NLM) estimates P (wt |ct ) with a parametric function \\xcf\\x86\\xce\\xb8\\r\\n\\r\\n1206\\r\\n\\r\\n(Equation 1), where \\xce\\xb8 refers to all learnable parameters of the network. This function is a composition\\r\\nof simple differentiable functions or layers.\\r\\n4.1\\r\\n\\r\\nMathematical notations and layers\\r\\n\\r\\nWe denote matrices as bold upper case letters (X,\\r\\nY, Z), and vectors as bold lower-case letters (a, b,\\r\\nc). Ai represents the ith row of matrix A. When\\r\\nA is a 3-d matrix, then Ai,j represents the vector\\r\\nof the ith first dimension and j th second dimension.\\r\\nUnless otherwise stated, vectors are assumed to be\\r\\ncolumn vectors. We use [v1 ; v2 ] to denote vector\\r\\nconcatenation. Next, we introduce the notation for\\r\\nthe different layers used in our approach.\\r\\nEmbedding layer. Given a parameter matrix\\r\\nX \\xe2\\x88\\x88 RN \\xc3\\x97d , the embedding layer is a lookup table\\r\\nthat performs an array indexing operation:\\r\\n\\xcf\\x88 X (xi ) = Xi \\xe2\\x88\\x88 Rd ,\\r\\n\\r\\n(9)\\r\\n\\r\\nwhere Xi corresponds to the embedding of the element xi at row i. When X is a 3-d matrix, the lookup\\r\\ntable takes two arguments:\\r\\n\\xcf\\x88 X (xi , xj ) = Xi,j \\xe2\\x88\\x88 Rd ,\\r\\n\\r\\n(10)\\r\\n\\r\\nwhere Xi,j corresponds to the embedding of the\\r\\npair (xi , xj ) at index (i, j). The lookup table operation can be applied for a sequence of elements\\r\\ns = x1 , . . . , xT . A common approach is to concatenate all resulting embeddings:\\r\\n\\x02\\r\\n\\x03\\r\\n\\xcf\\x88 X (s) = \\xcf\\x88 X (x1 ); . . . ; \\xcf\\x88 X (xT ) \\xe2\\x88\\x88 RT \\xc3\\x97d . (11)\\r\\n\\r\\nLinear layer. This layer applies a linear transformation to its inputs x \\xe2\\x88\\x88 Rn :\\r\\n\\xce\\xb3 \\xce\\xb8 (x) = Wx + b\\r\\n\\r\\n(12)\\r\\n\\r\\nwhere \\xce\\xb8 = {W, b} are the trainable parameters\\r\\nwith W \\xe2\\x88\\x88 Rm\\xc3\\x97n being the weight matrix, and\\r\\nb \\xe2\\x88\\x88 Rm is the bias term.\\r\\nSoftmax layer. Given a context input ct , the\\r\\nfinal layer outputs a score for each word wt \\xe2\\x88\\x88 W,\\r\\n\\xcf\\x86\\xce\\xb8 (ct ) \\xe2\\x88\\x88 R|W| . The probability distribution is obtained by applying the softmax activation function:\\r\\nexp(\\xcf\\x86\\xce\\xb8 (ct , w))\\r\\nP (wt = w|ct ) = P|W|\\r\\ni=1 exp(\\xcf\\x86\\xce\\xb8 (ct , wi ))\\r\\n\\r\\n(13)\\r\\n\\r\\n\\x0c4.2 Embeddings as inputs\\r\\nA key aspect of neural language models is the use\\r\\nof word embeddings. Similar words tend to have\\r\\nsimilar embeddings and thus share latent features.\\r\\nThe probability estimates of those models are\\r\\nsmooth functions of these embeddings, and a small\\r\\nchange in the features results in a small change\\r\\nin the probability estimates (Bengio et al., 2003).\\r\\nTherefore, neural language models can achieve\\r\\nbetter generalization for unseen n-grams. Next, we\\r\\nshow how we map fact tables to continuous space in\\r\\nsimilar spirit.\\r\\nWord embeddings. Formally, the embedding\\r\\nlayer maps each context word index to a continuous\\r\\nd-dimensional vector. It relies on a parameter matrix E \\xe2\\x88\\x88 R|W|\\xc3\\x97d to convert the input ct into n \\xe2\\x88\\x92 1\\r\\nvectors of dimension d:\\r\\n\\x02\\r\\n\\x03\\r\\n\\xcf\\x88 E (ct ) = \\xcf\\x88 E (wt\\xe2\\x88\\x92(n\\xe2\\x88\\x921) ); . . . ; \\xcf\\x88 E (wt\\xe2\\x88\\x921 ) . (14)\\r\\n\\r\\nE can be initialized randomly or with pre-trained\\r\\nword embeddings.\\r\\n\\r\\nTable embeddings. As described in Section 3.2,\\r\\nthe language model is conditioned on elements from\\r\\nthe table. Embedding matrices are therefore defined\\r\\nto model both local and global conditioning information. For local conditioning, we denote the maximum length of a sequence of words as l. Each field\\r\\nfj \\xe2\\x88\\x88 F is associated with 2 \\xc3\\x97 l vectors of d dimensions, the first l of those vectors embed all possible starting positions 1, . . . , l, and the remaining l\\r\\nvectors embed ending positions. This results in two\\r\\nparameter matrices Z = {Z+ , Z\\xe2\\x88\\x92 } \\xe2\\x88\\x88 R|F |\\xc3\\x97l\\xc3\\x97d .\\r\\n\\xe2\\x88\\x92\\r\\n+\\r\\nFor a given triplet (fj , p+\\r\\ni , pi ), \\xcf\\x88 Z+ (fj , pi ) and\\r\\n\\xe2\\x88\\x92\\r\\n\\xcf\\x88 Z\\xe2\\x88\\x92 (fj , pi ) refer to the embedding vectors of the\\r\\nstart and end position for field fj , respectively.\\r\\nFinally, global conditioning uses two parameter matrices Gf \\xe2\\x88\\x88 R|F |\\xc3\\x97g and Gw \\xe2\\x88\\x88 R|W|\\xc3\\x97g .\\r\\n\\xcf\\x88 Gf (fj ) maps a table field fj into a vector of\\r\\ndimension g, while \\xcf\\x88 Gw (wt ) maps a word wt into\\r\\na vector of the same dimension. In general, Gw\\r\\nshares its parameters with E, provided d = g.\\r\\nAggregating embeddings. We represent each occurence of a word w as a triplet (field, start, end)\\r\\nwhere we have embeddings for the start and end position as described above. Often times a particular\\r\\nword w occurs multiple times in a table, e.g., \\xe2\\x80\\x98lin-\\r\\n\\r\\n1207\\r\\n\\r\\nguistics\\xe2\\x80\\x99 has two instances in Figure 1. In this case,\\r\\nwe perform a component-wise max over the start\\r\\nembeddings of all instances of w to obtain the best\\r\\nfeatures across all occurrences of w. We do the same\\r\\nfor end position embeddings:\\r\\n\\xcf\\x88 Z (zwt ) =\\r\\nh\\r\\n\\x08\\r\\n+ \\xe2\\x88\\x92\\r\\nmax \\xcf\\x88 Z+ (fj , p+\\r\\ni ), \\xe2\\x88\\x80(fj , pi , pi ) \\xe2\\x88\\x88 zwt ;\\r\\ni\\r\\n\\x08\\r\\n+ \\xe2\\x88\\x92\\r\\nmax \\xcf\\x88 Z\\xe2\\x88\\x92 (fj , p\\xe2\\x88\\x92\\r\\n),\\r\\n\\xe2\\x88\\x80(f\\r\\n,\\r\\np\\r\\n,\\r\\np\\r\\n(15)\\r\\n)\\r\\n\\xe2\\x88\\x88\\r\\nz\\r\\nj\\r\\nw\\r\\nt\\r\\ni\\r\\ni\\r\\ni\\r\\n\\r\\nA special no-field embedding is assigned to wt when\\r\\nthe word is not associated to any fields. An embedding \\xcf\\x88 Z (zct ) for encoding the local conditioning of\\r\\nthe input ct is obtained by concatenation.\\r\\nFor global conditioning, we define F q \\xe2\\x8a\\x82 F as the\\r\\nset of all the fields in a given table q, and Q as the set\\r\\nof all words in q. We also perform max aggregation.\\r\\nThis yields the vectors\\r\\n\\x08\\r\\n\\xcf\\x88 Gf (gf ) = max \\xcf\\x88 Gf (fj ), \\xe2\\x88\\x80fj \\xe2\\x88\\x88 F q , (16)\\r\\nand\\r\\n\\r\\n\\x08\\r\\n\\xcf\\x88 Gw (gw ) = max \\xcf\\x88 Gw (wt ), \\xe2\\x88\\x80wt \\xe2\\x88\\x88 Q .\\r\\n\\r\\n(17)\\r\\n\\r\\nThe final embedding which encodes the context input with conditioning is then the concatenation of\\r\\nthese vectors:\\r\\n\\x02\\r\\n\\xcf\\x88 \\xce\\xb11 (ct , zct , gf , gw ) = \\xcf\\x88 E (ct ); \\xcf\\x88 Z (zct );\\r\\n\\x03\\r\\n1\\r\\n\\xcf\\x88 Gf (gf ); \\xcf\\x88 Gw (gw ) \\xe2\\x88\\x88 Rd , (18)\\r\\n\\r\\nwith \\xce\\xb11 = {E, Z+ , Z\\xe2\\x88\\x92 , Gf , Gw } and d1 = (n \\xe2\\x88\\x92\\r\\n1) \\xc3\\x97 (3 \\xc3\\x97 d) + (2 \\xc3\\x97 g). For simplification purpose,\\r\\nwe define the context input x = {ct , zct , gf , gw } in\\r\\nthe following equations. This context embedding is\\r\\nmapped to a latent context representation using a linear operation followed by a hyperbolic tangent:\\r\\n\\x10\\r\\n\\x01\\x11\\r\\nh(x) = tanh \\xce\\xb3 \\xce\\xb12 \\xcf\\x88 \\xce\\xb11 (x) \\xe2\\x88\\x88 Rnhu ,\\r\\n(19)\\r\\nwhere \\xce\\xb12 = {W2 , b2 }, with W2 \\xe2\\x88\\x88 Rnhu\\xc3\\x97d and\\r\\nb2 \\xe2\\x88\\x88 Rnhu .\\r\\n1\\r\\n\\r\\n4.3\\r\\n\\r\\nIn-vocabulary outputs\\r\\n\\r\\nThe hidden representation of the context then goes\\r\\nto another linear layer to produce a real value score\\r\\nfor each word in the vocabulary:\\r\\n\\x10\\r\\n\\x11\\r\\n\\xcf\\x86W\\r\\n(x)\\r\\n=\\r\\n\\xce\\xb3\\r\\nh(x)\\r\\n\\xe2\\x88\\x88 R|W| ,\\r\\n(20)\\r\\n\\xce\\xb13\\r\\n\\xce\\xb1\\r\\n\\r\\n\\x0cwhere \\xce\\xb13 = {W3 , b3 }, with W3 \\xe2\\x88\\x88 R|W|\\xc3\\x97nhu and\\r\\nb3 \\xe2\\x88\\x88 R|W| , and \\xce\\xb1 = {\\xce\\xb11 , \\xce\\xb12 , \\xce\\xb13 }.\\r\\n\\r\\nframework to evaluate our model, with Wikipedia\\r\\noffering a large and diverse dataset.\\r\\n\\r\\n4.4\\r\\n\\r\\n5.1\\r\\n\\r\\nMixing outputs for better copying\\r\\n\\r\\nSection 3.3 explains that each word w from the table\\r\\nis also associated with zw , the set of fields in which\\r\\nit occurs, along with the position in that field. Similar to local conditioning, we represent each field and\\r\\nposition pair (fj , pi ) with an embedding \\xcf\\x88 F (fj , pi ),\\r\\nwhere F \\xe2\\x88\\x88 R|F|\\xc3\\x97l\\xc3\\x97d . These embeddings are then\\r\\nprojected into the same space as the latent representation of context input h(x) \\xe2\\x88\\x88 Rnhu . Using the max\\r\\noperation over the embedding dimension, each word\\r\\nis finally embedded into a unique vector:\\r\\n\\x08\\r\\nq(w) = max\\r\\n\\x10\\r\\n\\x01\\x11\\r\\ntanh \\xce\\xb3 \\xce\\xb2 \\xcf\\x88 F (fj , pi ) , \\xe2\\x88\\x80(fj , pi ) \\xe2\\x88\\x88 zw , (21)\\r\\nwhere \\xce\\xb2 = {W4 , b4 } with W4 \\xe2\\x88\\x88 Rnhu\\xc3\\x97d , and\\r\\nb4 \\xe2\\x88\\x88 Rnhu . A dot product with the context vector\\r\\nproduces a score for each word w in the table,\\r\\n\\xcf\\x86Q\\r\\n\\xce\\xb2 (x, w) = h(x) \\xc2\\xb7 q(w) .\\r\\n\\r\\n(22)\\r\\n\\r\\nBiography dataset\\r\\n\\r\\nWe introduce a new dataset for text generation,\\r\\nW IKI B IO, a corpus of 728,321 articles from English Wikipedia (Sep 2015). It comprises all biography articles listed by WikiProject Biography1 which\\r\\nalso have a table (infobox). We extract and tokenize the first sentence of each article with Stanford\\r\\nCoreNLP (Manning et al., 2014). All numbers are\\r\\nmapped to a special token, except for years which\\r\\nare mapped to different special token. Field values\\r\\nfrom tables are similarly tokenized. All tokens are\\r\\nlower-cased. Table 2 summarizes the dataset statistics: on average, the first sentence is twice as short as\\r\\nthe table (26.1 vs 53.1 tokens), about a third of the\\r\\nsentence tokens (9.5) also occur in the table. The\\r\\nfinal corpus has been divided into three sub-parts\\r\\nto provide training (80%), validation (10%) and test\\r\\nsets (10%). The dataset is available for download2 .\\r\\n5.2\\r\\n\\r\\nBaseline\\r\\n\\r\\nOur baseline is an interpolated Kneser-Ney (KN)\\r\\nlanguage model and we use the KenLM toolkit\\r\\nto train 5-gram models without pruning (Heafield\\r\\nQ\\r\\nW\\r\\n(23) et al., 2013). We also learn a KN language\\r\\n\\xcf\\x86\\xce\\xb8 (x, w) = \\xcf\\x86\\xce\\xb1 (x, w) + \\xcf\\x86\\xce\\xb2 (x, w) ,\\r\\nmodel over templates. For that purpose, we reQ\\r\\nwith \\xce\\xb8 = {\\xce\\xb1, \\xce\\xb2}, and where \\xcf\\x86\\xce\\xb2 (x, w) = 0 when place the words occurring in both the table and\\r\\nw\\xe2\\x88\\x88\\r\\n/ Q. The softmax function then maps the scores the training sentences with a special token reflecting its table descriptor zw (Equation 3). The into a distribution over W \\xe2\\x88\\xaa Q,\\r\\ntroduction section of the table in Figure 1 looks\\r\\nX\\r\\nlog P (w|x) = \\xcf\\x86\\xce\\xb8 (x, w)\\xe2\\x88\\x92log\\r\\nexp \\xcf\\x86\\xce\\xb8 (x, w0 ) . as follows under this scheme: \\xe2\\x80\\x9cname 1 name 2\\r\\nw0 \\xe2\\x88\\x88W\\xe2\\x88\\xaaQ\\r\\n( birthdate 1 birthdate 2 birthdate 3 \\xe2\\x80\\x93\\r\\ndeathdate 1 deathdate 2 deathdate 3 ) was\\r\\n4.5 Training\\r\\nan english linguist , fields 3 pathologist ,\\r\\nThe neural language model is trained to minimize fields 10 scientist , mathematician , mystic and\\r\\nthe negative log-likelihood of a training sentence s mycologist .\\xe2\\x80\\x9d During inference, the decoder is conwith stochastic gradient descent (SGD; LeCun et al. strained to emit words from the regular vocabulary\\r\\n2012) :\\r\\nor special tokens occurring in the input table. When\\r\\npicking a special token we copy the corresponding\\r\\nT\\r\\nX\\r\\nL\\xce\\xb8 (s) = \\xe2\\x88\\x92\\r\\nlog P (wt |ct , zct , gf , gw ) . (24) word from the table.\\r\\nEach word w \\xe2\\x88\\x88 W \\xe2\\x88\\xaa Q receives a final score by\\r\\nsumming the vocabulary score and the field score:\\r\\n\\r\\nt=1\\r\\n\\r\\n5\\r\\n\\r\\n5.3\\r\\n\\r\\nExperiments\\r\\n\\r\\nOur neural network model (Section 4) is designed to\\r\\ngenerate sentences from tables for large-scale problems, where a diverse set of sentence types need\\r\\nto be generated. Biographies are therefore a good\\r\\n\\r\\n1208\\r\\n\\r\\nTraining setup\\r\\n\\r\\nFor our neural models, we train 11-gram language\\r\\nmodels (n = 11) with a learning rate set to 0.0025.\\r\\n1\\r\\nhttps://en.wikipedia.org/wiki/\\r\\nWikipedia:WikiProject_Biography\\r\\n2\\r\\nhttps://github.com/DavidGrangier/\\r\\nwikipedia-biography-dataset\\r\\n\\r\\n\\x0cModel\\r\\n\\r\\nPerplexity\\r\\n\\r\\nBLEU\\r\\n\\r\\nROUGE\\r\\n\\r\\nNIST\\r\\n\\r\\nKN\\r\\nNLM\\r\\n+ Local (field, start, end)\\r\\n\\r\\n10.51\\r\\n9.40 +\\r\\n\\xe2\\x88\\x92 0.01\\r\\n8.61 +\\r\\n\\xe2\\x88\\x92 0.01\\r\\n\\r\\n2.21\\r\\n2.41 +\\r\\n\\xe2\\x88\\x92 0.33\\r\\n4.17 +\\r\\n\\xe2\\x88\\x92 0.54\\r\\n\\r\\n0.38\\r\\n0.52 +\\r\\n\\xe2\\x88\\x92 0.08\\r\\n1.48 +\\r\\n\\xe2\\x88\\x92 0.23\\r\\n\\r\\n0.93\\r\\n1.27 +\\r\\n\\xe2\\x88\\x92 0.26\\r\\n1.41 +\\r\\n\\xe2\\x88\\x92 0.11\\r\\n\\r\\nTemplate KN\\r\\nTable NLM w/ Local (field, start)\\r\\n+ Local (field, start, end)\\r\\n+ Global (field)\\r\\n+ Global (field & word)\\r\\n\\r\\n7.46?\\r\\n\\xe2\\x80\\xa0\\r\\n4.60 +\\r\\n\\xe2\\x88\\x92 0.01\\r\\n\\xe2\\x80\\xa0\\r\\n4.60 +\\r\\n\\xe2\\x88\\x92 0.01\\r\\n\\xe2\\x80\\xa0\\r\\n4.30 +\\r\\n\\xe2\\x88\\x92 0.01\\r\\n\\xe2\\x80\\xa0\\r\\n4.40 +\\r\\n\\xe2\\x88\\x92 0.02\\r\\n\\r\\n19.8\\r\\n26.0 +\\r\\n\\xe2\\x88\\x92 0.39\\r\\n26.6 +\\r\\n\\xe2\\x88\\x92 0.42\\r\\n33.4 +\\r\\n\\xe2\\x88\\x92 0.18\\r\\n34.7 +\\r\\n\\xe2\\x88\\x92 0.36\\r\\n\\r\\n10.7\\r\\n19.2 +\\r\\n\\xe2\\x88\\x92 0.23\\r\\n19.7 +\\r\\n\\xe2\\x88\\x92 0.25\\r\\n23.9 +\\r\\n\\xe2\\x88\\x92 0.12\\r\\n25.8 +\\r\\n\\xe2\\x88\\x92 0.36\\r\\n\\r\\n5.19\\r\\n6.08 +\\r\\n\\xe2\\x88\\x92 0.08\\r\\n6.20 +\\r\\n\\xe2\\x88\\x92 0.09\\r\\n7.52 +\\r\\n\\xe2\\x88\\x92 0.03\\r\\n7.98 +\\r\\n\\xe2\\x88\\x92 0.07\\r\\n\\r\\nTable 1: BLEU, ROUGE, NIST and perplexity without copy actions (first three rows) and with copy actions (last five rows). For\\r\\nneural models we report \\xe2\\x80\\x9cmean +\\r\\n\\xe2\\x88\\x92 standard deviation\\xe2\\x80\\x9d for five training runs with different initialization. Decoding beam width is 5.\\r\\nPerplexities marked with ? and \\xe2\\x80\\xa0 are not directly comparable as the output vocabularies differ slightly.\\r\\n\\r\\nMean\\r\\n# tokens per sentence\\r\\n# tokens per table\\r\\n# table tokens per sent.\\r\\n# fields per table\\r\\n\\r\\n26.1\\r\\n53.1\\r\\n9.5\\r\\n19.7\\r\\n\\r\\nshown to be helpful for various applications (Lebret\\r\\nand Collobert, 2014).\\r\\n\\r\\nPercentile\\r\\n5% 95%\\r\\n13\\r\\n20\\r\\n3\\r\\n9\\r\\n\\r\\n46\\r\\n108\\r\\n19\\r\\n36\\r\\n\\r\\n5.4\\r\\n\\r\\nWe use different metrics to evaluate our models.\\r\\nPerformance is first evaluated in terms of perplexity which is the standard metric for language modeling. Generation quality is assessed automatically\\r\\nwith BLEU-4, ROUGE-4 (F-measure) and NIST43 (Belz and Reiter, 2006).\\r\\n\\r\\nTable 2: Dataset statistics\\r\\n\\r\\nParameter\\r\\n\\r\\nValue\\r\\n\\r\\n# word types\\r\\n# field types\\r\\nMax. # tokens in a field\\r\\nword/field embedding size\\r\\nglobal embedding size\\r\\n# hidden units\\r\\n\\r\\n|W|\\r\\n|F|\\r\\nl\\r\\nd\\r\\ng\\r\\nnhu\\r\\n\\r\\n=\\r\\n=\\r\\n=\\r\\n=\\r\\n=\\r\\n=\\r\\n\\r\\nEvaluation metrics\\r\\n\\r\\n20, 000\\r\\n1, 740\\r\\n10\\r\\n64\\r\\n128\\r\\n256\\r\\n\\r\\nTable 3: Model Hyperparameters\\r\\n\\r\\nTable 3 describes the other hyper-parameters. We\\r\\ninclude all fields occurring at least 100 times in the\\r\\ntraining data in F, the set of fields. We include\\r\\nthe 20, 000 most frequent words in the vocabulary.\\r\\nThe other hyperparameters are set through validation, maximizing BLEU over a validation subset of\\r\\n1, 000 sentences. Similarly, early stopping is applied: training ends when BLEU stops improving\\r\\non the same validation subset. One should note that\\r\\nthe maximum number of tokens in a field l = 10\\r\\nmeans that we encode only 10 positions: for longer\\r\\nfield values the final tokens are not dropped but their\\r\\nposition is capped to 10. We initialize the word embeddings W from Hellinger PCA computed over the\\r\\nset of training biographies. This representation has\\r\\n\\r\\n1209\\r\\n\\r\\n6\\r\\n\\r\\nResults\\r\\n\\r\\nThis section describes our results and discusses the\\r\\nimpact of the different conditioning variables.\\r\\n6.1\\r\\n\\r\\nThe more, the better\\r\\n\\r\\nThe results (Table 1) show that more conditioning\\r\\ninformation helps to improve the performance of our\\r\\nmodels. The generation metrics BLEU, ROUGE\\r\\nand NIST all gives the same performance ordering\\r\\nover models. We first discuss models without copy\\r\\nactions (the first three results) and then discuss models with copy actions (the remaining results). Note\\r\\nthat the factorization of our models results in three\\r\\ndifferent output domains which makes perplexity\\r\\ncomparisons less straightforward: models without\\r\\ncopy actions operate over a fixed vocabulary. Template KN adds a fixed set of field/position pairs to\\r\\nthis vocabulary while Table NLM models a variable\\r\\nset Q depending on the input table, see Section 3.3.\\r\\nWithout copy actions. In terms of perplexity the\\r\\n(i) neural language model (NLM) is slightly better\\r\\n3\\r\\nWe rely on standard software, NIST mteval-v13a.pl (for\\r\\nNIST, BLEU), and MSR rouge-1.5.5 (for ROUGE).\\r\\n\\r\\n\\x0c45\\r\\n40\\r\\n\\r\\n\\xe2\\x97\\x8f\\r\\n\\r\\n35\\r\\n\\r\\n3 4\\xe2\\x97\\x8f 5\\xe2\\x97\\x8f 6\\xe2\\x97\\x8f 7 810\\r\\n\\xe2\\x97\\x8f\\r\\n\\xe2\\x97\\x8f\\r\\n\\xe2\\x97\\x8f\\xe2\\x97\\x8f\\r\\n\\r\\n15\\r\\n\\r\\n\\xe2\\x97\\x8f\\r\\n\\r\\n\\xe2\\x97\\x8f\\r\\n\\r\\n1\\r\\n\\r\\n\\xe2\\x97\\x8f\\r\\n\\r\\n30\\r\\n25\\r\\n15\\r\\n\\r\\n20\\r\\n\\r\\n5\\r\\n4\\r\\n\\r\\n\\xe2\\x97\\x8f\\r\\n\\r\\n2\\r\\n\\r\\n6\\r\\n\\xe2\\x97\\x8f\\r\\n\\r\\n\\xe2\\x97\\x8f\\r\\n\\r\\n8 10\\r\\n\\r\\n\\xe2\\x97\\x8f \\xe2\\x97\\x8f\\r\\n\\r\\n\\xe2\\x97\\x8f \\xe2\\x97\\x8f\\r\\n\\r\\n25\\r\\n15 20\\r\\n\\xe2\\x97\\x8f\\r\\n\\xe2\\x97\\x8f\\r\\n\\xe2\\x97\\x8f\\r\\n\\r\\n3\\r\\n\\xe2\\x97\\x8f\\r\\n\\r\\n\\xe2\\x97\\x8f\\r\\n\\r\\n1\\r\\n\\xe2\\x97\\x8f\\r\\n\\r\\n100\\r\\n\\r\\n200\\r\\n\\r\\n500\\r\\n\\r\\n1000\\r\\n\\r\\n1 2\\r\\n\\r\\n1 2 3\\r\\n\\r\\n1 2\\r\\n\\r\\nn\\r\\ntio\\r\\nupa\\r\\n\\r\\ny\\r\\n\\r\\n1\\r\\n\\r\\nocc\\r\\n\\r\\nalit\\r\\nna t\\r\\nion\\r\\n\\r\\ne\\r\\nlac\\r\\nbir\\r\\nthp\\r\\n\\r\\nbir\\r\\nth\\r\\n\\r\\ndat\\r\\n\\r\\ne\\r\\n\\r\\n<s>\\r\\nnellie\\r\\nwong\\r\\n\\r\\n1 2\\r\\n\\r\\n(\\r\\n\\r\\nborn\\r\\nseptember\\r\\n12\\r\\n,\\r\\n1934\\r\\n)\\r\\nis\\r\\nan\\r\\namerican\\r\\npoet\\r\\nand\\r\\nactivist\\r\\n.\\r\\nFigure 4: Visualization of attention scores for Nellie Wong\\xe2\\x80\\x99s\\r\\nWikipedia infobox. Each row represents the probability distribution over (field, position) pairs given the previous words (i.e.\\r\\nthe words heading the preceding rows as well as the current\\r\\nrow). Darker colors depict higher probabilities.\\r\\n\\r\\n20 25\\r\\n\\xe2\\x97\\x8f\\r\\n\\r\\n\\xe2\\x97\\x8f\\r\\n\\r\\n\\xe2\\x97\\x8f\\r\\n\\r\\nBLEU\\r\\n\\r\\nTemplate KN\\r\\nTable NLM\\r\\nbeam size\\r\\n\\r\\nnam\\r\\ne\\r\\n\\r\\nthan an interpolated KN language model, and (ii)\\r\\nadding local conditioning on the field start and end\\r\\nposition further improves accuracy. Generation metrics are generally very low but there is a clear improvement when using local conditioning since it allows to learn transitions between fields by linking\\r\\nprevious predictions to the table unlike KN or plain\\r\\nNLM.\\r\\nWith copy actions. For experiments with copy\\r\\nactions we use the full local conditioning (Equation 4) in the neural language models. BLEU,\\r\\nROUGE and NIST all improves when moving from\\r\\nTemplate KN to Table NLM and more features successively improve accuracy. Global conditioning on\\r\\nthe fields improves the model by over 7 BLEU and\\r\\nadding words gives an additional 1.3 BLEU. This\\r\\nis a total improvement of nearly 15 BLEU over the\\r\\nTemplate Kneser-Ney baseline. Similar observations are made for ROUGE +15 and NIST +2.8.\\r\\n\\r\\n2000\\r\\n\\r\\ntime in ms\\r\\n\\r\\nFigure 3: Comparison between our best model (Table NLM)\\r\\nand the baseline (Template KN) for different beam sizes. The\\r\\nx-axis is the average timing (in milliseconds) for generating one\\r\\nsentence. The y-axis is the BLEU score. All results are measured on a subset of 1,000 samples of the validation set.\\r\\n\\r\\n6.2 Attention mechanism\\r\\nOur model implements attention over input table\\r\\nfields. For each word w in the table, Equation (23)\\r\\ntakes the language model score \\xcf\\x86W\\r\\nct and adds a bias\\r\\nQ\\r\\n\\xcf\\x86ct . The bias is the dot-product between a representation of the table field in which w occurs and a representation of the context, Equation (22) that summarizes the previously generated fields and words.\\r\\n\\r\\n1210\\r\\n\\r\\nFigure 4 shows that this mechanism adds a large\\r\\nbias to continue a field if it has not generated all\\r\\ntokens from the table, e.g., it emits the word occurring in name 2 after generating name 1. It also\\r\\nnicely handles transitions between field types, e.g.,\\r\\nthe model adds a large bias to the words occurring\\r\\nin the occupation field after emitting the birthdate.\\r\\n6.3\\r\\n\\r\\nSentence decoding\\r\\n\\r\\nWe use a standard beam search to explore a larger\\r\\nset of sentences compared to simple greedy search.\\r\\nThis allows us to explore K times more paths which\\r\\ncomes at a linear increase in the number of forward\\r\\ncomputation steps for our language model. We compare various beam settings for the baseline Template\\r\\nKN and our Table NLM (Figure 3). The best validation BLEU can be obtained with a beam size of\\r\\nK = 5. Our model is also several times faster than\\r\\nthe baseline, requiring only about 200 ms per sentence with K = 5. Beam search generates many ngram lookups for Kneser-Ney which requires many\\r\\n\\r\\n\\x0cModel\\r\\n\\r\\nGenerated Sentence\\r\\n\\r\\nReference\\r\\n\\r\\nfrederick parker-rhodes (21 march 1914 \\xe2\\x80\\x93 21 november 1987) was an english linguist, plant\\r\\npathologist, computer scientist, mathematician, mystic, and mycologist.\\r\\n\\r\\nBaseline\\r\\n(Template KN)\\r\\n\\r\\nfrederick parker-rhodes ( born november 21 , 1914 \\xe2\\x80\\x93 march 2 , 1987 ) was an english cricketer\\r\\n.\\r\\n\\r\\nTable NLM\\r\\n+Local (field, start)\\r\\n\\r\\nfrederick parker-rhodes ( 21 november 1914 \\xe2\\x80\\x93 2 march 1987 ) was an australian rules footballer who played with carlton in the victorian football league ( vfl ) during the XXXXs and\\r\\nXXXXs .\\r\\n\\r\\n+ Global (field)\\r\\n\\r\\nfrederick parker-rhodes ( 21 november 1914 \\xe2\\x80\\x93 2 march 1987 ) was an english mycology and\\r\\nplant pathology , mathematics at the university of uk .\\r\\n\\r\\n+ Global\\r\\n(field, word)\\r\\n\\r\\nfrederick parker-rhodes ( 21 november 1914 \\xe2\\x80\\x93 2 march 1987 ) was a british computer scientist\\r\\n, best known for his contributions to computational linguistics .\\r\\n\\r\\nTable 4: First sentence from the current Wikipedia article about Frederick Parker-Rhodes and the sentences generated from the\\r\\nthree versions of our table-conditioned neural language model (Table NLM) using the Wikipedia infobox seen in Figure 1.\\r\\n\\r\\nrandom memory accesses; while neural models perform scoring through matrix-matrix products, an operation which is more local and can be performed in\\r\\na block parallel manner where modern graphic processors shine (Kindratenko, 2014).\\r\\n6.4 Qualitative analysis\\r\\nTable 4 shows generations for different variants of\\r\\nour model based on the Wikipedia table in Figure 1.\\r\\nFirst of all, comparing the reference to the fact table\\r\\nreveals that our training data is not perfect. The birth\\r\\nmonth mentioned in the fact table and the first sentence of the Wikipedia article are different; this may\\r\\nhave been introduced by one contributor editing the\\r\\narticle and not keeping the information consistent.\\r\\nAll three versions of our model correctly generate\\r\\nthe beginning of the sentence by copying the name,\\r\\nthe birth date and the death date from the table. The\\r\\nmodel correctly uses the past tense since the death\\r\\ndate in the table indicates that the person has passed\\r\\naway. Frederick Parker-Rhodes was a scientist, but\\r\\nthis occupation is not directly mentioned in the table.\\r\\nThe model without global conditioning can therefore not predict the right occupation, and it continues the generation with the most common occupation (in Wikipedia) for a person who has died. In\\r\\ncontrast, the global conditioning over the fields helps\\r\\nthe model to understand that this person was indeed\\r\\na scientist. However, it is only with the global conditioning on the words that the model can infer the\\r\\ncorrect occupation, i.e., computer scientist.\\r\\n\\r\\n1211\\r\\n\\r\\n7\\r\\n\\r\\nConclusions\\r\\n\\r\\nWe have shown that our model can generate fluent descriptions of arbitrary people based on structured data. Local and global conditioning improves\\r\\nour model by a large margin and we outperform a\\r\\nKneser-Ney language model by nearly 15 BLEU.\\r\\nOur task uses an order of magnitude more data than\\r\\nprevious work and has a vocabulary that is three orders of magnitude larger.\\r\\nIn this paper, we have only focused on generating\\r\\nthe first sentence and we will tackle the generation of\\r\\nlonger biographies in future work. Also, the encoding of field values can be improved. Currently, we\\r\\nonly attach the field type and token position to each\\r\\nword type and perform a max-pooling for local conditioning. One could leverage a richer representation\\r\\nby learning an encoder conditioned on the field type,\\r\\ne.g. a recurrent encoder or a convolutional encoder\\r\\nwith different pooling strategies.\\r\\nFurthermore, the current training loss function\\r\\ndoes not explicitly penalize the model for generating\\r\\nincorrect facts, e.g. predicting an incorrect nationality or occupation is currently not considered worse\\r\\nthan choosing an incorrect determiner. A loss function that could assess factual accuracy would certainly improve sentence generation by avoiding such\\r\\nmistakes. Also it will be important to define a strategy for evaluating the factual accuracy of a generation, beyond BLEU, ROUGE or NIST.\\r\\n\\r\\n\\x0cReferences\\r\\nG. Angeli, P. Liang, and D. Klein. 2010. A simple\\r\\ndomain-independent probabilistic approach to generation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages\\r\\n502\\xe2\\x80\\x93512. Association for Computational Linguistics.\\r\\nD. Bahdanau, K. Cho, and Y. Bengio. 2015. Neural machine translation by jointly learning to align and translate. In International Conference on Learning Representations.\\r\\nR. Barzilay and M. Lapata. 2005. Collective content selection for concept-to-text generation. In Proceedings\\r\\nof the conference on Human Language Technology and\\r\\nEmpirical Methods in Natural Language Processing,\\r\\npages 331\\xe2\\x80\\x93338.\\r\\nR. Barzilay and M. Lapata. 2006. Aggregation via set\\r\\npartitioning for natural language generation. In Proceedings of the main conference on Human Language\\r\\nTechnology Conference of the North American Chapter\\r\\nof the Association of Computational Linguistics, pages\\r\\n359\\xe2\\x80\\x93366. Association for Computational Linguistics.\\r\\nA. Belz and E. Reiter. 2006. Comparing automatic and\\r\\nhuman evaluation of nlg systems. In In Proc. EACL06,\\r\\npages 313\\xe2\\x80\\x93320.\\r\\nA. Belz. 2008. Automatic generation of weather forecast\\r\\ntexts using comprehensive probabilistic generationspace models.\\r\\nNatural Language Engineering,\\r\\n14(04):431\\xe2\\x80\\x93455.\\r\\nY. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. 2003.\\r\\nA neural probabilistic language model. Journal of Machine Learning Research, 3:1137\\xe2\\x80\\x931155.\\r\\nK. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor. 2008. Freebase: a collaboratively created graph\\r\\ndatabase for structuring human knowledge. In International Conference on Management of Data, pages\\r\\n1247\\xe2\\x80\\x931250. ACM.\\r\\nR. Dale, S. Geldof, and J.-P. Prost. 2003. Coral: Using natural language generation for navigational assistance. In Proceedings of the 26th Australasian\\r\\ncomputer science conference-Volume 16, pages 35\\xe2\\x80\\x9344.\\r\\nAustralian Computer Society, Inc.\\r\\nJ. Devlin, R. Zbib, Z. Huang, T. Lamar, R. Schwartz, and\\r\\nJ. Makhoul. 2014. Fast and robust neural network\\r\\njoint models for statistical machine translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, volume 1, pages\\r\\n1370\\xe2\\x80\\x931380.\\r\\nP. A. Duboue and K. R. McKeown. 2002. Content\\r\\nplanner construction via evolutionary algorithms and a\\r\\ncorpus-based fitness function. In Proceedings of INLG\\r\\n2002, pages 89\\xe2\\x80\\x9396.\\r\\nH. Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. Deng,\\r\\nP. Dollar, J. Gao, X. He, M. Mitchell, J. C. Platt, L. C.\\r\\n\\r\\n1212\\r\\n\\r\\nZitnick, and G. Zweig. 2015. From captions to visual\\r\\nconcepts and back. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June.\\r\\nD. Ferrucci. 2012. Introduction to this is watson. IBM\\r\\nJournal of Research and Development, 56(3.4):1\\xe2\\x80\\x931.\\r\\nD. Galanis and I. Androutsopoulos. 2007. Generating\\r\\nmultilingual descriptions from linguistically annotated\\r\\nowl ontologies: the naturalowl system. In Proceedings of the Eleventh European Workshop on Natural\\r\\nLanguage Generation, pages 143\\xe2\\x80\\x93146. Association for\\r\\nComputational Linguistics.\\r\\nN. Green. 2006. Generation of biomedical arguments for\\r\\nlay readers. In Proceedings of the Fourth International\\r\\nNatural Language Generation Conference, pages 114\\xe2\\x80\\x93\\r\\n121. Association for Computational Linguistics.\\r\\nB. Gyawali and C. Gardent. 2014. Surface realisation\\r\\nfrom knowledge-bases. In Proc. of ACL.\\r\\nK. Heafield, I. Pouzyrevsky, J. H. Clark, and P. Koehn.\\r\\n2013. Scalable modified Kneser-Ney language model\\r\\nestimation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,\\r\\npages 690\\xe2\\x80\\x93696, Sofia, Bulgaria, August.\\r\\nA. Karpathy and L. Fei-Fei. 2015. Deep visual-semantic\\r\\nalignments for generating image descriptions. In The\\r\\nIEEE Conference on Computer Vision and Pattern\\r\\nRecognition (CVPR), June.\\r\\nJ. Kim and R. J. Mooney. 2010. Generative alignment\\r\\nand semantic parsing for learning from ambiguous supervision. In Proceedings of the 23rd International\\r\\nConference on Computational Linguistics: Posters,\\r\\npages 543\\xe2\\x80\\x93551. Association for Computational Linguistics.\\r\\nV. Kindratenko. 2014. Numerical Computations with\\r\\nGPUs. Springer.\\r\\nR. Kiros, R. Salakhutdinov, and R. S. Zemel. 2014.\\r\\nUnifying visual-semantic embeddings with multimodal neural language models.\\r\\narXiv preprint\\r\\narXiv:1411.2539.\\r\\nI. Konstas and M. Lapata. 2013. A global model\\r\\nfor concept-to-text generation. J. Artif. Int. Res.,\\r\\n48(1):305\\xe2\\x80\\x93346, October.\\r\\nI. Langkilde and K. Knight. 1998. Generation that exploits corpus-based statistical knowledge. In Proc.\\r\\nACL, pages 704\\xe2\\x80\\x93710.\\r\\nR. Lebret and R. Collobert. 2014. Word embeddings\\r\\nthrough hellinger pca. In Proceedings of the 14th Conference of the European Chapter of the Association for\\r\\nComputational Linguistics, pages 482\\xe2\\x80\\x93490, Gothenburg, Sweden, April. Association for Computational\\r\\nLinguistics.\\r\\nY. A LeCun, L. Bottou, G. B. Orr, and K.-R. Mu\\xcc\\x88ller.\\r\\n2012. Efficient backprop. In Neural networks: Tricks\\r\\nof the trade, pages 9\\xe2\\x80\\x9348. Springer.\\r\\n\\r\\n\\x0cP. Liang, M. I. Jordan, and D. Klein. 2009. Learning\\r\\nsemantic correspondences with less supervision. In\\r\\nProceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International\\r\\nJoint Conference on Natural Language Processing of\\r\\nthe AFNLP: Volume 1-Volume 1, pages 91\\xe2\\x80\\x9399. Association for Computational Linguistics.\\r\\nW. Lu and H. T. Ng. 2011. A probabilistic forestto-string model for language generation from typed\\r\\nlambda calculus expressions. In Proceedings of the\\r\\nConference on Empirical Methods in Natural Language Processing, pages 1611\\xe2\\x80\\x931622. Association for\\r\\nComputational Linguistics.\\r\\nM.-T. Luong, I. Sutskever, Q. V Le, O. Vinyals, and\\r\\nW. Zaremba. 2015. Addressing the rare word problem in neural machine translation. In Proc. ACL, pages\\r\\n11\\xe2\\x80\\x9319.\\r\\nF. Mairesse and M. Walker. 2011. Controlling user perceptions of linguistic style: Trainable generation of\\r\\npersonality traits. Comput. Linguist., 37(3):455\\xe2\\x80\\x93488.\\r\\nC. D. Manning, M. Surdeanu, J. Bauer, J. Finkel, S. J.\\r\\nBethard, and D. McClosky. 2014. The Stanford\\r\\nCoreNLP natural language processing toolkit. In Association for Computational Linguistics (ACL) System\\r\\nDemonstrations, pages 55\\xe2\\x80\\x9360.\\r\\nH. Mei, M. Bansal, and M. R. Walter. 2016. What to\\r\\ntalk about and how? selective generation using lstms\\r\\nwith coarse-to-fine alignment. In Proceedings of Human Language Technologies: The 2016 Annual Conference of the North American Chapter of the Association for Computational Linguistics.\\r\\nA. Oh and A. Rudnicky. 2000. Stochastic language generation for spoken dialogue systems. In ANLP/NAACL\\r\\nWorkshop on Conversational Systems, pages 27\\xe2\\x80\\x9332.\\r\\nA. Ratnaparkhi. 2002. Trainable approaches to surface natural language generation and their application\\r\\nto conversational dialog systems. Computer Speech &\\r\\nLanguage, 16(3):435\\xe2\\x80\\x93455.\\r\\nE. Reiter, R. Dale, and Z. Feng. 2000. Building natural\\r\\nlanguage generation systems, volume 33. MIT Press.\\r\\nE. Reiter, S. Sripada, J. Hunter, J. Yu, and I. Davy. 2005.\\r\\nChoosing words in computer-generated weather forecasts. Artificial Intelligence, 167(1):137\\xe2\\x80\\x93169.\\r\\nL. Shang, Z. Lu, and H. Li. 2015. Neural responding\\r\\nmachine for short-text conversation. arXiv preprint\\r\\narXiv:1503.02364.\\r\\nRadu Soricut and Daniel Marcu. 2006. Stochastic language generation using widl-expressions and its application in machine translation and summarization. In\\r\\nProc. ACL, pages 1105\\xe2\\x80\\x931112.\\r\\nR. Turner, S. Sripada, and E. Reiter. 2010. Generating\\r\\napproximate geographic descriptions. In Empirical\\r\\nmethods in natural language generation, pages 121\\xe2\\x80\\x93\\r\\n140. Springer.\\r\\n\\r\\n1213\\r\\n\\r\\nO. Vinyals, A. Toshev, S. Bengio, and D. Erhan. 2015.\\r\\nShow and tell: A neural image caption generator. In\\r\\nThe IEEE Conference on Computer Vision and Pattern\\r\\nRecognition (CVPR), June.\\r\\nT. Wen, M. Gasic, N. Mrks\\xcc\\x8cic\\xcc\\x81, P. Su, D. Vandyke, and\\r\\nS. Young. 2015. Semantically conditioned lstmbased natural language generation for spoken dialogue\\r\\nsystems. In Proceedings of the 2015 Conference on\\r\\nEmpirical Methods in Natural Language Processing,\\r\\npages 1711\\xe2\\x80\\x931721, Lisbon, Portugal, September. Association for Computational Linguistics.\\r\\nY. W. Wong and R. J. Mooney. 2007. Generation by\\r\\ninverting a semantic parser that uses statistical machine\\r\\ntranslation. In HLT-NAACL, pages 172\\xe2\\x80\\x93179.\\r\\nK. Xu, J. Ba, R. Kiros, A. Courville, R. Salakhutdinov,\\r\\nR. Zemel, and Y. Bengio. 2015. Show, attend and tell:\\r\\nNeural image caption generation with visual attention.\\r\\nIn Proceedings of The 32nd International Conference\\r\\non Machine Learning, volume 37, July.\\r\\nK. Yao, G. Zweig, and B. Peng. 2015. Attention with intention for a neural network conversation model. arXiv\\r\\npreprint arXiv:1510.08565.\\r\\n\\r\\n\\x0c'\n"
     ]
    }
   ],
   "source": [
    "path = r'C:\\Users\\User\\Desktop\\Terminology project\\Data'  \n",
    "doc = convertpdfstotext(path)\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Text Generation Structured Data Application Biography Domain Lebret EPFL Switzerland David Grangier Facebook Research Abstract This paper introduces neural model concept text generation scales large rich domains generates biographical sentences fact tables new dataset biographies Wikipedia This set order magnitude larger existing resources samples vocabulary Our model builds conditional neural language models text generation deal large vocabulary extend models mix fixed vocabulary copy actions transfer sample specific words input database generated output sentence deal structured data allow model embed words differently depending data fields occur Our neural model significantly outperforms Templated Kneser Ney language model nearly BLEU Introduction Concept text generation renders structured records natural language Reiter typical application generate weather forecast based set structured meteorological measurements contrast previous work scale large diverse problem generating biographies based Wikipedia infoboxes infobox fact table describing person similar person subgraph knowledge base Bollacker Ferrucci Similar generation applications include generation product descriptions based catalog millions items dozens attributes Previous work experimented datasets contain tens thousands records EATHERGOV ROBOCUP dataset dataset contains biographies performed work interning Facebook Michael Auli Facebook Research Wikipedia Furthermore datasets limited vocabulary words compared words dataset tackle problem introduce statistical generation model conditioned Wikipedia infobox focus generation first sentence biography requires model select among large number possible fields generate adequate output Such diversity makes difficult classical count based models estimate probabilities rare events due data sparsity address issue parameterizing words fields embeddings along neural language model operating Bengio This factorization allows scale larger number words fields Liang Kim Mooney number parameters grows product number words fields Moreover approach restrict relations field contents generated text This contrasts less flexible strategies assume generation follow either hybrid alignment tree Kim Mooney probabilistic context free grammar Konstas Lapata tree adjoining grammar Gyawali Gardent Our model exploits structured data globally locally Global conditioning summarizes information personality understand highlevel themes biography scientist artist local conditioning describes previously generated tokens terms relationship infobox analyze effectiveness demonstrate complementarity Related Work Traditionally generation systems relied rules hand crafted specifications Dale Reiter Green Galanis Androut Proceedings Conference Empirical Methods Natural Language Processing pages Austin Texas November Association Computational Linguistics sopoulos Turner Generation divided modular yet highly interdependent decisions content planning defines parts input fields meaning representations selected sentence planning determines selected fields dealt output sentence surface realization generates sentences Data driven approaches proposed automatically learn individual modules One approach first aligns records sentences learns content selection model Duboue McKeown Barzilay Lapata Hierarchical hidden semi Markov generative models also used first determine facts discuss generate words predicates arguments chosen facts Liang Sentence planning formulated supervised set partitioning problem facts partition corresponds sentence Barzilay Lapata End end approaches combined sentence planning surface realization using explicitly aligned sentence meaning pairs training data Ratnaparkhi Wong Mooney Belz More recently content selection surface realization combined Angeli Kim Mooney Konstas Lapata intersection rule based statistical methods hybrid systems aim leveraging human contributed rules corpus statistics Langkilde Knight Soricut Marcu Mairesse Walker Our approach inspired recent success neural language models image captioning Kiros Karpathy Fei Fei Vinyals Fang machine translation Devlin Bahdanau Luong modeling conversations dialogues Shang Wen Yao Our model similar Mei use encoder decoder style neural network model tackle EATHERGOV ROBOCUP tasks Their architecture relies LSTM units attention mechanism reduces scalability compared simpler design Figure Wikipedia infobox Frederick Parker Rhodes The introduction article reads Frederick Parker Rhodes March November English linguist plant pathologist computer scientist mathematician mystic mycologist Language Modeling Constrained Sentence generation Conditional language models popular choice generate sentences introduce tableconditioned language model constraining text generation include elements fact tables Language model Given sentence words vocabulary language model estimates Let sequence context words preceding gram language model makes order Markov assumption Language model conditioned tables table set field value pairs values sequences words therefore propose language models conditioned pairs Local conditioning refers information table applied description words already generated previous words constitute context language input text zct Doe April unk name Table name birthdate birthplace occupation spouse children John John Doe April Oxford placeholder Jane Doe Johnnie Doe zct name spouse children birthd birthd birthd output candidates april placeholder john doe unk birthd occupation name name spouse children Figure Table features right example table left set output words defined Section model The table allows describe word string index vocabulary also descriptor occurrence table Let define set possible fields The occurrence word table described set field position pairs number occurrences Each pair indicates occurs field position scheme words described empty set occur table For example word linguistics table Figure described follows zlinguistics fields known assuming words lower cased commas treated separate tokens Conditioning field type position within field allows model encode field specific regularities number token date field likely followed month token knowing number first token date field makes even likely The field position description scheme table allow express token terminates field useful capture field transitions For biographies last token name field often followed introduction birth date like born hence extend descriptor triplet includes position token counted end field example becomes zlinguistics fields known extend Equation use information additional conditioning context generating sentence zct zct zwt zwt referred local conditioning variables since describe local context previous word relations table Global conditioning refers information tokens fields table regardless whether appear previous generated words The set fields available table often impacts structure generation For biographies fields used describe politician different ones actor athlete introduce global conditioning available fields zct Similarly global conditioning available words occurring table introduced zct Tokens provide information complementary fields For example may hard distinguish basketball player hockey player looking field names teams league position weight height etc However actual field tokens team names league name player position help model give better prediction Here binary indicators fixed field word vocabularies Figure illustrates model schematic example For predicting next word given context language model conditioned sets triplets word occurring table zct along fields words table Copy actions far extended model conditioning features derived fact table turn using table information scoring output words particular sentences express facts given table often copy words table therefore extend model also score special field tokens name name subsequently added score corresponding words field value Our model reads table defines output domain defines tokens table might include vocabulary words For instance Park Rhodes Figure However Park Rhodes included name since second token name field allows model generate This mechanism inspired recent work attention based word copying neural machine translation Luong well delexicalization neural dialog systems Wen also builds upon older work class based language models dialog systems Rudnicky Neural Language Model Approach feed forward neural language model NLM estimates parametric function Equation refers learnable parameters network This function composition simple differentiable functions layers Mathematical notations layers denote matrices bold upper case letters vectors bold lower case letters represents ith row matrix When matrix represents vector ith first dimension second dimension Unless otherwise stated vectors assumed column vectors use denote vector concatenation Next introduce notation different layers used approach Embedding layer Given parameter matrix embedding layer lookup table performs array indexing operation corresponds embedding element row When matrix lookup table takes two arguments corresponds embedding pair index The lookup table operation applied sequence elements common approach concatenate resulting embeddings Linear layer This layer applies linear transformation inputs trainable parameters weight matrix bias term Softmax layer Given context input final layer outputs score word The probability distribution obtained applying softmax activation function exp exp Embeddings inputs key aspect neural language models use word embeddings Similar words tend similar embeddings thus share latent features The probability estimates models smooth functions embeddings small change features results small change probability estimates Bengio Therefore neural language models achieve better generalization unseen grams Next show map fact tables continuous space similar spirit Word embeddings Formally embedding layer maps context word index continuous dimensional vector relies parameter matrix convert input vectors dimension initialized randomly pre trained word embeddings Table embeddings described Section language model conditioned elements table Embedding matrices therefore defined model local global conditioning information For local conditioning denote maximum length sequence words Each field associated vectors dimensions first vectors embed possible starting positions remaining vectors embed ending positions This results two parameter matrices For given triplet refer embedding vectors start end position field respectively Finally global conditioning uses two parameter matrices maps table field vector dimension maps word vector dimension general shares parameters provided Aggregating embeddings represent occurence word triplet field start end embeddings start end position described Often times particular word occurs multiple times table lin guistics two instances Figure case perform component wise max start embeddings instances obtain best features across occurrences end position embeddings zwt max zwt max special field embedding assigned word associated fields embedding zct encoding local conditioning input obtained concatenation For global conditioning define set fields given table set words also perform max aggregation This yields vectors max max The final embedding encodes context input conditioning concatenation vectors zct zct For simplification purpose define context input zct following equations This context embedding mapped latent context representation using linear operation followed hyperbolic tangent tanh Rnhu Rnhu Rnhu vocabulary outputs The hidden representation context goes another linear layer produce real value score word vocabulary nhu framework evaluate model Wikipedia offering large diverse dataset Mixing outputs better copying Section explains word table also associated set fields occurs along position field Similar local conditioning represent field position pair embedding These embeddings projected space latent representation context input Rnhu Using max operation embedding dimension word finally embedded unique vector max tanh Rnhu Rnhu dot product context vector produces score word table Biography dataset introduce new dataset text generation IKI corpus articles English Wikipedia Sep comprises biography articles listed WikiProject Biography also table infobox extract tokenize first sentence article Stanford CoreNLP Manning All numbers mapped special token except years mapped different special token Field values tables similarly tokenized All tokens lower cased Table summarizes dataset statistics average first sentence twice short table tokens third sentence tokens also occur table The final corpus divided three sub parts provide training validation test sets The dataset available download Baseline Our baseline interpolated Kneser Ney language model use KenLM toolkit train gram models without pruning Heafield also learn language model templates For purpose reQ place words occurring table The softmax function maps scores training sentences special token reflecting table descriptor Equation The distribution troduction section table Figure looks log log exp follows scheme name name birthdate birthdate birthdate deathdate deathdate deathdate Training english linguist fields pathologist The neural language model trained minimize fields scientist mathematician mystic negative log likelihood training sentence mycologist During inference decoder conwith stochastic gradient descent SGD LeCun strained emit words regular vocabulary special tokens occurring input table When picking special token copy corresponding log zct word table Each word receives final score summing vocabulary score field score Experiments Our neural network model Section designed generate sentences tables large scale problems diverse set sentence types need generated Biographies therefore good Training setup For neural models train gram language models learning rate set Wikipedia WikiProject_Biography wikipedia biography dataset Model Perplexity BLEU ROUGE NIST NLM Local field start end Template Table NLM Local field start Local field start end Global field Global field word Table BLEU ROUGE NIST perplexity without copy actions first three rows copy actions last five rows For neural models report mean standard deviation five training runs different initialization Decoding beam width Perplexities marked directly comparable output vocabularies differ slightly Mean tokens per sentence tokens per table table tokens per sent fields per table shown helpful various applications Lebret Collobert Percentile use different metrics evaluate models Performance first evaluated terms perplexity standard metric language modeling Generation quality assessed automatically BLEU ROUGE measure NIST Belz Reiter Table Dataset statistics Parameter Value word types field types Max tokens field word field embedding size global embedding size hidden units nhu Evaluation metrics Table Model Hyperparameters Table describes hyper parameters include fields occurring least times training data set fields include frequent words vocabulary The hyperparameters set validation maximizing BLEU validation subset sentences Similarly early stopping applied training ends BLEU stops improving validation subset One note maximum number tokens field means encode positions longer field values final tokens dropped position capped initialize word embeddings Hellinger PCA computed set training biographies This representation Results This section describes results discusses impact different conditioning variables The better The results Table show conditioning information helps improve performance models The generation metrics BLEU ROUGE NIST gives performance ordering models first discuss models without copy actions first three results discuss models copy actions remaining results Note factorization models results three different output domains makes perplexity comparisons less straightforward models without copy actions operate fixed vocabulary Template adds fixed set field position pairs vocabulary Table NLM models variable set depending input table see Section Without copy actions terms perplexity neural language model NLM slightly better rely standard software NIST mteval NIST BLEU MSR rouge ROUGE tio upa occ alit ion lac bir thp bir dat nellie wong born september american poet activist Figure Visualization attention scores Nellie Wong Wikipedia infobox Each row represents probability distribution field position pairs given previous words words heading preceding rows well current row Darker colors depict higher probabilities BLEU Template Table NLM beam size nam interpolated language model adding local conditioning field start end position improves accuracy Generation metrics generally low clear improvement using local conditioning since allows learn transitions fields linking previous predictions table unlike plain NLM With copy actions For experiments copy actions use full local conditioning Equation neural language models BLEU ROUGE NIST improves moving Template Table NLM features successively improve accuracy Global conditioning fields improves model BLEU adding words gives additional BLEU This total improvement nearly BLEU Template Kneser Ney baseline Similar observations made ROUGE NIST time Figure Comparison best model Table NLM baseline Template different beam sizes The axis average timing milliseconds generating one sentence The axis BLEU score All results measured subset samples validation set Attention mechanism Our model implements attention input table fields For word table Equation takes language model score adds bias ct The bias dot product representation table field occurs representation context Equation summarizes previously generated fields words Figure shows mechanism adds large bias continue field generated tokens table emits word occurring name generating name also nicely handles transitions field types model adds large bias words occurring occupation field emitting birthdate Sentence decoding use standard beam search explore larger set sentences compared simple greedy search This allows explore times paths comes linear increase number forward computation steps language model compare various beam settings baseline Template Table NLM Figure The best validation BLEU obtained beam size Our model also several times faster baseline requiring per sentence Beam search generates many ngram lookups Kneser Ney requires many Model Generated Sentence Reference frederick parker rhodes march november english linguist plant pathologist computer scientist mathematician mystic mycologist Baseline Template frederick parker rhodes born november march english cricketer Table NLM Local field start frederick parker rhodes november march australian rules footballer played carlton victorian football league vfl XXXXs XXXXs Global field frederick parker rhodes november march english mycology plant pathology mathematics university Global field word frederick parker rhodes november march british computer scientist best known contributions computational linguistics Table First sentence current Wikipedia article Frederick Parker Rhodes sentences generated three versions table conditioned neural language model Table NLM using Wikipedia infobox seen Figure random memory accesses neural models perform scoring matrix matrix products operation local performed block parallel manner modern graphic processors shine Kindratenko Qualitative analysis Table shows generations different variants model based Wikipedia table Figure First comparing reference fact table reveals training data perfect The birth month mentioned fact table first sentence Wikipedia article different may introduced one contributor editing article keeping information consistent All three versions model correctly generate beginning sentence copying name birth date death date table The model correctly uses past tense since death date table indicates person passed away Frederick Parker Rhodes scientist occupation directly mentioned table The model without global conditioning therefore predict right occupation continues generation common occupation Wikipedia person died contrast global conditioning fields helps model understand person indeed scientist However global conditioning words model infer correct occupation computer scientist Conclusions shown model generate fluent descriptions arbitrary people based structured data Local global conditioning improves model large margin outperform Kneser Ney language model nearly BLEU Our task uses order magnitude data previous work vocabulary three orders magnitude larger paper focused generating first sentence tackle generation longer biographies future work Also encoding field values improved Currently attach field type token position word type perform max pooling local conditioning One could leverage richer representation learning encoder conditioned field type recurrent encoder convolutional encoder different pooling strategies Furthermore current training loss function explicitly penalize model generating incorrect facts predicting incorrect nationality occupation currently considered worse choosing incorrect determiner loss function could assess factual accuracy would certainly improve sentence generation avoiding mistakes Also important define strategy evaluating factual accuracy generation beyond BLEU ROUGE NIST References Angeli Liang Klein simple domain independent probabilistic approach generation Proceedings Conference Empirical Methods Natural Language Processing pages Association Computational Linguistics Bahdanau Cho Bengio Neural machine translation jointly learning align translate International Conference Learning Representations Barzilay Lapata Collective content selection concept text generation Proceedings conference Human Language Technology Empirical Methods Natural Language Processing pages Barzilay Lapata Aggregation via set partitioning natural language generation Proceedings main conference Human Language Technology Conference North American Chapter Association Computational Linguistics pages Association Computational Linguistics Belz Reiter Comparing automatic human evaluation nlg systems Proc EACL pages Belz Automatic generation weather forecast texts using comprehensive probabilistic generationspace models Natural Language Engineering Bengio Ducharme Vincent Jauvin neural probabilistic language model Journal Machine Learning Research Bollacker Evans Paritosh Sturge Taylor Freebase collaboratively created graph database structuring human knowledge International Conference Management Data pages ACM Dale Geldof Prost Coral Using natural language generation navigational assistance Proceedings Australasian computer science conference Volume pages Australian Computer Society Inc Devlin Zbib Huang Lamar Schwartz Makhoul Fast robust neural network joint models statistical machine translation Proceedings Annual Meeting Association Computational Linguistics volume pages Duboue McKeown Content planner construction via evolutionary algorithms corpus based fitness function Proceedings INLG pages Fang Gupta Iandola Srivastava Deng Dollar Gao Mitchell Platt Zitnick Zweig From captions visual concepts back The IEEE Conference Computer Vision Pattern Recognition CVPR June Ferrucci Introduction watson IBM Journal Research Development Galanis Androutsopoulos Generating multilingual descriptions linguistically annotated owl ontologies naturalowl system Proceedings Eleventh European Workshop Natural Language Generation pages Association Computational Linguistics Green Generation biomedical arguments lay readers Proceedings Fourth International Natural Language Generation Conference pages Association Computational Linguistics Gyawali Gardent Surface realisation knowledge bases Proc ACL Heafield Pouzyrevsky Clark Koehn Scalable modified Kneser Ney language model estimation Proceedings Annual Meeting Association Computational Linguistics pages Sofia Bulgaria August Karpathy Fei Fei Deep visual semantic alignments generating image descriptions The IEEE Conference Computer Vision Pattern Recognition CVPR June Kim Mooney Generative alignment semantic parsing learning ambiguous supervision Proceedings International Conference Computational Linguistics Posters pages Association Computational Linguistics Kindratenko Numerical Computations GPUs Springer Kiros Salakhutdinov Zemel Unifying visual semantic embeddings multimodal neural language models arXiv preprint arXiv Konstas Lapata global model concept text generation Artif Int Res October Langkilde Knight Generation exploits corpus based statistical knowledge Proc ACL pages Lebret Collobert Word embeddings hellinger pca Proceedings Conference European Chapter Association Computational Linguistics pages Gothenburg Sweden April Association Computational Linguistics LeCun Bottou Orr ller Efficient backprop Neural networks Tricks trade pages Springer Liang Jordan Klein Learning semantic correspondences less supervision Proceedings Joint Conference Annual Meeting ACL International Joint Conference Natural Language Processing AFNLP Volume Volume pages Association Computational Linguistics probabilistic forestto string model language generation typed lambda calculus expressions Proceedings Conference Empirical Methods Natural Language Processing pages Association Computational Linguistics Luong Sutskever Vinyals Zaremba Addressing rare word problem neural machine translation Proc ACL pages Mairesse Walker Controlling user perceptions linguistic style Trainable generation personality traits Comput Linguist Manning Surdeanu Bauer Finkel Bethard McClosky The Stanford CoreNLP natural language processing toolkit Association Computational Linguistics ACL System Demonstrations pages Mei Bansal Walter What talk selective generation using lstms coarse fine alignment Proceedings Human Language Technologies The Annual Conference North American Chapter Association Computational Linguistics Rudnicky Stochastic language generation spoken dialogue systems ANLP NAACL Workshop Conversational Systems pages Ratnaparkhi Trainable approaches surface natural language generation application conversational dialog systems Computer Speech Language Reiter Dale Feng Building natural language generation systems volume MIT Press Reiter Sripada Hunter Davy Choosing words computer generated weather forecasts Artificial Intelligence Shang Neural responding machine short text conversation arXiv preprint arXiv Radu Soricut Daniel Marcu Stochastic language generation using widl expressions application machine translation summarization Proc ACL pages Turner Sripada Reiter Generating approximate geographic descriptions Empirical methods natural language generation pages Springer Vinyals Toshev Bengio Erhan Show tell neural image caption generator The IEEE Conference Computer Vision Pattern Recognition CVPR June Wen Gasic Mrks Vandyke Young Semantically conditioned lstmbased natural language generation spoken dialogue systems Proceedings Conference Empirical Methods Natural Language Processing pages Lisbon Portugal September Association Computational Linguistics Wong Mooney Generation inverting semantic parser uses statistical machine translation HLT NAACL pages Kiros Courville Salakhutdinov Zemel Bengio Show attend tell Neural image caption generation visual attention Proceedings The International Conference Machine Learning volume July Yao Zweig Peng Attention intention neural network conversation model arXiv preprint arXiv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def preprocessing(convertpdftotext):\n",
    "    text = doc.decode('utf8') #convert to byte\n",
    "    removing = text.replace('\\n','') #remove the production of textract\n",
    "    removing = text.replace('\\r','') #remove the production of extract\n",
    "    sentence=str(removing)\n",
    "    sentence=sentence.replace('{html}',\"\") \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(rem_num)  \n",
    "    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
    "    referenceremoval = text.partition(\"References\")[0]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "cleandata = preprocessing(doc)\n",
    "print(cleandata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Text Generation Structured Data Application Biography Domain Lebret EPFL Switzerland David Grangier Facebook Research Abstract This paper introduces neural model concept text generation scales large rich domains generates biographical sentences fact tables new dataset biographies Wikipedia This set order magnitude larger existing resources samples vocabulary Our model builds conditional neural language models text generation deal large vocabulary extend models mix fixed vocabulary copy actions transfer sample specific words input database generated output sentence deal structured data allow model embed words differently depending data fields occur Our neural model significantly outperforms Templated Kneser Ney language model nearly BLEU Introduction Concept text generation renders structured records natural language Reiter typical application generate weather forecast based set structured meteorological measurements contrast previous work scale large diverse problem generating biographies based Wikipedia infoboxes infobox fact table describing person similar person subgraph knowledge base Bollacker Ferrucci Similar generation applications include generation product descriptions based catalog millions items dozens attributes Previous work experimented datasets contain tens thousands records EATHERGOV ROBOCUP dataset dataset contains biographies performed work interning Facebook Michael Auli Facebook Research Wikipedia Furthermore datasets limited vocabulary words compared words dataset tackle problem introduce statistical generation model conditioned Wikipedia infobox focus generation first sentence biography requires model select among large number possible fields generate adequate output Such diversity makes difficult classical count based models estimate probabilities rare events due data sparsity address issue parameterizing words fields embeddings along neural language model operating Bengio This factorization allows scale larger number words fields Liang Kim Mooney number parameters grows product number words fields Moreover approach restrict relations field contents generated text This contrasts less flexible strategies assume generation follow either hybrid alignment tree Kim Mooney probabilistic context free grammar Konstas Lapata tree adjoining grammar Gyawali Gardent Our model exploits structured data globally locally Global conditioning summarizes information personality understand highlevel themes biography scientist artist local conditioning describes previously generated tokens terms relationship infobox analyze effectiveness demonstrate complementarity Related Work Traditionally generation systems relied rules hand crafted specifications Dale Reiter Green Galanis Androut Proceedings Conference Empirical Methods Natural Language Processing pages Austin Texas November Association Computational Linguistics sopoulos Turner Generation divided modular yet highly interdependent decisions content planning defines parts input fields meaning representations selected sentence planning determines selected fields dealt output sentence surface realization generates sentences Data driven approaches proposed automatically learn individual modules One approach first aligns records sentences learns content selection model Duboue McKeown Barzilay Lapata Hierarchical hidden semi Markov generative models also used first determine facts discuss generate words predicates arguments chosen facts Liang Sentence planning formulated supervised set partitioning problem facts partition corresponds sentence Barzilay Lapata End end approaches combined sentence planning surface realization using explicitly aligned sentence meaning pairs training data Ratnaparkhi Wong Mooney Belz More recently content selection surface realization combined Angeli Kim Mooney Konstas Lapata intersection rule based statistical methods hybrid systems aim leveraging human contributed rules corpus statistics Langkilde Knight Soricut Marcu Mairesse Walker Our approach inspired recent success neural language models image captioning Kiros Karpathy Fei Fei Vinyals Fang machine translation Devlin Bahdanau Luong modeling conversations dialogues Shang Wen Yao Our model similar Mei use encoder decoder style neural network model tackle EATHERGOV ROBOCUP tasks Their architecture relies LSTM units attention mechanism reduces scalability compared simpler design Figure Wikipedia infobox Frederick Parker Rhodes The introduction article reads Frederick Parker Rhodes March November English linguist plant pathologist computer scientist mathematician mystic mycologist Language Modeling Constrained Sentence generation Conditional language models popular choice generate sentences introduce tableconditioned language model constraining text generation include elements fact tables Language model Given sentence words vocabulary language model estimates Let sequence context words preceding gram language model makes order Markov assumption Language model conditioned tables table set field value pairs values sequences words therefore propose language models conditioned pairs Local conditioning refers information table applied description words already generated previous words constitute context language input text zct Doe April unk name Table name birthdate birthplace occupation spouse children John John Doe April Oxford placeholder Jane Doe Johnnie Doe zct name spouse children birthd birthd birthd output candidates april placeholder john doe unk birthd occupation name name spouse children Figure Table features right example table left set output words defined Section model The table allows describe word string index vocabulary also descriptor occurrence table Let define set possible fields The occurrence word table described set field position pairs number occurrences Each pair indicates occurs field position scheme words described empty set occur table For example word linguistics table Figure described follows zlinguistics fields known assuming words lower cased commas treated separate tokens Conditioning field type position within field allows model encode field specific regularities number token date field likely followed month token knowing number first token date field makes even likely The field position description scheme table allow express token terminates field useful capture field transitions For biographies last token name field often followed introduction birth date like born hence extend descriptor triplet includes position token counted end field example becomes zlinguistics fields known extend Equation use information additional conditioning context generating sentence zct zct zwt zwt referred local conditioning variables since describe local context previous word relations table Global conditioning refers information tokens fields table regardless whether appear previous generated words The set fields available table often impacts structure generation For biographies fields used describe politician different ones actor athlete introduce global conditioning available fields zct Similarly global conditioning available words occurring table introduced zct Tokens provide information complementary fields For example may hard distinguish basketball player hockey player looking field names teams league position weight height etc However actual field tokens team names league name player position help model give better prediction Here binary indicators fixed field word vocabularies Figure illustrates model schematic example For predicting next word given context language model conditioned sets triplets word occurring table zct along fields words table Copy actions far extended model conditioning features derived fact table turn using table information scoring output words particular sentences express facts given table often copy words table therefore extend model also score special field tokens name name subsequently added score corresponding words field value Our model reads table defines output domain defines tokens table might include vocabulary words For instance Park Rhodes Figure However Park Rhodes included name since second token name field allows model generate This mechanism inspired recent work attention based word copying neural machine translation Luong well delexicalization neural dialog systems Wen also builds upon older work class based language models dialog systems Rudnicky Neural Language Model Approach feed forward neural language model NLM estimates parametric function Equation refers learnable parameters network This function composition simple differentiable functions layers Mathematical notations layers denote matrices bold upper case letters vectors bold lower case letters represents ith row matrix When matrix represents vector ith first dimension second dimension Unless otherwise stated vectors assumed column vectors use denote vector concatenation Next introduce notation different layers used approach Embedding layer Given parameter matrix embedding layer lookup table performs array indexing operation corresponds embedding element row When matrix lookup table takes two arguments corresponds embedding pair index The lookup table operation applied sequence elements common approach concatenate resulting embeddings Linear layer This layer applies linear transformation inputs trainable parameters weight matrix bias term Softmax layer Given context input final layer outputs score word The probability distribution obtained applying softmax activation function exp exp Embeddings inputs key aspect neural language models use word embeddings Similar words tend similar embeddings thus share latent features The probability estimates models smooth functions embeddings small change features results small change probability estimates Bengio Therefore neural language models achieve better generalization unseen grams Next show map fact tables continuous space similar spirit Word embeddings Formally embedding layer maps context word index continuous dimensional vector relies parameter matrix convert input vectors dimension initialized randomly pre trained word embeddings Table embeddings described Section language model conditioned elements table Embedding matrices therefore defined model local global conditioning information For local conditioning denote maximum length sequence words Each field associated vectors dimensions first vectors embed possible starting positions remaining vectors embed ending positions This results two parameter matrices For given triplet refer embedding vectors start end position field respectively Finally global conditioning uses two parameter matrices maps table field vector dimension maps word vector dimension general shares parameters provided Aggregating embeddings represent occurence word triplet field start end embeddings start end position described Often times particular word occurs multiple times table lin guistics two instances Figure case perform component wise max start embeddings instances obtain best features across occurrences end position embeddings zwt max zwt max special field embedding assigned word associated fields embedding zct encoding local conditioning input obtained concatenation For global conditioning define set fields given table set words also perform max aggregation This yields vectors max max The final embedding encodes context input conditioning concatenation vectors zct zct For simplification purpose define context input zct following equations This context embedding mapped latent context representation using linear operation followed hyperbolic tangent tanh Rnhu Rnhu Rnhu vocabulary outputs The hidden representation context goes another linear layer produce real value score word vocabulary nhu framework evaluate model Wikipedia offering large diverse dataset Mixing outputs better copying Section explains word table also associated set fields occurs along position field Similar local conditioning represent field position pair embedding These embeddings projected space latent representation context input Rnhu Using max operation embedding dimension word finally embedded unique vector max tanh Rnhu Rnhu dot product context vector produces score word table Biography dataset introduce new dataset text generation IKI corpus articles English Wikipedia Sep comprises biography articles listed WikiProject Biography also table infobox extract tokenize first sentence article Stanford CoreNLP Manning All numbers mapped special token except years mapped different special token Field values tables similarly tokenized All tokens lower cased Table summarizes dataset statistics average first sentence twice short table tokens third sentence tokens also occur table The final corpus divided three sub parts provide training validation test sets The dataset available download Baseline Our baseline interpolated Kneser Ney language model use KenLM toolkit train gram models without pruning Heafield also learn language model templates For purpose reQ place words occurring table The softmax function maps scores training sentences special token reflecting table descriptor Equation The distribution troduction section table Figure looks log log exp follows scheme name name birthdate birthdate birthdate deathdate deathdate deathdate Training english linguist fields pathologist The neural language model trained minimize fields scientist mathematician mystic negative log likelihood training sentence mycologist During inference decoder conwith stochastic gradient descent SGD LeCun strained emit words regular vocabulary special tokens occurring input table When picking special token copy corresponding log zct word table Each word receives final score summing vocabulary score field score Experiments Our neural network model Section designed generate sentences tables large scale problems diverse set sentence types need generated Biographies therefore good Training setup For neural models train gram language models learning rate set Wikipedia WikiProject_Biography wikipedia biography dataset Model Perplexity BLEU ROUGE NIST NLM Local field start end Template Table NLM Local field start Local field start end Global field Global field word Table BLEU ROUGE NIST perplexity without copy actions first three rows copy actions last five rows For neural models report mean standard deviation five training runs different initialization Decoding beam width Perplexities marked directly comparable output vocabularies differ slightly Mean tokens per sentence tokens per table table tokens per sent fields per table shown helpful various applications Lebret Collobert Percentile use different metrics evaluate models Performance first evaluated terms perplexity standard metric language modeling Generation quality assessed automatically BLEU ROUGE measure NIST Belz Reiter Table Dataset statistics Parameter Value word types field types Max tokens field word field embedding size global embedding size hidden units nhu Evaluation metrics Table Model Hyperparameters Table describes hyper parameters include fields occurring least times training data set fields include frequent words vocabulary The hyperparameters set validation maximizing BLEU validation subset sentences Similarly early stopping applied training ends BLEU stops improving validation subset One note maximum number tokens field means encode positions longer field values final tokens dropped position capped initialize word embeddings Hellinger PCA computed set training biographies This representation Results This section describes results discusses impact different conditioning variables The better The results Table show conditioning information helps improve performance models The generation metrics BLEU ROUGE NIST gives performance ordering models first discuss models without copy actions first three results discuss models copy actions remaining results Note factorization models results three different output domains makes perplexity comparisons less straightforward models without copy actions operate fixed vocabulary Template adds fixed set field position pairs vocabulary Table NLM models variable set depending input table see Section Without copy actions terms perplexity neural language model NLM slightly better rely standard software NIST mteval NIST BLEU MSR rouge ROUGE tio upa occ alit ion lac bir thp bir dat nellie wong born september american poet activist Figure Visualization attention scores Nellie Wong Wikipedia infobox Each row represents probability distribution field position pairs given previous words words heading preceding rows well current row Darker colors depict higher probabilities BLEU Template Table NLM beam size nam interpolated language model adding local conditioning field start end position improves accuracy Generation metrics generally low clear improvement using local conditioning since allows learn transitions fields linking previous predictions table unlike plain NLM With copy actions For experiments copy actions use full local conditioning Equation neural language models BLEU ROUGE NIST improves moving Template Table NLM features successively improve accuracy Global conditioning fields improves model BLEU adding words gives additional BLEU This total improvement nearly BLEU Template Kneser Ney baseline Similar observations made ROUGE NIST time Figure Comparison best model Table NLM baseline Template different beam sizes The axis average timing milliseconds generating one sentence The axis BLEU score All results measured subset samples validation set Attention mechanism Our model implements attention input table fields For word table Equation takes language model score adds bias ct The bias dot product representation table field occurs representation context Equation summarizes previously generated fields words Figure shows mechanism adds large bias continue field generated tokens table emits word occurring name generating name also nicely handles transitions field types model adds large bias words occurring occupation field emitting birthdate Sentence decoding use standard beam search explore larger set sentences compared simple greedy search This allows explore times paths comes linear increase number forward computation steps language model compare various beam settings baseline Template Table NLM Figure The best validation BLEU obtained beam size Our model also several times faster baseline requiring per sentence Beam search generates many ngram lookups Kneser Ney requires many Model Generated Sentence Reference frederick parker rhodes march november english linguist plant pathologist computer scientist mathematician mystic mycologist Baseline Template frederick parker rhodes born november march english cricketer Table NLM Local field start frederick parker rhodes november march australian rules footballer played carlton victorian football league vfl XXXXs XXXXs Global field frederick parker rhodes november march english mycology plant pathology mathematics university Global field word frederick parker rhodes november march british computer scientist best known contributions computational linguistics Table First sentence current Wikipedia article Frederick Parker Rhodes sentences generated three versions table conditioned neural language model Table NLM using Wikipedia infobox seen Figure random memory accesses neural models perform scoring matrix matrix products operation local performed block parallel manner modern graphic processors shine Kindratenko Qualitative analysis Table shows generations different variants model based Wikipedia table Figure First comparing reference fact table reveals training data perfect The birth month mentioned fact table first sentence Wikipedia article different may introduced one contributor editing article keeping information consistent All three versions model correctly generate beginning sentence copying name birth date death date table The model correctly uses past tense since death date table indicates person passed away Frederick Parker Rhodes scientist occupation directly mentioned table The model without global conditioning therefore predict right occupation continues generation common occupation Wikipedia person died contrast global conditioning fields helps model understand person indeed scientist However global conditioning words model infer correct occupation computer scientist Conclusions shown model generate fluent descriptions arbitrary people based structured data Local global conditioning improves model large margin outperform Kneser Ney language model nearly BLEU Our task uses order magnitude data previous work vocabulary three orders magnitude larger paper focused generating first sentence tackle generation longer biographies future work Also encoding field values improved Currently attach field type token position word type perform max pooling local conditioning One could leverage richer representation learning encoder conditioned field type recurrent encoder convolutional encoder different pooling strategies Furthermore current training loss function explicitly penalize model generating incorrect facts predicting incorrect nationality occupation currently considered worse choosing incorrect determiner loss function could assess factual accuracy would certainly improve sentence generation avoiding mistakes Also important define strategy evaluating factual accuracy generation beyond BLEU ROUGE NIST References Angeli Liang Klein simple domain independent probabilistic approach generation Proceedings Conference Empirical Methods Natural Language Processing pages Association Computational Linguistics Bahdanau Cho Bengio Neural machine translation jointly learning align translate International Conference Learning Representations Barzilay Lapata Collective content selection concept text generation Proceedings conference Human Language Technology Empirical Methods Natural Language Processing pages Barzilay Lapata Aggregation via set partitioning natural language generation Proceedings main conference Human Language Technology Conference North American Chapter Association Computational Linguistics pages Association Computational Linguistics Belz Reiter Comparing automatic human evaluation nlg systems Proc EACL pages Belz Automatic generation weather forecast texts using comprehensive probabilistic generationspace models Natural Language Engineering Bengio Ducharme Vincent Jauvin neural probabilistic language model Journal Machine Learning Research Bollacker Evans Paritosh Sturge Taylor Freebase collaboratively created graph database structuring human knowledge International Conference Management Data pages ACM Dale Geldof Prost Coral Using natural language generation navigational assistance Proceedings Australasian computer science conference Volume pages Australian Computer Society Inc Devlin Zbib Huang Lamar Schwartz Makhoul Fast robust neural network joint models statistical machine translation Proceedings Annual Meeting Association Computational Linguistics volume pages Duboue McKeown Content planner construction via evolutionary algorithms corpus based fitness function Proceedings INLG pages Fang Gupta Iandola Srivastava Deng Dollar Gao Mitchell Platt Zitnick Zweig From captions visual concepts back The IEEE Conference Computer Vision Pattern Recognition CVPR June Ferrucci Introduction watson IBM Journal Research Development Galanis Androutsopoulos Generating multilingual descriptions linguistically annotated owl ontologies naturalowl system Proceedings Eleventh European Workshop Natural Language Generation pages Association Computational Linguistics Green Generation biomedical arguments lay readers Proceedings Fourth International Natural Language Generation Conference pages Association Computational Linguistics Gyawali Gardent Surface realisation knowledge bases Proc ACL Heafield Pouzyrevsky Clark Koehn Scalable modified Kneser Ney language model estimation Proceedings Annual Meeting Association Computational Linguistics pages Sofia Bulgaria August Karpathy Fei Fei Deep visual semantic alignments generating image descriptions The IEEE Conference Computer Vision Pattern Recognition CVPR June Kim Mooney Generative alignment semantic parsing learning ambiguous supervision Proceedings International Conference Computational Linguistics Posters pages Association Computational Linguistics Kindratenko Numerical Computations GPUs Springer Kiros Salakhutdinov Zemel Unifying visual semantic embeddings multimodal neural language models arXiv preprint arXiv Konstas Lapata global model concept text generation Artif Int Res October Langkilde Knight Generation exploits corpus based statistical knowledge Proc ACL pages Lebret Collobert Word embeddings hellinger pca Proceedings Conference European Chapter Association Computational Linguistics pages Gothenburg Sweden April Association Computational Linguistics LeCun Bottou Orr ller Efficient backprop Neural networks Tricks trade pages Springer Liang Jordan Klein Learning semantic correspondences less supervision Proceedings Joint Conference Annual Meeting ACL International Joint Conference Natural Language Processing AFNLP Volume Volume pages Association Computational Linguistics probabilistic forestto string model language generation typed lambda calculus expressions Proceedings Conference Empirical Methods Natural Language Processing pages Association Computational Linguistics Luong Sutskever Vinyals Zaremba Addressing rare word problem neural machine translation Proc ACL pages Mairesse Walker Controlling user perceptions linguistic style Trainable generation personality traits Comput Linguist Manning Surdeanu Bauer Finkel Bethard McClosky The Stanford CoreNLP natural language processing toolkit Association Computational Linguistics ACL System Demonstrations pages Mei Bansal Walter What talk selective generation using lstms coarse fine alignment Proceedings Human Language Technologies The Annual Conference North American Chapter Association Computational Linguistics Rudnicky Stochastic language generation spoken dialogue systems ANLP NAACL Workshop Conversational Systems pages Ratnaparkhi Trainable approaches surface natural language generation application conversational dialog systems Computer Speech Language Reiter Dale Feng Building natural language generation systems volume MIT Press Reiter Sripada Hunter Davy Choosing words computer generated weather forecasts Artificial Intelligence Shang Neural responding machine short text conversation arXiv preprint arXiv Radu Soricut Daniel Marcu Stochastic language generation using widl expressions application machine translation summarization Proc ACL pages Turner Sripada Reiter Generating approximate geographic descriptions Empirical methods natural language generation pages Springer Vinyals Toshev Bengio Erhan Show tell neural image caption generator The IEEE Conference Computer Vision Pattern Recognition CVPR June Wen Gasic Mrks Vandyke Young Semantically conditioned lstmbased natural language generation spoken dialogue systems Proceedings Conference Empirical Methods Natural Language Processing pages Lisbon Portugal September Association Computational Linguistics Wong Mooney Generation inverting semantic parser uses statistical machine translation HLT NAACL pages Kiros Courville Salakhutdinov Zemel Bengio Show attend tell Neural image caption generation visual attention Proceedings The International Conference Machine Learning volume July Yao Zweig Peng Attention intention neural network conversation model arXiv preprint arXiv\n"
     ]
    }
   ],
   "source": [
    "import spacy #import library\n",
    "nlp = spacy.load('en_core_web_sm') #load model which is sm small model and en means english\n",
    "data = nlp(cleandata) \n",
    "print(data)\n",
    "# print(type(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a rule-based system for extracting candidates for the first time (before manual filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N paper (2)\n",
      "ADJ+N neural model (2)\n",
      "N model (57)\n",
      "ADJ+N+N neural model concept (1)\n",
      "N+N model concept (2)\n",
      "N concept (3)\n",
      "N+N+N model concept text (2)\n",
      "N+N concept text (3)\n",
      "N text (10)\n",
      "N+N+N concept text generation (3)\n",
      "N+N text generation (7)\n",
      "N generation (34)\n",
      "ADJ+N rich domains (1)\n",
      "N domains (2)\n",
      "ADJ+N biographical sentences (1)\n",
      "N sentences (10)\n",
      "ADJ+N+N biographical sentences fact (1)\n",
      "N+N sentences fact (1)\n",
      "N fact (7)\n",
      "ADJ+N dataset biographies (1)\n",
      "N biographies (6)\n",
      "N order (3)\n",
      "N+N order magnitude (2)\n",
      "N magnitude (3)\n",
      "N resources (1)\n",
      "N+N resources samples (1)\n",
      "N samples (2)\n",
      "ADJ+N neural language (11)\n",
      "N language (42)\n",
      "ADJ+N+N neural language models (6)\n",
      "N+N language models (10)\n",
      "N models (28)\n",
      "N+N+N language models text (1)\n",
      "N+N models text (1)\n",
      "N+N+N models text generation (1)\n",
      "ADJ+N vocabulary extend (1)\n",
      "N extend (1)\n",
      "ADJ+N+N vocabulary extend models (1)\n",
      "N+N extend models (1)\n",
      "ADJ+N vocabulary copy (1)\n",
      "N copy (9)\n",
      "ADJ+N+N vocabulary copy actions (1)\n",
      "N+N copy actions (8)\n",
      "N actions (10)\n",
      "N sample (1)\n",
      "ADJ+N specific words (1)\n",
      "N words (36)\n",
      "ADJ+N+N specific words input (1)\n",
      "N+N words input (1)\n",
      "N input (12)\n",
      "N+N+N words input database (1)\n",
      "N+N input database (1)\n",
      "N database (2)\n",
      "N output (9)\n",
      "N+N output sentence (2)\n",
      "N sentence (22)\n",
      "N+N+N output sentence deal (1)\n",
      "N+N sentence deal (1)\n",
      "N deal (1)\n",
      "N data (9)\n",
      "N+N data fields (1)\n",
      "N fields (24)\n",
      "N+N language model (19)\n",
      "N records (2)\n",
      "ADJ+N natural language (8)\n",
      "ADJ+N+N natural language Reiter (1)\n",
      "N+N language Reiter (1)\n",
      "N Reiter (1)\n",
      "ADJ+N typical application (1)\n",
      "N application (3)\n",
      "ADJ+N+N typical application generate (1)\n",
      "N+N application generate (1)\n",
      "N generate (1)\n",
      "N+N+N application generate weather (1)\n",
      "N+N generate weather (1)\n",
      "N weather (3)\n",
      "N+N+N generate weather forecast (1)\n",
      "N+N weather forecast (2)\n",
      "N forecast (2)\n",
      "ADJ+N meteorological measurements (1)\n",
      "N measurements (1)\n",
      "ADJ+N previous work (2)\n",
      "N work (7)\n",
      "ADJ+N+N previous work scale (1)\n",
      "N+N work scale (1)\n",
      "N scale (3)\n",
      "ADJ+N diverse problem (1)\n",
      "N problem (4)\n",
      "ADJ+N+N diverse problem generating (1)\n",
      "N+N problem generating (1)\n",
      "N generating (1)\n",
      "N+N+N problem generating biographies (1)\n",
      "N+N generating biographies (1)\n",
      "N infobox (7)\n",
      "N+N infobox fact (1)\n",
      "N+N+N infobox fact table (1)\n",
      "N+N fact table (4)\n",
      "N table (55)\n",
      "N person (5)\n",
      "ADJ+N similar person (1)\n",
      "ADJ+N+N similar person subgraph (1)\n",
      "N+N person subgraph (1)\n",
      "N subgraph (1)\n",
      "N+N+N person subgraph knowledge (1)\n",
      "N+N subgraph knowledge (1)\n",
      "N knowledge (4)\n",
      "N+N+N subgraph knowledge base (1)\n",
      "N+N knowledge base (1)\n",
      "N base (1)\n",
      "ADJ+N Similar generation (1)\n",
      "ADJ+N+N Similar generation applications (1)\n",
      "N+N generation applications (1)\n",
      "N applications (2)\n",
      "N+N generation product (1)\n",
      "N product (4)\n",
      "N+N+N generation product descriptions (1)\n",
      "N+N product descriptions (1)\n",
      "N descriptions (5)\n",
      "N catalog (1)\n",
      "N+N catalog millions (1)\n",
      "N millions (1)\n",
      "N+N+N catalog millions items (1)\n",
      "N+N millions items (1)\n",
      "N items (1)\n",
      "N+N+N millions items dozens (1)\n",
      "N+N items dozens (1)\n",
      "N dozens (1)\n",
      "ADJ+N Previous work (1)\n",
      "N datasets (1)\n",
      "N tens (1)\n",
      "N+N tens thousands (1)\n",
      "N thousands (1)\n",
      "N+N+N tens thousands records (1)\n",
      "N+N thousands records (1)\n",
      "ADJ+N vocabulary words (2)\n",
      "ADJ+N dataset tackle (1)\n",
      "N tackle (2)\n",
      "ADJ+N+N dataset tackle problem (1)\n",
      "N+N tackle problem (1)\n",
      "ADJ+N statistical generation (1)\n",
      "ADJ+N+N statistical generation model (1)\n",
      "N+N generation model (1)\n",
      "N+N infobox focus (1)\n",
      "N focus (1)\n",
      "N+N+N infobox focus generation (1)\n",
      "N+N focus generation (1)\n",
      "ADJ+N first sentence (5)\n",
      "ADJ+N+N first sentence biography (1)\n",
      "N+N sentence biography (1)\n",
      "N biography (3)\n",
      "N+N model select (1)\n",
      "N select (1)\n",
      "ADJ+N large number (1)\n",
      "N number (9)\n",
      "ADJ+N possible fields (2)\n",
      "ADJ+N adequate output (1)\n",
      "ADJ+N Such diversity (1)\n",
      "N diversity (1)\n",
      "ADJ+N classical count (1)\n",
      "N count (1)\n",
      "N probabilities (2)\n",
      "ADJ+N rare events (1)\n",
      "N events (1)\n",
      "ADJ+N due data (1)\n",
      "ADJ+N+N due data sparsity (1)\n",
      "N+N data sparsity (1)\n",
      "N sparsity (1)\n",
      "N+N+N data sparsity address (1)\n",
      "N+N sparsity address (1)\n",
      "N address (1)\n",
      "N+N+N sparsity address issue (1)\n",
      "N+N address issue (1)\n",
      "N issue (1)\n",
      "N+N+N address issue parameterizing (1)\n",
      "N+N issue parameterizing (1)\n",
      "N parameterizing (1)\n",
      "N+N+N issue parameterizing words (1)\n",
      "N+N parameterizing words (1)\n",
      "N embeddings (15)\n",
      "ADJ+N+N neural language model (5)\n",
      "N factorization (2)\n",
      "ADJ+N larger number (1)\n",
      "ADJ+N+N larger number words (1)\n",
      "N+N number words (2)\n",
      "N+N number parameters (1)\n",
      "N parameters (5)\n",
      "N+N product number (1)\n",
      "N+N+N product number words (1)\n",
      "N approach (6)\n",
      "N+N approach restrict (1)\n",
      "N restrict (1)\n",
      "N+N+N approach restrict relations (1)\n",
      "N+N restrict relations (1)\n",
      "N relations (2)\n",
      "N+N+N restrict relations field (1)\n",
      "N+N relations field (1)\n",
      "N field (51)\n",
      "N+N+N relations field contents (1)\n",
      "N+N field contents (1)\n",
      "N contents (1)\n",
      "ADJ+N flexible strategies (1)\n",
      "N strategies (2)\n",
      "ADJ+N hybrid alignment (1)\n",
      "N alignment (3)\n",
      "ADJ+N+N hybrid alignment tree (1)\n",
      "N+N alignment tree (1)\n",
      "N tree (2)\n",
      "ADJ+N probabilistic context (1)\n",
      "N context (16)\n",
      "ADJ+N free grammar (1)\n",
      "N grammar (2)\n",
      "ADJ+N adjoining grammar (1)\n",
      "ADJ+N Global conditioning (3)\n",
      "N conditioning (27)\n",
      "N information (9)\n",
      "N+N information personality (1)\n",
      "N personality (2)\n",
      "N highlevel (1)\n",
      "N+N highlevel themes (1)\n",
      "N themes (1)\n",
      "N+N+N highlevel themes biography (1)\n",
      "N+N themes biography (1)\n",
      "N+N+N themes biography scientist (1)\n",
      "N+N biography scientist (1)\n",
      "N scientist (6)\n",
      "N+N+N biography scientist artist (1)\n",
      "N+N scientist artist (1)\n",
      "N artist (1)\n",
      "ADJ+N local conditioning (9)\n",
      "ADJ+N+N local conditioning describes (1)\n",
      "N+N conditioning describes (1)\n",
      "N describes (1)\n",
      "N tokens (16)\n",
      "N+N tokens terms (1)\n",
      "N terms (2)\n",
      "N+N+N tokens terms relationship (1)\n",
      "N+N terms relationship (1)\n",
      "N relationship (1)\n",
      "N+N+N terms relationship infobox (1)\n",
      "N+N relationship infobox (1)\n",
      "N+N+N relationship infobox analyze (1)\n",
      "N+N infobox analyze (1)\n",
      "N analyze (1)\n",
      "N+N+N infobox analyze effectiveness (1)\n",
      "N+N analyze effectiveness (1)\n",
      "N effectiveness (1)\n",
      "N+N+N analyze effectiveness demonstrate (1)\n",
      "N+N effectiveness demonstrate (1)\n",
      "N demonstrate (1)\n",
      "N+N+N effectiveness demonstrate complementarity (1)\n",
      "N+N demonstrate complementarity (1)\n",
      "N complementarity (1)\n",
      "N Work (1)\n",
      "N+N generation systems (2)\n",
      "N systems (8)\n",
      "N rules (3)\n",
      "N+N rules hand (1)\n",
      "N hand (1)\n",
      "N specifications (1)\n",
      "N pages (16)\n",
      "N sopoulos (1)\n",
      "ADJ+N interdependent decisions (1)\n",
      "N decisions (1)\n",
      "N planning (4)\n",
      "N parts (2)\n",
      "N+N parts input (1)\n",
      "N+N+N parts input fields (1)\n",
      "N+N input fields (1)\n",
      "N representations (1)\n",
      "N+N sentence planning (2)\n",
      "N+N+N output sentence surface (1)\n",
      "N+N sentence surface (1)\n",
      "N surface (3)\n",
      "N+N+N sentence surface realization (1)\n",
      "N+N surface realization (3)\n",
      "N realization (3)\n",
      "N approaches (1)\n",
      "ADJ+N individual modules (1)\n",
      "N modules (1)\n",
      "ADJ+N content selection (2)\n",
      "N selection (3)\n",
      "ADJ+N+N content selection model (1)\n",
      "N+N selection model (1)\n",
      "ADJ+N generative models (1)\n",
      "ADJ+N first determine (1)\n",
      "N determine (1)\n",
      "ADJ+N+N first determine facts (1)\n",
      "N+N determine facts (1)\n",
      "N facts (4)\n",
      "N arguments (3)\n",
      "N set (5)\n",
      "N+N set partitioning (1)\n",
      "N partitioning (1)\n",
      "N+N+N set partitioning problem (1)\n",
      "N+N partitioning problem (1)\n",
      "N partition (1)\n",
      "N+N partition corresponds (1)\n",
      "N corresponds (1)\n",
      "N+N+N partition corresponds sentence (1)\n",
      "N+N corresponds sentence (1)\n",
      "N End (1)\n",
      "N+N End end (1)\n",
      "N end (7)\n",
      "N+N+N sentence planning surface (1)\n",
      "N+N planning surface (1)\n",
      "N+N+N planning surface realization (1)\n",
      "N+N sentence meaning (1)\n",
      "N meaning (1)\n",
      "N training (9)\n",
      "N+N training data (2)\n",
      "ADJ+N+N content selection surface (1)\n",
      "N+N selection surface (1)\n",
      "N+N+N selection surface realization (1)\n",
      "N intersection (1)\n",
      "N+N intersection rule (1)\n",
      "N rule (1)\n",
      "ADJ+N statistical methods (1)\n",
      "N methods (2)\n",
      "ADJ+N+N statistical methods hybrid (1)\n",
      "N+N methods hybrid (1)\n",
      "N hybrid (1)\n",
      "N+N+N methods hybrid systems (1)\n",
      "N+N hybrid systems (1)\n",
      "N statistics (2)\n",
      "ADJ+N recent success (1)\n",
      "N success (1)\n",
      "N machine (8)\n",
      "N+N machine translation (7)\n",
      "N translation (7)\n",
      "N modeling (1)\n",
      "N+N modeling conversations (1)\n",
      "N conversations (1)\n",
      "N use (4)\n",
      "N+N use encoder (1)\n",
      "N encoder (4)\n",
      "N+N+N use encoder decoder (1)\n",
      "N+N encoder decoder (1)\n",
      "N decoder (2)\n",
      "N+N+N encoder decoder style (1)\n",
      "N+N decoder style (1)\n",
      "N style (2)\n",
      "N network (5)\n",
      "N+N network model (2)\n",
      "N architecture (1)\n",
      "ADJ+N LSTM units (1)\n",
      "N units (2)\n",
      "ADJ+N+N LSTM units attention (1)\n",
      "N+N units attention (1)\n",
      "N attention (5)\n",
      "N+N+N units attention mechanism (1)\n",
      "N+N attention mechanism (1)\n",
      "N mechanism (4)\n",
      "N scalability (1)\n",
      "ADJ+N simpler design (1)\n",
      "N design (1)\n",
      "N introduction (2)\n",
      "N+N introduction article (1)\n",
      "N article (5)\n",
      "ADJ+N English linguist (1)\n",
      "N linguist (3)\n",
      "ADJ+N+N English linguist plant (1)\n",
      "N+N linguist plant (2)\n",
      "N plant (3)\n",
      "N+N+N linguist plant pathologist (2)\n",
      "N+N plant pathologist (2)\n",
      "N pathologist (2)\n",
      "N+N+N plant pathologist computer (2)\n",
      "N+N pathologist computer (2)\n",
      "N computer (6)\n",
      "N+N+N pathologist computer scientist (2)\n",
      "N+N computer scientist (4)\n",
      "N mycologist (2)\n",
      "ADJ+N Conditional language (1)\n",
      "ADJ+N+N Conditional language models (1)\n",
      "ADJ+N popular choice (1)\n",
      "N choice (1)\n",
      "N elements (2)\n",
      "N+N elements fact (1)\n",
      "N Language (2)\n",
      "N+N Language model (2)\n",
      "N+N sentence words (1)\n",
      "ADJ+N vocabulary language (1)\n",
      "ADJ+N+N vocabulary language model (1)\n",
      "N+N+N language model estimates (1)\n",
      "N+N model estimates (1)\n",
      "N estimates (1)\n",
      "N sequence (3)\n",
      "N+N sequence context (1)\n",
      "N+N+N sequence context words (1)\n",
      "N+N context words (1)\n",
      "N gram (3)\n",
      "N+N gram language (2)\n",
      "N+N+N gram language model (1)\n",
      "N assumption (1)\n",
      "N+N assumption Language (1)\n",
      "N+N+N assumption Language model (1)\n",
      "N tables (2)\n",
      "N+N tables table (1)\n",
      "N+N field value (2)\n",
      "N value (3)\n",
      "N values (4)\n",
      "N pairs (3)\n",
      "ADJ+N Local conditioning (1)\n",
      "N+N information table (1)\n",
      "N description (2)\n",
      "N+N description words (1)\n",
      "ADJ+N previous words (2)\n",
      "N+N context language (2)\n",
      "N+N+N context language input (1)\n",
      "N+N language input (1)\n",
      "N+N+N language input text (1)\n",
      "N+N input text (1)\n",
      "N+N+N input text zct (1)\n",
      "N+N text zct (1)\n",
      "N zct (7)\n",
      "N unk (2)\n",
      "N+N unk name (1)\n",
      "N name (15)\n",
      "N+N+N unk name Table (1)\n",
      "N+N name Table (1)\n",
      "N Table (12)\n",
      "N+N+N name Table name (1)\n",
      "N+N Table name (1)\n",
      "N+N+N Table name birthdate (1)\n",
      "N+N name birthdate (2)\n",
      "N birthdate (5)\n",
      "N+N+N name birthdate birthplace (1)\n",
      "N+N birthdate birthplace (1)\n",
      "N birthplace (1)\n",
      "N+N+N birthdate birthplace occupation (1)\n",
      "N+N birthplace occupation (1)\n",
      "N occupation (8)\n",
      "N+N+N birthplace occupation spouse (1)\n",
      "N+N occupation spouse (1)\n",
      "N spouse (3)\n",
      "N+N+N occupation spouse children (1)\n",
      "N+N spouse children (3)\n",
      "N children (3)\n",
      "N+N zct name (1)\n",
      "N+N+N zct name spouse (1)\n",
      "N+N name spouse (2)\n",
      "N+N+N name spouse children (2)\n",
      "ADJ+N birthd output (1)\n",
      "ADJ+N+N birthd output candidates (1)\n",
      "N+N output candidates (1)\n",
      "N candidates (1)\n",
      "N placeholder (1)\n",
      "ADJ+N birthd occupation (1)\n",
      "ADJ+N+N birthd occupation name (1)\n",
      "N+N occupation name (1)\n",
      "N+N+N occupation name name (1)\n",
      "N+N name name (2)\n",
      "N+N+N name name spouse (1)\n",
      "ADJ+N right example (1)\n",
      "N example (5)\n",
      "ADJ+N+N right example table (1)\n",
      "N+N example table (1)\n",
      "ADJ+N set output (1)\n",
      "ADJ+N+N set output words (1)\n",
      "N+N output words (2)\n",
      "N Section (3)\n",
      "N+N Section model (1)\n",
      "N word (31)\n",
      "N+N word string (1)\n",
      "N string (2)\n",
      "N+N+N word string index (1)\n",
      "N+N string index (1)\n",
      "N index (3)\n",
      "N+N+N string index vocabulary (1)\n",
      "N+N index vocabulary (1)\n",
      "N vocabulary (4)\n",
      "N descriptor (3)\n",
      "N+N descriptor occurrence (1)\n",
      "N occurrence (2)\n",
      "N+N+N descriptor occurrence table (1)\n",
      "N+N occurrence table (1)\n",
      "N+N occurrence word (1)\n",
      "N+N+N occurrence word table (1)\n",
      "N+N word table (5)\n",
      "N+N set field (1)\n",
      "N+N+N set field position (1)\n",
      "N+N field position (6)\n",
      "N position (17)\n",
      "N+N number occurrences (1)\n",
      "N occurrences (2)\n",
      "N pair (3)\n",
      "ADJ+N occurs field (1)\n",
      "ADJ+N+N occurs field position (1)\n",
      "N+N+N field position scheme (1)\n",
      "N+N position scheme (1)\n",
      "N scheme (3)\n",
      "N+N+N position scheme words (1)\n",
      "N+N scheme words (1)\n",
      "N occur (1)\n",
      "N+N occur table (1)\n",
      "N+N example word (1)\n",
      "N+N+N example word linguistics (1)\n",
      "N+N word linguistics (1)\n",
      "N linguistics (2)\n",
      "N+N+N word linguistics table (1)\n",
      "N+N linguistics table (1)\n",
      "N+N+N linguistics table Figure (1)\n",
      "N+N table Figure (3)\n",
      "N Figure (5)\n",
      "N zlinguistics (2)\n",
      "N+N zlinguistics fields (2)\n",
      "ADJ+N cased commas (1)\n",
      "N commas (1)\n",
      "ADJ+N separate tokens (1)\n",
      "ADJ+N+N separate tokens Conditioning (1)\n",
      "N+N tokens Conditioning (1)\n",
      "N Conditioning (1)\n",
      "N+N+N tokens Conditioning field (1)\n",
      "N+N Conditioning field (1)\n",
      "N+N+N Conditioning field type (1)\n",
      "N+N field type (3)\n",
      "N type (4)\n",
      "N+N+N field type position (1)\n",
      "N+N type position (1)\n",
      "ADJ+N encode field (1)\n",
      "ADJ+N specific regularities (1)\n",
      "N regularities (1)\n",
      "ADJ+N+N specific regularities number (1)\n",
      "N+N regularities number (1)\n",
      "N+N+N regularities number token (1)\n",
      "N+N number token (1)\n",
      "N token (2)\n",
      "N+N+N number token date (1)\n",
      "N+N token date (1)\n",
      "N date (6)\n",
      "N+N+N token date field (1)\n",
      "N+N date field (2)\n",
      "N month (2)\n",
      "ADJ+N token date (1)\n",
      "ADJ+N+N token date field (1)\n",
      "N+N+N field position description (1)\n",
      "N+N position description (1)\n",
      "N+N+N position description scheme (1)\n",
      "N+N description scheme (1)\n",
      "N+N+N description scheme table (1)\n",
      "N+N scheme table (1)\n",
      "N terminates (1)\n",
      "N+N terminates field (1)\n",
      "ADJ+N useful capture (1)\n",
      "N capture (1)\n",
      "ADJ+N+N useful capture field (1)\n",
      "N+N capture field (1)\n",
      "N+N+N capture field transitions (1)\n",
      "N+N field transitions (1)\n",
      "N transitions (3)\n",
      "ADJ+N token name (2)\n",
      "ADJ+N+N token name field (2)\n",
      "N+N name field (2)\n",
      "N+N introduction birth (1)\n",
      "N birth (3)\n",
      "N+N+N introduction birth date (1)\n",
      "N+N birth date (2)\n",
      "N+N descriptor triplet (1)\n",
      "N triplet (3)\n",
      "N+N end field (1)\n",
      "N+N+N end field example (1)\n",
      "N+N field example (1)\n",
      "N Equation (2)\n",
      "N+N Equation use (1)\n",
      "N+N+N Equation use information (1)\n",
      "N+N use information (1)\n",
      "ADJ+N additional conditioning (1)\n",
      "ADJ+N+N additional conditioning context (1)\n",
      "N+N conditioning context (1)\n",
      "ADJ+N+N local conditioning variables (1)\n",
      "N+N conditioning variables (2)\n",
      "N variables (2)\n",
      "ADJ+N local context (1)\n",
      "ADJ+N previous word (1)\n",
      "ADJ+N+N previous word relations (1)\n",
      "N+N word relations (1)\n",
      "N+N+N word relations table (1)\n",
      "N+N relations table (1)\n",
      "N+N information tokens (1)\n",
      "N+N set fields (2)\n",
      "ADJ+N available table (1)\n",
      "N structure (1)\n",
      "N+N structure generation (1)\n",
      "N+N biographies fields (1)\n",
      "N politician (1)\n",
      "ADJ+N different ones (1)\n",
      "N ones (1)\n",
      "ADJ+N+N different ones actor (1)\n",
      "N+N ones actor (1)\n",
      "N actor (1)\n",
      "N+N+N ones actor athlete (1)\n",
      "N+N actor athlete (1)\n",
      "N athlete (1)\n",
      "ADJ+N global conditioning (9)\n",
      "ADJ+N available fields (1)\n",
      "ADJ+N available words (1)\n",
      "ADJ+N complementary fields (1)\n",
      "N basketball (1)\n",
      "N+N basketball player (1)\n",
      "N player (3)\n",
      "N+N+N basketball player hockey (1)\n",
      "N+N player hockey (1)\n",
      "N hockey (1)\n",
      "N+N+N player hockey player (1)\n",
      "N+N hockey player (1)\n",
      "N+N field names (1)\n",
      "N names (1)\n",
      "N league (1)\n",
      "N+N league position (1)\n",
      "N+N+N league position weight (1)\n",
      "N+N position weight (1)\n",
      "N weight (2)\n",
      "N+N+N position weight height (1)\n",
      "N+N weight height (1)\n",
      "N height (1)\n",
      "ADJ+N actual field (1)\n",
      "ADJ+N+N actual field tokens (1)\n",
      "N+N field tokens (2)\n",
      "N+N+N field tokens team (1)\n",
      "N+N tokens team (1)\n",
      "N team (1)\n",
      "N+N name player (1)\n",
      "N+N+N name player position (1)\n",
      "N+N player position (1)\n",
      "N+N+N player position help (1)\n",
      "N+N position help (1)\n",
      "N help (1)\n",
      "N+N+N position help model (1)\n",
      "N+N help model (1)\n",
      "ADJ+N better prediction (1)\n",
      "N prediction (1)\n",
      "ADJ+N binary indicators (1)\n",
      "N indicators (1)\n",
      "N+N field word (4)\n",
      "N+N+N field word vocabularies (1)\n",
      "N+N word vocabularies (1)\n",
      "N vocabularies (2)\n",
      "ADJ+N schematic example (1)\n",
      "ADJ+N next word (1)\n",
      "N+N+N context language model (1)\n",
      "N sets (1)\n",
      "N+N table zct (1)\n",
      "N+N words table (2)\n",
      "ADJ+N Copy actions (1)\n",
      "N+N model conditioning (1)\n",
      "N+N+N model conditioning features (1)\n",
      "N+N conditioning features (1)\n",
      "N features (4)\n",
      "N+N+N fact table turn (1)\n",
      "N+N table turn (1)\n",
      "N turn (1)\n",
      "N+N table information (1)\n",
      "N+N+N table information scoring (1)\n",
      "N+N information scoring (1)\n",
      "N scoring (2)\n",
      "N+N+N information scoring output (1)\n",
      "N+N scoring output (1)\n",
      "N+N+N scoring output words (1)\n",
      "ADJ+N particular sentences (1)\n",
      "ADJ+N special field (2)\n",
      "ADJ+N+N special field tokens (1)\n",
      "N score (6)\n",
      "N+N words field (1)\n",
      "N+N+N words field value (1)\n",
      "N+N table defines (1)\n",
      "N defines (1)\n",
      "N+N+N table defines output (1)\n",
      "N+N defines output (1)\n",
      "N+N+N defines output domain (1)\n",
      "N+N output domain (1)\n",
      "N domain (2)\n",
      "N+N tokens table (2)\n",
      "N instance (1)\n",
      "ADJ+N recent work (1)\n",
      "ADJ+N+N recent work attention (1)\n",
      "N+N work attention (1)\n",
      "ADJ+N neural machine (2)\n",
      "ADJ+N+N neural machine translation (2)\n",
      "N delexicalization (1)\n",
      "ADJ+N neural dialog (1)\n",
      "N dialog (3)\n",
      "ADJ+N+N neural dialog systems (1)\n",
      "N+N dialog systems (3)\n",
      "ADJ+N older work (1)\n",
      "ADJ+N+N older work class (1)\n",
      "N+N work class (1)\n",
      "N class (1)\n",
      "N+N+N language models dialog (1)\n",
      "N+N models dialog (1)\n",
      "N+N+N models dialog systems (1)\n",
      "ADJ+N parametric function (1)\n",
      "N function (7)\n",
      "ADJ+N learnable parameters (1)\n",
      "ADJ+N+N learnable parameters network (1)\n",
      "N+N parameters network (1)\n",
      "N+N function composition (1)\n",
      "N composition (1)\n",
      "ADJ+N differentiable functions (1)\n",
      "N functions (2)\n",
      "ADJ+N+N differentiable functions layers (1)\n",
      "N+N functions layers (1)\n",
      "N layers (3)\n",
      "N notations (1)\n",
      "N+N notations layers (1)\n",
      "N matrices (4)\n",
      "ADJ+N upper case (1)\n",
      "N case (3)\n",
      "ADJ+N+N upper case letters (1)\n",
      "N+N case letters (2)\n",
      "N letters (2)\n",
      "N+N+N case letters vectors (1)\n",
      "N+N letters vectors (1)\n",
      "N vectors (9)\n",
      "ADJ+N lower case (1)\n",
      "ADJ+N+N lower case letters (1)\n",
      "N row (4)\n",
      "N+N row matrix (1)\n",
      "N matrix (8)\n",
      "N vector (7)\n",
      "ADJ+N second dimension (1)\n",
      "N dimension (5)\n",
      "N column (1)\n",
      "N+N column vectors (1)\n",
      "ADJ+N denote vector (1)\n",
      "ADJ+N+N denote vector concatenation (1)\n",
      "N+N vector concatenation (1)\n",
      "N concatenation (3)\n",
      "N introduce (1)\n",
      "N+N introduce notation (1)\n",
      "N notation (1)\n",
      "ADJ+N different layers (1)\n",
      "N layer (8)\n",
      "N parameter (4)\n",
      "N+N parameter matrix (2)\n",
      "N+N layer lookup (1)\n",
      "N lookup (3)\n",
      "N+N+N layer lookup table (1)\n",
      "N+N lookup table (3)\n",
      "N array (1)\n",
      "N+N array indexing (1)\n",
      "N indexing (1)\n",
      "N+N+N array indexing operation (1)\n",
      "N+N indexing operation (1)\n",
      "N operation (5)\n",
      "N element (1)\n",
      "N+N element row (1)\n",
      "N+N matrix lookup (1)\n",
      "N+N+N matrix lookup table (1)\n",
      "N+N pair index (1)\n",
      "N+N+N lookup table operation (1)\n",
      "N+N table operation (1)\n",
      "ADJ+N common approach (1)\n",
      "ADJ+N+N common approach concatenate (1)\n",
      "N+N approach concatenate (1)\n",
      "N concatenate (1)\n",
      "ADJ+N linear transformation (1)\n",
      "N transformation (1)\n",
      "ADJ+N trainable parameters (1)\n",
      "ADJ+N+N trainable parameters weight (1)\n",
      "N+N parameters weight (1)\n",
      "N+N+N parameters weight matrix (1)\n",
      "N+N weight matrix (1)\n",
      "N+N+N weight matrix bias (1)\n",
      "N+N matrix bias (1)\n",
      "N bias (5)\n",
      "N+N+N matrix bias term (1)\n",
      "N+N bias term (1)\n",
      "N term (1)\n",
      "N+N context input (4)\n",
      "ADJ+N final layer (1)\n",
      "ADJ+N+N final layer outputs (1)\n",
      "N+N layer outputs (1)\n",
      "N outputs (3)\n",
      "N probability (4)\n",
      "N+N probability distribution (2)\n",
      "N distribution (3)\n",
      "N softmax (2)\n",
      "N+N softmax activation (1)\n",
      "N activation (1)\n",
      "N+N+N softmax activation function (1)\n",
      "N+N activation function (1)\n",
      "ADJ+N key aspect (1)\n",
      "N aspect (1)\n",
      "N+N word embeddings (3)\n",
      "ADJ+N Similar words (1)\n",
      "ADJ+N similar embeddings (1)\n",
      "N latent (3)\n",
      "N+N latent features (1)\n",
      "ADJ+N smooth functions (1)\n",
      "ADJ+N small change (2)\n",
      "N change (2)\n",
      "N results (6)\n",
      "ADJ+N+N small change probability (1)\n",
      "N+N change probability (1)\n",
      "ADJ+N better generalization (1)\n",
      "N generalization (1)\n",
      "ADJ+N unseen grams (1)\n",
      "N grams (1)\n",
      "N show (2)\n",
      "N+N show map (1)\n",
      "N map (1)\n",
      "N+N+N show map fact (1)\n",
      "N+N map fact (1)\n",
      "ADJ+N continuous space (1)\n",
      "N space (2)\n",
      "ADJ+N similar spirit (1)\n",
      "N spirit (1)\n",
      "N+N layer maps (1)\n",
      "N maps (1)\n",
      "N+N+N layer maps context (1)\n",
      "N+N maps context (1)\n",
      "N+N+N maps context word (1)\n",
      "N+N context word (1)\n",
      "N+N+N context word index (1)\n",
      "N+N word index (1)\n",
      "ADJ+N dimensional vector (1)\n",
      "N+N+N parameter matrix convert (1)\n",
      "N+N matrix convert (1)\n",
      "N convert (1)\n",
      "N+N+N matrix convert input (1)\n",
      "N+N convert input (1)\n",
      "N+N+N convert input vectors (1)\n",
      "N+N input vectors (1)\n",
      "N+N+N input vectors dimension (1)\n",
      "N+N vectors dimension (1)\n",
      "N+N+N word embeddings Table (1)\n",
      "N+N embeddings Table (1)\n",
      "N+N+N embeddings Table embeddings (1)\n",
      "N+N Table embeddings (1)\n",
      "N+N elements table (1)\n",
      "ADJ+N+N global conditioning information (1)\n",
      "N+N conditioning information (2)\n",
      "ADJ+N maximum length (1)\n",
      "N length (1)\n",
      "ADJ+N+N maximum length sequence (1)\n",
      "N+N length sequence (1)\n",
      "N+N+N length sequence words (1)\n",
      "N+N sequence words (1)\n",
      "N+N vectors dimensions (1)\n",
      "N dimensions (1)\n",
      "ADJ+N first vectors (1)\n",
      "ADJ+N possible starting (1)\n",
      "N starting (1)\n",
      "ADJ+N+N possible starting positions (1)\n",
      "N+N starting positions (1)\n",
      "N positions (3)\n",
      "N+N parameter matrices (2)\n",
      "N+N triplet refer (1)\n",
      "N refer (1)\n",
      "N+N end position (4)\n",
      "N+N+N end position field (1)\n",
      "N+N position field (2)\n",
      "N+N table field (2)\n",
      "N+N+N table field vector (1)\n",
      "N+N field vector (1)\n",
      "N+N+N field vector dimension (1)\n",
      "N+N vector dimension (2)\n",
      "N+N word vector (1)\n",
      "N+N+N word vector dimension (1)\n",
      "ADJ+N general shares (1)\n",
      "N shares (1)\n",
      "ADJ+N+N general shares parameters (1)\n",
      "N+N shares parameters (1)\n",
      "N Aggregating (1)\n",
      "N+N Aggregating embeddings (1)\n",
      "N occurence (1)\n",
      "N+N occurence word (1)\n",
      "N+N+N occurence word triplet (1)\n",
      "N+N word triplet (1)\n",
      "N+N+N word triplet field (1)\n",
      "N+N triplet field (1)\n",
      "N+N end embeddings (1)\n",
      "N times (5)\n",
      "ADJ+N particular word (1)\n",
      "ADJ+N multiple times (1)\n",
      "ADJ+N+N multiple times table (1)\n",
      "N+N times table (1)\n",
      "N guistics (1)\n",
      "N instances (2)\n",
      "N+N instances Figure (1)\n",
      "N+N+N instances Figure case (1)\n",
      "N+N Figure case (1)\n",
      "N component (1)\n",
      "ADJ+N wise max (1)\n",
      "N max (7)\n",
      "ADJ+N+N wise max start (1)\n",
      "N+N max start (1)\n",
      "N start (1)\n",
      "N+N+N max start embeddings (1)\n",
      "N+N start embeddings (1)\n",
      "N+N+N start embeddings instances (1)\n",
      "N+N embeddings instances (1)\n",
      "ADJ+N best features (1)\n",
      "N+N occurrences end (1)\n",
      "N+N+N occurrences end position (1)\n",
      "N+N+N end position embeddings (1)\n",
      "N+N position embeddings (1)\n",
      "ADJ+N+N local conditioning input (1)\n",
      "N+N conditioning input (1)\n",
      "ADJ+N+N global conditioning define (1)\n",
      "N+N conditioning define (1)\n",
      "N define (1)\n",
      "N+N+N conditioning define set (1)\n",
      "N+N define set (1)\n",
      "N+N+N define set fields (1)\n",
      "N+N max aggregation (1)\n",
      "N aggregation (1)\n",
      "N yields (1)\n",
      "N+N max max (1)\n",
      "N encodes (1)\n",
      "N+N encodes context (1)\n",
      "N+N+N encodes context input (1)\n",
      "N+N+N context input conditioning (1)\n",
      "N+N input conditioning (1)\n",
      "N+N+N input conditioning concatenation (1)\n",
      "N+N conditioning concatenation (1)\n",
      "N+N+N conditioning concatenation vectors (1)\n",
      "N+N concatenation vectors (1)\n",
      "N simplification (1)\n",
      "N+N simplification purpose (1)\n",
      "N purpose (2)\n",
      "ADJ+N define context (1)\n",
      "ADJ+N+N define context input (1)\n",
      "N+N+N context input zct (1)\n",
      "N+N input zct (1)\n",
      "N equations (1)\n",
      "N+N latent context (1)\n",
      "N+N+N latent context representation (1)\n",
      "N+N context representation (1)\n",
      "N representation (7)\n",
      "ADJ+N linear operation (1)\n",
      "ADJ+N hyperbolic tangent (1)\n",
      "N tangent (1)\n",
      "ADJ+N+N hyperbolic tangent tanh (1)\n",
      "N+N tangent tanh (1)\n",
      "N tanh (2)\n",
      "ADJ+N vocabulary outputs (1)\n",
      "N+N representation context (3)\n",
      "ADJ+N linear layer (1)\n",
      "ADJ+N real value (1)\n",
      "ADJ+N+N real value score (1)\n",
      "N+N value score (1)\n",
      "N+N+N value score word (1)\n",
      "N+N score word (1)\n",
      "N framework (1)\n",
      "ADJ+N diverse dataset (1)\n",
      "N dataset (2)\n",
      "ADJ+N set fields (1)\n",
      "N+N+N field position pair (1)\n",
      "N+N position pair (1)\n",
      "N+N space latent (1)\n",
      "N+N+N space latent representation (1)\n",
      "N+N latent representation (1)\n",
      "N+N+N latent representation context (1)\n",
      "N+N+N representation context input (1)\n",
      "N+N max operation (1)\n",
      "N+N dimension word (1)\n",
      "ADJ+N unique vector (1)\n",
      "ADJ+N+N unique vector max (1)\n",
      "N+N vector max (1)\n",
      "N+N+N vector max tanh (1)\n",
      "N+N max tanh (1)\n",
      "N dot (2)\n",
      "N+N dot product (2)\n",
      "N+N+N dot product context (1)\n",
      "N+N product context (1)\n",
      "N+N+N product context vector (1)\n",
      "N+N context vector (1)\n",
      "N+N+N word table Biography (1)\n",
      "N+N table Biography (1)\n",
      "N Biography (1)\n",
      "N+N+N table Biography dataset (1)\n",
      "N+N Biography dataset (1)\n",
      "ADJ+N dataset text (1)\n",
      "ADJ+N+N dataset text generation (1)\n",
      "N+N biography articles (1)\n",
      "N articles (1)\n",
      "N+N table infobox (1)\n",
      "N+N+N table infobox extract (1)\n",
      "N+N infobox extract (1)\n",
      "N extract (1)\n",
      "ADJ+N+N first sentence article (1)\n",
      "N+N sentence article (1)\n",
      "N CoreNLP (2)\n",
      "N numbers (1)\n",
      "ADJ+N special token (1)\n",
      "N years (1)\n",
      "ADJ+N token Field (1)\n",
      "N Field (1)\n",
      "ADJ+N+N token Field values (1)\n",
      "N+N Field values (1)\n",
      "N+N+N Field values tables (1)\n",
      "N+N values tables (1)\n",
      "ADJ+N dataset statistics (1)\n",
      "ADJ+N short table (1)\n",
      "ADJ+N+N short table tokens (1)\n",
      "N+N table tokens (2)\n",
      "ADJ+N third sentence (1)\n",
      "ADJ+N+N third sentence tokens (1)\n",
      "N+N sentence tokens (2)\n",
      "ADJ+N final corpus (1)\n",
      "N corpus (1)\n",
      "N sub (1)\n",
      "N+N sub parts (1)\n",
      "N+N training validation (1)\n",
      "N validation (6)\n",
      "N+N+N training validation test (1)\n",
      "N+N validation test (1)\n",
      "N test (1)\n",
      "ADJ+N available download (1)\n",
      "N download (1)\n",
      "N baseline (4)\n",
      "N+N+N language model use (1)\n",
      "N+N model use (1)\n",
      "N toolkit (2)\n",
      "N+N toolkit train (1)\n",
      "N train (1)\n",
      "N+N+N toolkit train gram (1)\n",
      "N+N train gram (1)\n",
      "N+N+N train gram models (1)\n",
      "N+N gram models (1)\n",
      "N+N+N language model templates (1)\n",
      "N+N model templates (1)\n",
      "N templates (1)\n",
      "N+N softmax function (1)\n",
      "N scores (2)\n",
      "N+N scores training (1)\n",
      "N+N+N scores training sentences (1)\n",
      "N+N training sentences (1)\n",
      "N+N table descriptor (1)\n",
      "N+N distribution troduction (1)\n",
      "N troduction (1)\n",
      "N+N+N distribution troduction section (1)\n",
      "N+N troduction section (1)\n",
      "N section (2)\n",
      "N+N+N troduction section table (1)\n",
      "N+N section table (1)\n",
      "N+N+N section table Figure (1)\n",
      "N log (3)\n",
      "N+N scheme name (1)\n",
      "N+N+N scheme name name (1)\n",
      "N+N+N name name birthdate (1)\n",
      "N+N+N name birthdate birthdate (1)\n",
      "N+N birthdate birthdate (2)\n",
      "N+N+N birthdate birthdate birthdate (1)\n",
      "N+N+N birthdate birthdate deathdate (1)\n",
      "N+N birthdate deathdate (1)\n",
      "N deathdate (2)\n",
      "N+N+N birthdate deathdate deathdate (1)\n",
      "N+N deathdate deathdate (1)\n",
      "ADJ+N english linguist (1)\n",
      "ADJ+N+N english linguist fields (1)\n",
      "N+N linguist fields (1)\n",
      "N minimize (1)\n",
      "N+N minimize fields (1)\n",
      "ADJ+N negative log (1)\n",
      "ADJ+N+N negative log likelihood (1)\n",
      "N+N log likelihood (1)\n",
      "N likelihood (1)\n",
      "N+N+N log likelihood training (1)\n",
      "N+N likelihood training (1)\n",
      "N+N+N likelihood training sentence (1)\n",
      "N+N training sentence (1)\n",
      "N+N+N training sentence mycologist (1)\n",
      "N+N sentence mycologist (1)\n",
      "N inference (1)\n",
      "N+N inference decoder (1)\n",
      "ADJ+N stochastic gradient (1)\n",
      "N gradient (1)\n",
      "ADJ+N+N stochastic gradient descent (1)\n",
      "N+N gradient descent (1)\n",
      "N descent (1)\n",
      "ADJ+N special tokens (1)\n",
      "N+N input table (3)\n",
      "ADJ+N token copy (1)\n",
      "ADJ+N final score (1)\n",
      "ADJ+N vocabulary score (1)\n",
      "ADJ+N+N vocabulary score field (1)\n",
      "N+N score field (1)\n",
      "N Experiments (1)\n",
      "ADJ+N neural network (2)\n",
      "ADJ+N+N neural network model (1)\n",
      "ADJ+N generate sentences (1)\n",
      "ADJ+N large scale (1)\n",
      "ADJ+N+N large scale problems (1)\n",
      "N+N scale problems (1)\n",
      "N problems (1)\n",
      "ADJ+N diverse set (1)\n",
      "ADJ+N+N diverse set sentence (1)\n",
      "N+N set sentence (1)\n",
      "N+N+N set sentence types (1)\n",
      "N+N sentence types (1)\n",
      "N types (4)\n",
      "ADJ+N good Training (1)\n",
      "N Training (1)\n",
      "ADJ+N+N good Training setup (1)\n",
      "N+N Training setup (1)\n",
      "N setup (1)\n",
      "ADJ+N neural models (3)\n",
      "N+N+N gram language models (1)\n",
      "N rate (1)\n",
      "ADJ+N Local field (4)\n",
      "ADJ+N Global field (4)\n",
      "ADJ+N+N Global field word (2)\n",
      "N+N+N field word Table (1)\n",
      "N+N word Table (1)\n",
      "N+N+N word Table BLEU (1)\n",
      "N+N Table BLEU (1)\n",
      "N BLEU (4)\n",
      "N perplexity (3)\n",
      "N rows (3)\n",
      "N+N rows copy (1)\n",
      "N+N+N rows copy actions (1)\n",
      "ADJ+N standard deviation (1)\n",
      "N deviation (1)\n",
      "ADJ+N different initialization (1)\n",
      "N initialization (1)\n",
      "N beam (6)\n",
      "N+N beam width (1)\n",
      "N width (1)\n",
      "N+N+N beam width Perplexities (1)\n",
      "N+N width Perplexities (1)\n",
      "N Perplexities (1)\n",
      "ADJ+N comparable output (1)\n",
      "ADJ+N+N comparable output vocabularies (1)\n",
      "N+N output vocabularies (1)\n",
      "ADJ+N Mean tokens (1)\n",
      "N+N table table (1)\n",
      "N+N+N table table tokens (1)\n",
      "ADJ+N various applications (1)\n",
      "ADJ+N different metrics (1)\n",
      "N metrics (4)\n",
      "ADJ+N metric language (1)\n",
      "N Generation (2)\n",
      "N+N Generation quality (1)\n",
      "N quality (1)\n",
      "N measure (1)\n",
      "N+N word types (1)\n",
      "N+N+N word types field (1)\n",
      "N+N types field (1)\n",
      "N+N+N types field types (1)\n",
      "N+N field types (2)\n",
      "N+N+N field word field (1)\n",
      "N+N word field (1)\n",
      "N size (4)\n",
      "N Evaluation (1)\n",
      "N+N Evaluation metrics (1)\n",
      "N+N+N Evaluation metrics Table (1)\n",
      "N+N metrics Table (1)\n",
      "ADJ+N hyper parameters (1)\n",
      "ADJ+N least times (1)\n",
      "ADJ+N+N least times training (1)\n",
      "N+N times training (1)\n",
      "N+N+N times training data (1)\n",
      "ADJ+N frequent words (1)\n",
      "ADJ+N+N frequent words vocabulary (1)\n",
      "N+N words vocabulary (1)\n",
      "N hyperparameters (1)\n",
      "N+N validation subset (1)\n",
      "N subset (2)\n",
      "N note (1)\n",
      "ADJ+N maximum number (1)\n",
      "ADJ+N+N maximum number tokens (1)\n",
      "N+N number tokens (1)\n",
      "N+N+N number tokens field (1)\n",
      "N+N tokens field (1)\n",
      "ADJ+N encode positions (1)\n",
      "ADJ+N longer field (1)\n",
      "ADJ+N+N longer field values (1)\n",
      "N+N field values (2)\n",
      "ADJ+N final tokens (1)\n",
      "N initialize (1)\n",
      "N+N initialize word (1)\n",
      "N+N+N initialize word embeddings (1)\n",
      "ADJ+N set training (1)\n",
      "ADJ+N+N set training biographies (1)\n",
      "N+N training biographies (1)\n",
      "N+N representation Results (1)\n",
      "N Results (1)\n",
      "ADJ+N different conditioning (1)\n",
      "ADJ+N+N different conditioning variables (1)\n",
      "N+N results Table (1)\n",
      "N+N+N results Table show (1)\n",
      "N+N Table show (1)\n",
      "N+N+N Table show conditioning (1)\n",
      "N+N show conditioning (1)\n",
      "N+N+N show conditioning information (1)\n",
      "N performance (2)\n",
      "N+N performance models (1)\n",
      "N+N generation metrics (1)\n",
      "N+N+N generation metrics BLEU (1)\n",
      "N+N metrics BLEU (1)\n",
      "N+N performance ordering (1)\n",
      "N ordering (1)\n",
      "N+N+N performance ordering models (1)\n",
      "N+N ordering models (1)\n",
      "N+N models copy (1)\n",
      "N+N+N models copy actions (1)\n",
      "N+N factorization models (1)\n",
      "ADJ+N different output (1)\n",
      "ADJ+N+N different output domains (1)\n",
      "N+N output domains (1)\n",
      "N+N perplexity comparisons (1)\n",
      "N comparisons (1)\n",
      "ADJ+N straightforward models (1)\n",
      "ADJ+N fixed vocabulary (1)\n",
      "ADJ+N+N fixed vocabulary Template (1)\n",
      "N+N vocabulary Template (1)\n",
      "N Template (2)\n",
      "ADJ+N set field (1)\n",
      "ADJ+N+N set field position (1)\n",
      "N+N+N field position pairs (2)\n",
      "N+N position pairs (2)\n",
      "ADJ+N vocabulary Table (1)\n",
      "ADJ+N standard software (1)\n",
      "N software (1)\n",
      "ADJ+N+N standard software NIST (1)\n",
      "N+N software NIST (1)\n",
      "N NIST (1)\n",
      "N+N+N software NIST mteval (1)\n",
      "N+N NIST mteval (1)\n",
      "N mteval (1)\n",
      "N rouge (1)\n",
      "N+N attention scores (1)\n",
      "N+N+N probability distribution field (1)\n",
      "N+N distribution field (1)\n",
      "N+N+N distribution field position (1)\n",
      "ADJ+N+N previous words words (1)\n",
      "N+N words words (1)\n",
      "ADJ+N current row (1)\n",
      "N colors (1)\n",
      "ADJ+N higher probabilities (1)\n",
      "N+N beam size (2)\n",
      "ADJ+N+N local conditioning field (1)\n",
      "N+N conditioning field (1)\n",
      "N accuracy (4)\n",
      "N+N accuracy Generation (1)\n",
      "N+N+N accuracy Generation metrics (1)\n",
      "N+N Generation metrics (1)\n",
      "ADJ+N clear improvement (1)\n",
      "N improvement (2)\n",
      "N+N transitions fields (1)\n",
      "ADJ+N previous predictions (1)\n",
      "N predictions (1)\n",
      "ADJ+N+N previous predictions table (1)\n",
      "N+N predictions table (1)\n",
      "N experiments (1)\n",
      "ADJ+N+N Global conditioning fields (1)\n",
      "N+N conditioning fields (2)\n",
      "ADJ+N additional BLEU (1)\n",
      "ADJ+N total improvement (1)\n",
      "ADJ+N Similar observations (1)\n",
      "N observations (1)\n",
      "N time (1)\n",
      "ADJ+N best model (1)\n",
      "ADJ+N+N best model Table (1)\n",
      "N+N model Table (2)\n",
      "N+N baseline Template (1)\n",
      "ADJ+N different beam (1)\n",
      "ADJ+N+N different beam sizes (1)\n",
      "N+N beam sizes (1)\n",
      "N sizes (1)\n",
      "N axis (2)\n",
      "ADJ+N average timing (1)\n",
      "N timing (1)\n",
      "ADJ+N+N average timing milliseconds (1)\n",
      "N+N timing milliseconds (1)\n",
      "N milliseconds (1)\n",
      "N+N axis BLEU (1)\n",
      "N+N+N axis BLEU score (1)\n",
      "N+N BLEU score (1)\n",
      "N+N subset samples (1)\n",
      "N+N+N subset samples validation (1)\n",
      "N+N samples validation (1)\n",
      "N Attention (1)\n",
      "N+N Attention mechanism (1)\n",
      "N+N attention input (1)\n",
      "N+N+N attention input table (1)\n",
      "N+N+N input table fields (1)\n",
      "N+N table fields (1)\n",
      "N+N+N language model score (1)\n",
      "N+N model score (1)\n",
      "N+N bias ct (1)\n",
      "N ct (1)\n",
      "N+N bias dot (1)\n",
      "N+N+N bias dot product (1)\n",
      "N+N+N dot product representation (1)\n",
      "N+N product representation (1)\n",
      "N+N+N product representation table (1)\n",
      "N+N representation table (1)\n",
      "N+N+N representation table field (1)\n",
      "N+N+N representation context Equation (1)\n",
      "N+N context Equation (1)\n",
      "N+N+N context Equation summarizes (1)\n",
      "N+N Equation summarizes (1)\n",
      "N summarizes (1)\n",
      "N+N fields words (1)\n",
      "N+N+N fields words Figure (1)\n",
      "N+N words Figure (1)\n",
      "N+N+N words Figure shows (1)\n",
      "N+N Figure shows (1)\n",
      "N shows (1)\n",
      "N+N+N Figure shows mechanism (1)\n",
      "N+N shows mechanism (1)\n",
      "ADJ+N large bias (2)\n",
      "N+N+N tokens table emits (1)\n",
      "N+N table emits (1)\n",
      "N emits (1)\n",
      "N+N+N table emits word (1)\n",
      "N+N emits word (1)\n",
      "N+N transitions field (1)\n",
      "N+N+N transitions field types (1)\n",
      "N+N+N field types model (1)\n",
      "N+N types model (1)\n",
      "ADJ+N+N large bias words (1)\n",
      "N+N bias words (1)\n",
      "N+N occupation field (1)\n",
      "ADJ+N standard beam (1)\n",
      "ADJ+N+N standard beam search (1)\n",
      "N+N beam search (1)\n",
      "N search (3)\n",
      "ADJ+N set sentences (1)\n",
      "ADJ+N greedy search (1)\n",
      "N explore (1)\n",
      "N+N explore times (1)\n",
      "N+N+N explore times paths (1)\n",
      "N+N times paths (1)\n",
      "N paths (1)\n",
      "ADJ+N linear increase (1)\n",
      "N increase (1)\n",
      "ADJ+N+N linear increase number (1)\n",
      "N+N increase number (1)\n",
      "ADJ+N forward computation (1)\n",
      "N computation (1)\n",
      "ADJ+N+N forward computation steps (1)\n",
      "N+N computation steps (1)\n",
      "N steps (1)\n",
      "N+N+N computation steps language (1)\n",
      "N+N steps language (1)\n",
      "N+N+N steps language model (1)\n",
      "ADJ+N various beam (1)\n",
      "ADJ+N+N various beam settings (1)\n",
      "N+N beam settings (1)\n",
      "N settings (1)\n",
      "ADJ+N best validation (1)\n",
      "ADJ+N several times (1)\n",
      "ADJ+N faster baseline (1)\n",
      "N+N sentence Beam (1)\n",
      "N Beam (1)\n",
      "N+N+N sentence Beam search (1)\n",
      "N+N Beam search (1)\n",
      "ADJ+N many ngram (1)\n",
      "N ngram (1)\n",
      "ADJ+N+N many ngram lookups (1)\n",
      "N+N ngram lookups (1)\n",
      "N lookups (1)\n",
      "N rhodes (1)\n",
      "N cricketer (1)\n",
      "N+N cricketer Table (1)\n",
      "ADJ+N australian rules (1)\n",
      "N XXXXs (1)\n",
      "N mycology (1)\n",
      "N+N mycology plant (1)\n",
      "N+N+N mycology plant pathology (1)\n",
      "N+N plant pathology (1)\n",
      "N pathology (1)\n",
      "N+N+N plant pathology mathematics (1)\n",
      "N+N pathology mathematics (1)\n",
      "N mathematics (1)\n",
      "N+N+N pathology mathematics university (1)\n",
      "N+N mathematics university (1)\n",
      "N university (1)\n",
      "N contributions (1)\n",
      "ADJ+N computational linguistics (1)\n",
      "ADJ+N+N computational linguistics Table (1)\n",
      "N+N linguistics Table (1)\n",
      "ADJ+N First sentence (1)\n",
      "N versions (2)\n",
      "N+N versions table (1)\n",
      "N+N+N language model Table (1)\n",
      "ADJ+N random memory (1)\n",
      "N memory (1)\n",
      "N+N scoring matrix (1)\n",
      "N+N+N scoring matrix matrix (1)\n",
      "N+N matrix matrix (1)\n",
      "N+N+N matrix matrix products (1)\n",
      "N+N matrix products (1)\n",
      "N products (1)\n",
      "N+N+N matrix products operation (1)\n",
      "N+N products operation (1)\n",
      "N block (1)\n",
      "N+N block parallel (1)\n",
      "N parallel (1)\n",
      "N+N+N block parallel manner (1)\n",
      "N+N parallel manner (1)\n",
      "N manner (1)\n",
      "ADJ+N graphic processors (1)\n",
      "N processors (1)\n",
      "N analysis (1)\n",
      "N+N analysis Table (1)\n",
      "N generations (1)\n",
      "ADJ+N variants model (1)\n",
      "N reference (1)\n",
      "N+N reference fact (1)\n",
      "N+N+N reference fact table (1)\n",
      "N+N birth month (1)\n",
      "N contributor (1)\n",
      "N+N versions model (1)\n",
      "N beginning (1)\n",
      "N+N beginning sentence (1)\n",
      "N+N+N beginning sentence copying (1)\n",
      "N+N sentence copying (1)\n",
      "N copying (1)\n",
      "N+N+N sentence copying name (1)\n",
      "N+N copying name (1)\n",
      "N+N+N copying name birth (1)\n",
      "N+N name birth (1)\n",
      "N+N+N name birth date (1)\n",
      "N+N+N birth date death (1)\n",
      "N+N date death (1)\n",
      "N death (2)\n",
      "N+N+N date death date (1)\n",
      "N+N death date (2)\n",
      "N+N+N death date table (2)\n",
      "N+N date table (2)\n",
      "N+N scientist occupation (1)\n",
      "ADJ+N right occupation (1)\n",
      "ADJ+N common occupation (1)\n",
      "ADJ+N+N global conditioning fields (1)\n",
      "ADJ+N+N global conditioning words (1)\n",
      "N+N conditioning words (1)\n",
      "N+N+N conditioning words model (1)\n",
      "N+N words model (1)\n",
      "ADJ+N correct occupation (1)\n",
      "ADJ+N+N correct occupation computer (1)\n",
      "N+N occupation computer (1)\n",
      "N+N+N occupation computer scientist (1)\n",
      "ADJ+N fluent descriptions (1)\n",
      "ADJ+N arbitrary people (1)\n",
      "N people (1)\n",
      "ADJ+N structured data (1)\n",
      "ADJ+N large margin (1)\n",
      "N margin (1)\n",
      "ADJ+N+N large margin outperform (1)\n",
      "N+N margin outperform (1)\n",
      "N outperform (1)\n",
      "N task (1)\n",
      "N+N+N order magnitude data (1)\n",
      "N+N magnitude data (1)\n",
      "ADJ+N+N previous work vocabulary (1)\n",
      "N+N work vocabulary (1)\n",
      "N orders (1)\n",
      "N+N orders magnitude (1)\n",
      "ADJ+N larger paper (1)\n",
      "ADJ+N+N first sentence tackle (1)\n",
      "N+N sentence tackle (1)\n",
      "N+N+N sentence tackle generation (1)\n",
      "N+N tackle generation (1)\n",
      "ADJ+N future work (1)\n",
      "ADJ+N token position (1)\n",
      "ADJ+N+N token position word (1)\n",
      "N+N position word (1)\n",
      "N+N+N position word type (1)\n",
      "N+N word type (1)\n",
      "N+N+N word type perform (1)\n",
      "N+N type perform (1)\n",
      "N perform (1)\n",
      "N+N+N type perform max (1)\n",
      "N+N perform max (1)\n",
      "ADJ+N richer representation (1)\n",
      "ADJ+N recurrent encoder (1)\n",
      "ADJ+N convolutional encoder (1)\n",
      "ADJ+N current training (1)\n",
      "ADJ+N+N current training loss (1)\n",
      "N+N training loss (1)\n",
      "N loss (2)\n",
      "N+N+N training loss function (1)\n",
      "N+N loss function (2)\n",
      "ADJ+N incorrect facts (1)\n",
      "ADJ+N incorrect nationality (1)\n",
      "N nationality (1)\n",
      "ADJ+N+N incorrect nationality occupation (1)\n",
      "N+N nationality occupation (1)\n",
      "ADJ+N incorrect determiner (1)\n",
      "N determiner (1)\n",
      "ADJ+N+N incorrect determiner loss (1)\n",
      "N+N determiner loss (1)\n",
      "N+N+N determiner loss function (1)\n",
      "ADJ+N factual accuracy (2)\n",
      "N+N sentence generation (1)\n",
      "N mistakes (1)\n",
      "ADJ+N define strategy (1)\n",
      "N strategy (1)\n",
      "ADJ+N+N factual accuracy generation (1)\n",
      "N+N accuracy generation (1)\n",
      "N References (1)\n",
      "N+N References Angeli (1)\n",
      "N Angeli (1)\n",
      "ADJ+N simple domain (1)\n",
      "ADJ+N probabilistic approach (1)\n",
      "ADJ+N+N probabilistic approach generation (1)\n",
      "N+N approach generation (1)\n",
      "N align (1)\n",
      "ADJ+N Collective content (1)\n",
      "N content (1)\n",
      "ADJ+N+N Collective content selection (1)\n",
      "N+N content selection (1)\n",
      "N+N+N content selection concept (1)\n",
      "N+N selection concept (1)\n",
      "N+N+N selection concept text (1)\n",
      "N conference (3)\n",
      "ADJ+N+N natural language generation (6)\n",
      "N+N language generation (9)\n",
      "ADJ+N main conference (1)\n",
      "ADJ+N human evaluation (1)\n",
      "N evaluation (1)\n",
      "ADJ+N+N human evaluation nlg (1)\n",
      "N+N evaluation nlg (1)\n",
      "N nlg (1)\n",
      "N+N+N evaluation nlg systems (1)\n",
      "N+N nlg systems (1)\n",
      "ADJ+N Automatic generation (1)\n",
      "ADJ+N+N Automatic generation weather (1)\n",
      "N+N generation weather (1)\n",
      "N+N+N generation weather forecast (1)\n",
      "N+N+N weather forecast texts (1)\n",
      "N+N forecast texts (1)\n",
      "N texts (1)\n",
      "ADJ+N probabilistic generationspace (1)\n",
      "N generationspace (1)\n",
      "ADJ+N+N probabilistic generationspace models (1)\n",
      "N+N generationspace models (1)\n",
      "ADJ+N probabilistic language (1)\n",
      "ADJ+N+N probabilistic language model (1)\n",
      "N graph (1)\n",
      "N+N graph database (1)\n",
      "ADJ+N human knowledge (1)\n",
      "ADJ+N navigational assistance (1)\n",
      "N assistance (1)\n",
      "ADJ+N Australasian computer (1)\n",
      "ADJ+N+N Australasian computer science (1)\n",
      "N+N computer science (1)\n",
      "N science (1)\n",
      "N+N+N computer science conference (1)\n",
      "N+N science conference (1)\n",
      "N+N+N science conference Volume (1)\n",
      "N+N conference Volume (1)\n",
      "N Volume (1)\n",
      "ADJ+N joint models (1)\n",
      "ADJ+N statistical machine (2)\n",
      "ADJ+N+N statistical machine translation (2)\n",
      "N volume (3)\n",
      "N+N volume pages (1)\n",
      "N planner (1)\n",
      "N+N planner construction (1)\n",
      "N construction (1)\n",
      "ADJ+N evolutionary algorithms (1)\n",
      "N algorithms (1)\n",
      "N fitness (1)\n",
      "N+N fitness function (1)\n",
      "N captions (1)\n",
      "ADJ+N visual concepts (1)\n",
      "N concepts (1)\n",
      "N CVPR (1)\n",
      "N watson (1)\n",
      "ADJ+N multilingual descriptions (1)\n",
      "N owl (1)\n",
      "N+N owl ontologies (1)\n",
      "N ontologies (1)\n",
      "N+N+N owl ontologies naturalowl (1)\n",
      "N+N ontologies naturalowl (1)\n",
      "N naturalowl (1)\n",
      "N+N+N ontologies naturalowl system (1)\n",
      "N+N naturalowl system (1)\n",
      "N system (1)\n",
      "ADJ+N biomedical arguments (1)\n",
      "N readers (1)\n",
      "N realisation (1)\n",
      "N+N realisation knowledge (1)\n",
      "N+N+N realisation knowledge bases (1)\n",
      "N+N knowledge bases (1)\n",
      "N bases (1)\n",
      "N+N+N language model estimation (1)\n",
      "N+N model estimation (1)\n",
      "N estimation (1)\n",
      "ADJ+N semantic alignments (1)\n",
      "N alignments (1)\n",
      "N image (3)\n",
      "N+N image descriptions (1)\n",
      "ADJ+N semantic parsing (1)\n",
      "N parsing (1)\n",
      "ADJ+N semantic embeddings (1)\n",
      "N+N+N language models arXiv (1)\n",
      "N+N models arXiv (1)\n",
      "N arXiv (6)\n",
      "N+N+N models arXiv preprint (1)\n",
      "N+N arXiv preprint (3)\n",
      "N preprint (3)\n",
      "N+N+N arXiv preprint arXiv (3)\n",
      "N+N preprint arXiv (3)\n",
      "ADJ+N global model (1)\n",
      "ADJ+N+N global model concept (1)\n",
      "ADJ+N statistical knowledge (1)\n",
      "N+N embeddings hellinger (1)\n",
      "N hellinger (1)\n",
      "N ller (1)\n",
      "ADJ+N Efficient backprop (1)\n",
      "N backprop (1)\n",
      "N networks (1)\n",
      "N+N networks Tricks (1)\n",
      "N Tricks (1)\n",
      "N+N+N networks Tricks trade (1)\n",
      "N+N Tricks trade (1)\n",
      "N trade (1)\n",
      "N+N+N Tricks trade pages (1)\n",
      "N+N trade pages (1)\n",
      "ADJ+N semantic correspondences (1)\n",
      "N correspondences (1)\n",
      "ADJ+N less supervision (1)\n",
      "N supervision (1)\n",
      "ADJ+N forestto string (1)\n",
      "ADJ+N+N forestto string model (1)\n",
      "N+N string model (1)\n",
      "N+N+N string model language (1)\n",
      "N+N model language (1)\n",
      "N+N+N model language generation (1)\n",
      "N calculus (1)\n",
      "N+N calculus expressions (1)\n",
      "N expressions (2)\n",
      "ADJ+N rare word (1)\n",
      "ADJ+N+N rare word problem (1)\n",
      "N+N word problem (1)\n",
      "N user (1)\n",
      "ADJ+N linguistic style (1)\n",
      "ADJ+N Trainable generation (1)\n",
      "ADJ+N+N Trainable generation personality (1)\n",
      "N+N generation personality (1)\n",
      "N+N+N generation personality traits (1)\n",
      "N+N personality traits (1)\n",
      "N traits (1)\n",
      "ADJ+N+N natural language processing (1)\n",
      "N+N language processing (1)\n",
      "N processing (1)\n",
      "N+N+N language processing toolkit (1)\n",
      "N+N processing toolkit (1)\n",
      "N Demonstrations (1)\n",
      "ADJ+N selective generation (1)\n",
      "ADJ+N fine alignment (1)\n",
      "N dialogue (2)\n",
      "N+N dialogue systems (1)\n",
      "N+N+N language generation application (1)\n",
      "N+N generation application (1)\n",
      "ADJ+N conversational dialog (1)\n",
      "ADJ+N+N conversational dialog systems (1)\n",
      "N+N+N language generation systems (1)\n",
      "N+N+N generation systems volume (1)\n",
      "N+N systems volume (1)\n",
      "N+N words computer (1)\n",
      "N+N weather forecasts (1)\n",
      "N forecasts (1)\n",
      "ADJ+N short text (1)\n",
      "ADJ+N+N short text conversation (1)\n",
      "N+N text conversation (1)\n",
      "N conversation (2)\n",
      "N+N+N text conversation arXiv (1)\n",
      "N+N conversation arXiv (1)\n",
      "N+N+N conversation arXiv preprint (1)\n",
      "N widl (1)\n",
      "N+N widl expressions (1)\n",
      "N+N+N widl expressions application (1)\n",
      "N+N expressions application (1)\n",
      "N+N+N expressions application machine (1)\n",
      "N+N application machine (1)\n",
      "N+N+N application machine translation (1)\n",
      "N+N+N machine translation summarization (1)\n",
      "N+N translation summarization (1)\n",
      "N summarization (1)\n",
      "ADJ+N geographic descriptions (1)\n",
      "ADJ+N Empirical methods (1)\n",
      "N+N+N language generation pages (1)\n",
      "N+N generation pages (1)\n",
      "ADJ+N neural image (1)\n",
      "ADJ+N+N neural image caption (1)\n",
      "N+N image caption (2)\n",
      "N caption (2)\n",
      "N+N+N image caption generator (1)\n",
      "N+N caption generator (1)\n",
      "N generator (1)\n",
      "ADJ+N semantic parser (1)\n",
      "N parser (1)\n",
      "N+N+N image caption generation (1)\n",
      "N+N caption generation (1)\n",
      "ADJ+N visual attention (1)\n",
      "N intention (1)\n",
      "N+N intention neural (1)\n",
      "N neural (1)\n",
      "N+N+N intention neural network (1)\n",
      "N+N neural network (1)\n",
      "N+N+N neural network conversation (1)\n",
      "N+N network conversation (1)\n",
      "N+N+N network conversation model (1)\n",
      "N+N conversation model (1)\n",
      "N+N+N conversation model arXiv (1)\n",
      "N+N model arXiv (1)\n",
      "N+N+N model arXiv preprint (1)\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from collections import Counter\n",
    "from spacy.matcher import Matcher #import matcher\n",
    "\n",
    "\n",
    "matcher = Matcher(nlp.vocab) # Initialize the matcher with the shared vocab\n",
    "\n",
    "#define rules\n",
    "pattern1 = [{'POS': 'ADJ'},{'POS': 'NOUN'}]\n",
    "pattern2 = [{'POS': 'NOUN'}, {'POS': 'NOUN'}]\n",
    "pattern3 = [{'POS': 'NOUN'}, {'POS': 'NOUN'}, {'POS': 'NOUN'} ]\n",
    "pattern4 = [{'POS': 'NOUN'}]\n",
    "pattern5 = [{'POS': 'ADJ'},{'POS': 'NOUN'}, {'POS': 'NOUN'}]\n",
    "\n",
    "#add rules to matcher \n",
    "matcher.add('ADJ+N', [pattern1]) \n",
    "matcher.add('N+N', [pattern2]) \n",
    "matcher.add('N+N+N', [pattern3]) \n",
    "matcher.add('N', [pattern4]) \n",
    "matcher.add('ADJ+N+N', [pattern5]) \n",
    "\n",
    "matches = matcher(data)\n",
    "\n",
    "d=[]\n",
    "for match_id, start, end in matches:\n",
    "    rule_id = nlp.vocab.strings[match_id]  # get the unicode ID, i.e. 'COLOR'\n",
    "    span = data[start : end]  # get the matched slice of the doc\n",
    "    d.append((rule_id, span.text))\n",
    "    keyterm = span.text\n",
    "print(\"\\n\".join(f'{i[0]} {i[1]} ({j})' for i,j in Counter(d).items()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rank the extracted key term and do manual filtering based on the list below \\\n",
    "note: most frequent words don't always refer that they are terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_terms = Counter(d)\n",
    "rank_term = total_terms.most_common() #list all\n",
    "# total_terms.most_common()[:200] #top 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    terms  frequency\n",
      "0                              (N, model)         57\n",
      "1                              (N, table)         55\n",
      "2                              (N, field)         51\n",
      "3                           (N, language)         42\n",
      "4                              (N, words)         36\n",
      "...                                   ...        ...\n",
      "1700  (N+N+N, network conversation model)          1\n",
      "1701            (N+N, conversation model)          1\n",
      "1702    (N+N+N, conversation model arXiv)          1\n",
      "1703                   (N+N, model arXiv)          1\n",
      "1704        (N+N+N, model arXiv preprint)          1\n",
      "\n",
      "[1705 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(rank_term, columns=['terms', 'frequency'])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('first_time_keyterm_extraction.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we need to do manual filtering to extract a list of terms (real terms) used to adapt a new rule based system with IOB tagging according to the list of terms derived from manual annotation (almost the same like the above system except that we need to modify to get exacly the keyterm we want to have ex: string matching , lemmatization, junker and so on). then apply this modified rule based system on all the same documents (pdf files) to get IOB annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the new rule based system, we will get a set of silver corpora to train our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neural model', 'language models', 'neural language models', 'extend models', 'vocabulary extend models', 'copy actions', 'neural model', 'language model', 'infobox', 'knowledge base', 'statistical generation model', 'generation model', 'infobox', 'language model', 'neural language model', 'factorization', 'alignment tree', 'tokens', 'infobox', 'sentence planning', 'surface realization', 'content selection', 'content selection model', 'selection model', 'generative models', 'Sentence planning', 'sentence planning', 'sentence planning surface', 'surface realization', 'content selection', 'content selection surface', 'surface realization', 'language models', 'neural language models', 'machine translation', 'use encoder', 'encoder', 'encoder decoder', 'decoder', 'style neural network', 'neural network', 'neural network model', 'network model', 'LSTM', 'LSTM units', 'units attention', 'attention', 'attention mechanism', 'infobox', 'language models', 'Conditional language models', 'language model', 'Language model', 'language model', 'vocabulary language model', 'language model', 'gram language model', 'Language model', 'assumption Language model', 'language models', 'Section model', 'tokens', 'token', 'token', 'token', 'token', 'token', 'token', 'tokens', 'weight', 'weight height', 'tokens', 'help model', 'position help model', 'language model', 'context language model', 'Copy actions', 'model conditioning', 'tokens', 'tokens', 'token', 'work attention', 'attention', 'neural machine translation', 'machine translation', 'dialog systems', 'language models', 'dialog systems', 'language model', 'neural language model', 'parametric function', 'differentiable functions', 'functions layers', 'notations layers', 'letters vectors', 'vectors', 'vector', 'vectors', 'column vectors', 'vectors', 'vector', 'different layers', 'approach Embedding', 'matrix embedding', 'array', 'Linear layer', 'linear transformation', 'weight', 'weight matrix', 'Softmax', 'Softmax layer', 'final layer', 'softmax', 'activation function', 'softmax activation function', 'language models', 'neural language models', 'word embeddings', 'similar embeddings', 'smooth functions', 'functions embeddings', 'language models', 'neural language models', 'vector', 'input vectors', 'vectors', 'word embeddings', 'Table embeddings', 'language model', 'table Embedding', 'vectors', 'vectors', 'vectors', 'refer embedding', 'vectors', 'field vector', 'vector', 'word vector', 'vector', 'Aggregating embeddings', 'end embeddings', 'start embeddings', 'position embeddings', 'field embedding', 'fields embedding', 'yields vectors', 'vectors', 'final embedding', 'concatenation vectors', 'vectors', 'context embedding', 'linear layer', 'pair embedding', 'space latent', 'operation embedding', 'vector', 'dot product', 'context vector', 'vector', 'infobox', 'CoreNLP', 'token', 'token', 'tokens', 'tokens', 'tokens', 'language model', 'gram models', 'train gram models', 'language model', 'softmax', 'softmax function', 'token', 'language model', 'neural language model', 'log likelihood', 'likelihood', 'inference decoder', 'decoder', 'gradient descent', 'tokens', 'token', 'neural network', 'neural network model', 'network model', 'neural models', 'language models', 'gram language models', 'Local field', 'Local field', 'Local field', 'Global field', 'Global field', 'copy actions', 'copy actions', 'neural models', 'beam width', 'tokens', 'tokens', 'tokens', 'language modeling', 'metric language modeling', 'field embedding', 'global embedding', 'hyper parameters', 'hyperparameters', 'tokens', 'tokens', 'word embeddings', 'performance models', 'ordering models', 'performance ordering models', 'copy actions', 'copy actions', 'factorization', 'factorization models', 'straightforward models', 'copy actions', 'copy actions', 'language model', 'neural language model', 'attention', 'attention scores', 'infobox', 'beam size', 'language model', 'copy actions', 'copy actions', 'language models', 'neural language models', 'best model', 'beam sizes', 'BLEU score', 'Attention', 'Attention mechanism', 'attention', 'attention input', 'language model', 'dot product', 'tokens', 'types model', 'field types model', 'beam search', 'greedy search', 'language model', 'steps language model', 'beam settings', 'beam size', 'Beam search', 'ngram', 'Local field', 'Global field', 'Global field', 'language model', 'neural language model', 'infobox', 'neural models', 'variants model', 'versions model', 'words model', 'conditioning words model', 'language model', 'token', 'encoder', 'recurrent encoder', 'encoder', 'convolutional encoder', 'encoder', 'loss function', 'training loss function', 'loss function', 'determiner loss function', 'machine translation', 'content selection', 'content selection concept', 'nlg', 'generationspace models', 'probabilistic generationspace models', 'language model', 'probabilistic language model', 'robust neural network', 'neural network', 'joint models', 'statistical machine translation', 'machine translation', 'fitness function', 'ontologies', 'knowledge bases', 'language model', 'semantic embeddings', 'language models', 'neural language models', 'global model', 'backprop', 'string model', 'forestto string model', 'neural machine translation', 'machine translation', 'CoreNLP', 'dialog systems', 'application machine translation', 'machine translation', 'machine translation summarization', 'generator', 'parser', 'statistical machine translation', 'machine translation', 'visual attention', 'attention', 'Attention', 'Attention intention', 'intention neural network', 'neural network', 'neural network conversation', 'conversation model', 'network conversation model']\n"
     ]
    }
   ],
   "source": [
    "p1 = [{'LOWER': 'encoder'}, {'IS_PUNCT': True, 'OP':'?'}, {'LOWER': 'decoder'}]\n",
    "p2 = [{\"POS\": {\"IN\": [\"NOUN\", \"ADJ\"]}}, {'LOWER': 'encoder'}]\n",
    "p3 = [{\"POS\": {\"IN\": [\"NOUN\", \"ADJ\"]}}, {'LOWER': 'decoder'}]\n",
    "p4 = [{'LOWER': 'encoder'}, {'IS_PUNCT': True}, {'LOWER': 'decoder'},{\"POS\": {\"IN\": [\"NOUN\"]}}]\n",
    "p5 = [{'LOWER': 'encoder'}]\n",
    "p6 = [{'LOWER': 'decoder'}]\n",
    "p7 = [{'LEMMA': 'encoder'}]\n",
    "p8 = [{'LEMMA': 'decoder'}]\n",
    "p9 = [{'LOWER': 'token'}]\n",
    "p10 = [{'LEMMA': 'token'}]\n",
    "p11 = [{'LOWER': 'infobox'}]\n",
    "p12 = [{'LOWER': 'info'}, {'IS_PUNCT': True, 'OP':'?'} , {'LOWER': 'box'}]\n",
    "p13 = [{'LOWER': 'machine'}, {'LOWER': 'translation'},{'POS': 'NOUN','OP':'?'}]\n",
    "p14 = [{\"POS\": {\"IN\": [\"NOUN\", \"ADJ\"]}}, {'LOWER': 'machine'}, {'LOWER': 'translation'}]\n",
    "p15 = [{'LOWER': 'nlg'}]\n",
    "p16 = [{'LOWER': 'natural'},{'IS_PUNCT': True}, {'LOWER': 'language'},{'IS_PUNCT': True}, {'LOWER': 'generation'}]\n",
    "p17 = [{'LOWER': 'neural'}, {'LOWER': 'network'},{'POS': 'NOUN','OP':'?'}]\n",
    "p18 = [{\"POS\": {\"IN\": [\"NOUN\", \"ADJ\"]}}, {'LOWER': 'neural'}, {'LOWER': 'network'}]\n",
    "p19 = [{\"POS\": {\"IN\": [\"NOUN\", \"ADJ\"]}}, {'LOWER': 'embedding'}]\n",
    "p20 = [{\"POS\": {\"IN\": [\"NOUN\", \"ADJ\"]}}, {'LEMMA': 'embedding'}]\n",
    "p21 = [{'LOWER': 'beam'}, {'POS': 'NOUN'}]\n",
    "p22 = [{'LOWER': 'ngram'}]\n",
    "p23 = [{'LOWER': 'n'}, {'LOWER': 'gram'}]\n",
    "p24 = [{'LOWER': 'n'},{'IS_PUNCT': True, 'OP':'?'}, {'LOWER': 'gram'}]\n",
    "p25 = [{'LOWER':'generator'}]\n",
    "p26 = [{'LOWER':'parser'}]\n",
    "p27 = [{'LOWER':'ontology'}]\n",
    "p28 = [{'LOWER':'softmax'}]\n",
    "p29 = [{'LEMMA':'ontology'}]\n",
    "p30 = [{'LOWER':'backprop'}]\n",
    "p31 = [{'LOWER':'token'}]\n",
    "p32 = [{'LOWER':'vector'}]\n",
    "p33 = [{'LOWER':'array'}]\n",
    "p34 = [{'LOWER':'copy'}, {'LEMMA':'action'},{'POS': 'NOUN','OP':'?'} ]\n",
    "p35 = [{'LOWER':'copy'}, {'LOWER':'action'},{'POS': 'NOUN','OP':'?'} ]\n",
    "p36 = [{'LOWER':'sochastic','OP':'?'}, {'LOWER':'gradient'}, {'LOWER':'descent'}]\n",
    "p37 = [{'LOWER':'attention'}, {'POS': 'NOUN','OP':'?'}]\n",
    "p38 = [{\"POS\": {\"IN\": [\"NOUN\",\"ADJ\"]}}, {'LOWER':'attention'}]\n",
    "p39 = [{\"POS\": {\"IN\": [\"NOUN\",\"ADJ\",\"PROPN\"]}},{'POS': 'NOUN','OP':'?'}, {'LOWER': 'function'}]\n",
    "p40 = [{\"POS\": {\"IN\": [\"NOUN\",\"ADJ\",\"PROPN\"]}},{'POS': 'NOUN','OP':'?'}, {'LEMMA': 'function'}]\n",
    "p41 = [{\"POS\": {\"IN\": [\"NOUN\",\"ADJ\",\"PROPN\"]}}, {'LOWER': 'layer'}]\n",
    "p42 = [{\"POS\": {\"IN\": [\"NOUN\",\"ADJ\",\"PROPN\"]}}, {'LEMMA': 'layer'}]\n",
    "p43 = [{\"POS\": {\"IN\": [\"NOUN\",\"ADJ\"]}}, {'LOWER': 'search'}]\n",
    "p44 = [{'POS': 'NOUN','OP':'?'}, {'LOWER':'vector'}]\n",
    "p45 = [{'POS': 'NOUN','OP':'?'}, {'LEMMA':'vector'}]\n",
    "p46 = [{'POS': 'COMP','OP':'?'}, {'LOWER':'knowledge'},{'IS_PUNCT': True, 'OP':'?'}, {'LOWER':'base'}]\n",
    "p47 = [{'LOWER':'knowledge'},{'IS_PUNCT': True, 'OP':'?'}, {'LOWER':'bases'}]\n",
    "p48 = [{'LOWER':'dialog'}, {'LOWER':'system'}, {'POS': 'NOUN','OP':'?'} ]\n",
    "p49 = [{'LOWER':'dialog'}, {'LEMMA':'system'}, {'POS': 'NOUN','OP':'?'} ]\n",
    "p50 = [{'LOWER':'local'}, {'LOWER':'field'}]\n",
    "p51 = [{'LOWER':'global'}, {'LOWER':'field'}]\n",
    "p52 = [{'LOWER':'dot'}, {'LOWER':'product'}]\n",
    "p53 = [{'LOWER':'statistical'}, {'LOWER':'generation'}, {'LOWER':'model'}]\n",
    "p54 = [{'LOWER':'alignment'}, {'LEMMA':'tree'}]\n",
    "p55 = [{'LOWER':'lstm'}, {'POS': 'NOUN','OP':'?'}]\n",
    "p56 = [{'LOWER':'linear'}, {'LOWER': 'transformation'}]\n",
    "p57 = [{'POS': 'NOUN','OP':'?'}, {'LOWER':'likelihood'}, ]\n",
    "p58 = [{'LOWER':'computational'}, {'LEMMA': 'linguistic'}]\n",
    "p59 = [{'LOWER':'bleu'}, {'LEMMA': 'score'}]\n",
    "p60 = [{'LOWER':'hyperparameter'}]\n",
    "p61 = [{'LOWER':'corenlp'}]\n",
    "p62 = [{'LEMMA':'hyperparameter'}]\n",
    "p63 = [{'LOWER':'hyper'}, {'IS_PUNCT': True, 'OP':'?'}, {'lemma':'parameter'} ]\n",
    "p64 = [{'LOWER':'content'}, {'LOWER':'selection'}, {'POS': 'NOUN','OP':'?'} ]\n",
    "p65 = [{'LOWER':'sentence'}, {'LOWER':'planning'}, {'POS': 'NOUN','OP':'?'} ]\n",
    "p66 = [{'LOWER':'factorization'}, {'POS': 'NOUN','OP':'?'} ]\n",
    "p67 = [{'LEMMA':'weight'}, {'POS': 'NOUN','OP':'?'} ]\n",
    "p68 = [{'LOWER':'space'}, {'LOWER': 'latent'}]\n",
    "p69 = [{\"POS\": {\"IN\": [\"NOUN\",\"ADJ\"]}},{'POS': 'NOUN','OP':'?'}, {'LEMMA':'model'}]\n",
    "p70 = [{'LOWER':'surface'}, {'LOWER':'realization'}, {'POS': 'NOUN','OP':'?'} ]\n",
    "p71 = [{'LOWER':'model'}, {'LOWER':'conditioning'}]\n",
    "\n",
    "string_matcher = Matcher(nlp.vocab)\n",
    "\n",
    "string_matcher.add('rulebased', [p1,p2,p3,p4,p5,p6,p7,p8,p9,p10,p11,p12,p13,p14,p15,p16,p17,p18,p19,p20,p21,p22,p23,p24,p25,p26,p27,\n",
    "                        p28,p29,p30,p31,p32,p33,p34,p35,p36,p37,p38,p39,p40,p41,p42,p43,p44,p45,p46,p47,p48,p49,p50,p51,p52,\n",
    "                        p53,p54,p55,p56,p57,p58,p59,p60,p61,p62,p63,p64,p65,p66,p67,p68,p69,p70,p71])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "string_matches = string_matcher(data)\n",
    "\n",
    "# print(string_matches)\n",
    "\n",
    "d1=[]\n",
    "for match_id, start, end in string_matches:\n",
    "    rule_id = nlp.vocab.strings[match_id]  # get the unicode ID, i.e. 'COLOR'\n",
    "    span = data[start : end]  # get the matched slice of the doc\n",
    "    d1.append(span.text)\n",
    "    keyterm = span.text\n",
    "print(d1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('language model', 19),\n",
       " ('tokens', 16),\n",
       " ('token', 12),\n",
       " ('language models', 10),\n",
       " ('vectors', 10),\n",
       " ('copy actions', 9),\n",
       " ('infobox', 7),\n",
       " ('machine translation', 7),\n",
       " ('vector', 7),\n",
       " ('neural language models', 6),\n",
       " ('neural language model', 5),\n",
       " ('attention', 5),\n",
       " ('encoder', 4),\n",
       " ('neural network', 4),\n",
       " ('Local field', 4),\n",
       " ('Global field', 4),\n",
       " ('surface realization', 3),\n",
       " ('content selection', 3),\n",
       " ('dialog systems', 3),\n",
       " ('word embeddings', 3),\n",
       " ('neural models', 3),\n",
       " ('neural model', 2),\n",
       " ('factorization', 2),\n",
       " ('sentence planning', 2),\n",
       " ('decoder', 2),\n",
       " ('neural network model', 2),\n",
       " ('network model', 2),\n",
       " ('Language model', 2),\n",
       " ('weight', 2),\n",
       " ('neural machine translation', 2),\n",
       " ('softmax', 2),\n",
       " ('field embedding', 2),\n",
       " ('dot product', 2),\n",
       " ('CoreNLP', 2),\n",
       " ('beam size', 2),\n",
       " ('Attention', 2),\n",
       " ('loss function', 2),\n",
       " ('statistical machine translation', 2),\n",
       " ('extend models', 1),\n",
       " ('vocabulary extend models', 1),\n",
       " ('knowledge base', 1),\n",
       " ('statistical generation model', 1),\n",
       " ('generation model', 1),\n",
       " ('alignment tree', 1),\n",
       " ('content selection model', 1),\n",
       " ('selection model', 1),\n",
       " ('generative models', 1),\n",
       " ('Sentence planning', 1),\n",
       " ('sentence planning surface', 1),\n",
       " ('content selection surface', 1),\n",
       " ('use encoder', 1),\n",
       " ('encoder decoder', 1),\n",
       " ('style neural network', 1),\n",
       " ('LSTM', 1),\n",
       " ('LSTM units', 1),\n",
       " ('units attention', 1),\n",
       " ('attention mechanism', 1),\n",
       " ('Conditional language models', 1),\n",
       " ('vocabulary language model', 1),\n",
       " ('gram language model', 1),\n",
       " ('assumption Language model', 1),\n",
       " ('Section model', 1),\n",
       " ('weight height', 1),\n",
       " ('help model', 1),\n",
       " ('position help model', 1),\n",
       " ('context language model', 1),\n",
       " ('Copy actions', 1),\n",
       " ('model conditioning', 1),\n",
       " ('work attention', 1),\n",
       " ('parametric function', 1),\n",
       " ('differentiable functions', 1),\n",
       " ('functions layers', 1),\n",
       " ('notations layers', 1),\n",
       " ('letters vectors', 1),\n",
       " ('column vectors', 1),\n",
       " ('different layers', 1),\n",
       " ('approach Embedding', 1),\n",
       " ('matrix embedding', 1),\n",
       " ('array', 1),\n",
       " ('Linear layer', 1),\n",
       " ('linear transformation', 1),\n",
       " ('weight matrix', 1),\n",
       " ('Softmax', 1),\n",
       " ('Softmax layer', 1),\n",
       " ('final layer', 1),\n",
       " ('activation function', 1),\n",
       " ('softmax activation function', 1),\n",
       " ('similar embeddings', 1),\n",
       " ('smooth functions', 1),\n",
       " ('functions embeddings', 1),\n",
       " ('input vectors', 1),\n",
       " ('Table embeddings', 1),\n",
       " ('table Embedding', 1),\n",
       " ('refer embedding', 1),\n",
       " ('field vector', 1),\n",
       " ('word vector', 1),\n",
       " ('Aggregating embeddings', 1),\n",
       " ('end embeddings', 1),\n",
       " ('start embeddings', 1),\n",
       " ('position embeddings', 1),\n",
       " ('fields embedding', 1),\n",
       " ('yields vectors', 1),\n",
       " ('final embedding', 1),\n",
       " ('concatenation vectors', 1),\n",
       " ('context embedding', 1),\n",
       " ('linear layer', 1),\n",
       " ('pair embedding', 1),\n",
       " ('space latent', 1),\n",
       " ('operation embedding', 1),\n",
       " ('context vector', 1),\n",
       " ('gram models', 1),\n",
       " ('train gram models', 1),\n",
       " ('softmax function', 1),\n",
       " ('log likelihood', 1),\n",
       " ('likelihood', 1),\n",
       " ('inference decoder', 1),\n",
       " ('gradient descent', 1),\n",
       " ('gram language models', 1),\n",
       " ('beam width', 1),\n",
       " ('language modeling', 1),\n",
       " ('metric language modeling', 1),\n",
       " ('global embedding', 1),\n",
       " ('hyper parameters', 1),\n",
       " ('hyperparameters', 1),\n",
       " ('performance models', 1),\n",
       " ('ordering models', 1),\n",
       " ('performance ordering models', 1),\n",
       " ('factorization models', 1),\n",
       " ('straightforward models', 1),\n",
       " ('attention scores', 1),\n",
       " ('best model', 1),\n",
       " ('beam sizes', 1),\n",
       " ('BLEU score', 1),\n",
       " ('Attention mechanism', 1),\n",
       " ('attention input', 1),\n",
       " ('types model', 1),\n",
       " ('field types model', 1),\n",
       " ('beam search', 1),\n",
       " ('greedy search', 1),\n",
       " ('steps language model', 1),\n",
       " ('beam settings', 1),\n",
       " ('Beam search', 1),\n",
       " ('ngram', 1),\n",
       " ('variants model', 1),\n",
       " ('versions model', 1),\n",
       " ('words model', 1),\n",
       " ('conditioning words model', 1),\n",
       " ('recurrent encoder', 1),\n",
       " ('convolutional encoder', 1),\n",
       " ('training loss function', 1),\n",
       " ('determiner loss function', 1),\n",
       " ('content selection concept', 1),\n",
       " ('nlg', 1),\n",
       " ('generationspace models', 1),\n",
       " ('probabilistic generationspace models', 1),\n",
       " ('probabilistic language model', 1),\n",
       " ('robust neural network', 1),\n",
       " ('joint models', 1),\n",
       " ('fitness function', 1),\n",
       " ('ontologies', 1),\n",
       " ('knowledge bases', 1),\n",
       " ('semantic embeddings', 1),\n",
       " ('global model', 1),\n",
       " ('backprop', 1),\n",
       " ('string model', 1),\n",
       " ('forestto string model', 1),\n",
       " ('application machine translation', 1),\n",
       " ('machine translation summarization', 1),\n",
       " ('generator', 1),\n",
       " ('parser', 1),\n",
       " ('visual attention', 1),\n",
       " ('Attention intention', 1),\n",
       " ('intention neural network', 1),\n",
       " ('neural network conversation', 1),\n",
       " ('conversation model', 1),\n",
       " ('network conversation model', 1)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_terms = Counter(d1)\n",
    "rank_term = total_terms.most_common() #list all\n",
    "# total_terms.most_common()[:200] #top 200\n",
    "rank_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           terms  frequency\n",
      "0                 language model         19\n",
      "1                         tokens         16\n",
      "2                          token         12\n",
      "3                language models         10\n",
      "4                        vectors         10\n",
      "..                           ...        ...\n",
      "171          Attention intention          1\n",
      "172     intention neural network          1\n",
      "173  neural network conversation          1\n",
      "174           conversation model          1\n",
      "175   network conversation model          1\n",
      "\n",
      "[176 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(rank_term, columns=['terms', 'frequency'])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define iob tagging rule-based system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "def iob_rule(all_corpora,silver_corpus):\n",
    "\n",
    "\n",
    "    tokens = word_tokenize(all_corpora)\n",
    "    offset = 0\n",
    "    entities = []\n",
    "    i = 0\n",
    "    while i in  range(len(tokens)):\n",
    "        offset = all_corpora[0].find(tokens[i], offset)\n",
    "        if i < len(tokens) - 3 and \" \".join(tokens[i:i+3]) in silver_corpus:\n",
    "            entities.append((offset,offset+len(tokens[i]),'B'))\n",
    "            entities.append((offset+len(tokens[i])+1,offset+len(tokens[i])+len(tokens[i+1])+1,'I'))\n",
    "            entities.append((offset+len(tokens[i])+len(tokens[i+1])+1,offset+len(tokens[i])+len(tokens[i+1])+len(tokens[i+2])+2,'I'))\n",
    "            #offset = offset+len(tokens[i])+len(tokens[i+1])\n",
    "            i = i+3\n",
    "        elif i < len(tokens) - 2 and \" \".join(tokens[i:i+2]) in silver_corpus:\n",
    "            entities.append((offset,offset+len(tokens[i]),'B'))\n",
    "            entities.append((offset+len(tokens[i])+1,offset+len(tokens[i])+len(tokens[i+1])+1,'I'))\n",
    "            i = i+2 \n",
    "        elif i < len(tokens) - 1 and \" \".join(tokens[i:i+1]) in silver_corpus:\n",
    "            entities.append((offset,offset+len(tokens[i]),'B'))\n",
    "            i = i+1\n",
    "        else:\n",
    "            entities.append((offset,offset+len(tokens[i]),'O'))\n",
    "            i = i+1\n",
    "    return(tokens,entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_corpora = data.text\n",
    "silver_corpus = d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Neural',\n",
       "  'Text',\n",
       "  'Generation',\n",
       "  'Structured',\n",
       "  'Data',\n",
       "  'Application',\n",
       "  'Biography',\n",
       "  'Domain',\n",
       "  'Lebret',\n",
       "  'EPFL',\n",
       "  'Switzerland',\n",
       "  'David',\n",
       "  'Grangier',\n",
       "  'Facebook',\n",
       "  'Research',\n",
       "  'Abstract',\n",
       "  'This',\n",
       "  'paper',\n",
       "  'introduces',\n",
       "  'neural',\n",
       "  'model',\n",
       "  'concept',\n",
       "  'text',\n",
       "  'generation',\n",
       "  'scales',\n",
       "  'large',\n",
       "  'rich',\n",
       "  'domains',\n",
       "  'generates',\n",
       "  'biographical',\n",
       "  'sentences',\n",
       "  'fact',\n",
       "  'tables',\n",
       "  'new',\n",
       "  'dataset',\n",
       "  'biographies',\n",
       "  'Wikipedia',\n",
       "  'This',\n",
       "  'set',\n",
       "  'order',\n",
       "  'magnitude',\n",
       "  'larger',\n",
       "  'existing',\n",
       "  'resources',\n",
       "  'samples',\n",
       "  'vocabulary',\n",
       "  'Our',\n",
       "  'model',\n",
       "  'builds',\n",
       "  'conditional',\n",
       "  'neural',\n",
       "  'language',\n",
       "  'models',\n",
       "  'text',\n",
       "  'generation',\n",
       "  'deal',\n",
       "  'large',\n",
       "  'vocabulary',\n",
       "  'extend',\n",
       "  'models',\n",
       "  'mix',\n",
       "  'fixed',\n",
       "  'vocabulary',\n",
       "  'copy',\n",
       "  'actions',\n",
       "  'transfer',\n",
       "  'sample',\n",
       "  'specific',\n",
       "  'words',\n",
       "  'input',\n",
       "  'database',\n",
       "  'generated',\n",
       "  'output',\n",
       "  'sentence',\n",
       "  'deal',\n",
       "  'structured',\n",
       "  'data',\n",
       "  'allow',\n",
       "  'model',\n",
       "  'embed',\n",
       "  'words',\n",
       "  'differently',\n",
       "  'depending',\n",
       "  'data',\n",
       "  'fields',\n",
       "  'occur',\n",
       "  'Our',\n",
       "  'neural',\n",
       "  'model',\n",
       "  'significantly',\n",
       "  'outperforms',\n",
       "  'Templated',\n",
       "  'Kneser',\n",
       "  'Ney',\n",
       "  'language',\n",
       "  'model',\n",
       "  'nearly',\n",
       "  'BLEU',\n",
       "  'Introduction',\n",
       "  'Concept',\n",
       "  'text',\n",
       "  'generation',\n",
       "  'renders',\n",
       "  'structured',\n",
       "  'records',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'Reiter',\n",
       "  'typical',\n",
       "  'application',\n",
       "  'generate',\n",
       "  'weather',\n",
       "  'forecast',\n",
       "  'based',\n",
       "  'set',\n",
       "  'structured',\n",
       "  'meteorological',\n",
       "  'measurements',\n",
       "  'contrast',\n",
       "  'previous',\n",
       "  'work',\n",
       "  'scale',\n",
       "  'large',\n",
       "  'diverse',\n",
       "  'problem',\n",
       "  'generating',\n",
       "  'biographies',\n",
       "  'based',\n",
       "  'Wikipedia',\n",
       "  'infoboxes',\n",
       "  'infobox',\n",
       "  'fact',\n",
       "  'table',\n",
       "  'describing',\n",
       "  'person',\n",
       "  'similar',\n",
       "  'person',\n",
       "  'subgraph',\n",
       "  'knowledge',\n",
       "  'base',\n",
       "  'Bollacker',\n",
       "  'Ferrucci',\n",
       "  'Similar',\n",
       "  'generation',\n",
       "  'applications',\n",
       "  'include',\n",
       "  'generation',\n",
       "  'product',\n",
       "  'descriptions',\n",
       "  'based',\n",
       "  'catalog',\n",
       "  'millions',\n",
       "  'items',\n",
       "  'dozens',\n",
       "  'attributes',\n",
       "  'Previous',\n",
       "  'work',\n",
       "  'experimented',\n",
       "  'datasets',\n",
       "  'contain',\n",
       "  'tens',\n",
       "  'thousands',\n",
       "  'records',\n",
       "  'EATHERGOV',\n",
       "  'ROBOCUP',\n",
       "  'dataset',\n",
       "  'dataset',\n",
       "  'contains',\n",
       "  'biographies',\n",
       "  'performed',\n",
       "  'work',\n",
       "  'interning',\n",
       "  'Facebook',\n",
       "  'Michael',\n",
       "  'Auli',\n",
       "  'Facebook',\n",
       "  'Research',\n",
       "  'Wikipedia',\n",
       "  'Furthermore',\n",
       "  'datasets',\n",
       "  'limited',\n",
       "  'vocabulary',\n",
       "  'words',\n",
       "  'compared',\n",
       "  'words',\n",
       "  'dataset',\n",
       "  'tackle',\n",
       "  'problem',\n",
       "  'introduce',\n",
       "  'statistical',\n",
       "  'generation',\n",
       "  'model',\n",
       "  'conditioned',\n",
       "  'Wikipedia',\n",
       "  'infobox',\n",
       "  'focus',\n",
       "  'generation',\n",
       "  'first',\n",
       "  'sentence',\n",
       "  'biography',\n",
       "  'requires',\n",
       "  'model',\n",
       "  'select',\n",
       "  'among',\n",
       "  'large',\n",
       "  'number',\n",
       "  'possible',\n",
       "  'fields',\n",
       "  'generate',\n",
       "  'adequate',\n",
       "  'output',\n",
       "  'Such',\n",
       "  'diversity',\n",
       "  'makes',\n",
       "  'difficult',\n",
       "  'classical',\n",
       "  'count',\n",
       "  'based',\n",
       "  'models',\n",
       "  'estimate',\n",
       "  'probabilities',\n",
       "  'rare',\n",
       "  'events',\n",
       "  'due',\n",
       "  'data',\n",
       "  'sparsity',\n",
       "  'address',\n",
       "  'issue',\n",
       "  'parameterizing',\n",
       "  'words',\n",
       "  'fields',\n",
       "  'embeddings',\n",
       "  'along',\n",
       "  'neural',\n",
       "  'language',\n",
       "  'model',\n",
       "  'operating',\n",
       "  'Bengio',\n",
       "  'This',\n",
       "  'factorization',\n",
       "  'allows',\n",
       "  'scale',\n",
       "  'larger',\n",
       "  'number',\n",
       "  'words',\n",
       "  'fields',\n",
       "  'Liang',\n",
       "  'Kim',\n",
       "  'Mooney',\n",
       "  'number',\n",
       "  'parameters',\n",
       "  'grows',\n",
       "  'product',\n",
       "  'number',\n",
       "  'words',\n",
       "  'fields',\n",
       "  'Moreover',\n",
       "  'approach',\n",
       "  'restrict',\n",
       "  'relations',\n",
       "  'field',\n",
       "  'contents',\n",
       "  'generated',\n",
       "  'text',\n",
       "  'This',\n",
       "  'contrasts',\n",
       "  'less',\n",
       "  'flexible',\n",
       "  'strategies',\n",
       "  'assume',\n",
       "  'generation',\n",
       "  'follow',\n",
       "  'either',\n",
       "  'hybrid',\n",
       "  'alignment',\n",
       "  'tree',\n",
       "  'Kim',\n",
       "  'Mooney',\n",
       "  'probabilistic',\n",
       "  'context',\n",
       "  'free',\n",
       "  'grammar',\n",
       "  'Konstas',\n",
       "  'Lapata',\n",
       "  'tree',\n",
       "  'adjoining',\n",
       "  'grammar',\n",
       "  'Gyawali',\n",
       "  'Gardent',\n",
       "  'Our',\n",
       "  'model',\n",
       "  'exploits',\n",
       "  'structured',\n",
       "  'data',\n",
       "  'globally',\n",
       "  'locally',\n",
       "  'Global',\n",
       "  'conditioning',\n",
       "  'summarizes',\n",
       "  'information',\n",
       "  'personality',\n",
       "  'understand',\n",
       "  'highlevel',\n",
       "  'themes',\n",
       "  'biography',\n",
       "  'scientist',\n",
       "  'artist',\n",
       "  'local',\n",
       "  'conditioning',\n",
       "  'describes',\n",
       "  'previously',\n",
       "  'generated',\n",
       "  'tokens',\n",
       "  'terms',\n",
       "  'relationship',\n",
       "  'infobox',\n",
       "  'analyze',\n",
       "  'effectiveness',\n",
       "  'demonstrate',\n",
       "  'complementarity',\n",
       "  'Related',\n",
       "  'Work',\n",
       "  'Traditionally',\n",
       "  'generation',\n",
       "  'systems',\n",
       "  'relied',\n",
       "  'rules',\n",
       "  'hand',\n",
       "  'crafted',\n",
       "  'specifications',\n",
       "  'Dale',\n",
       "  'Reiter',\n",
       "  'Green',\n",
       "  'Galanis',\n",
       "  'Androut',\n",
       "  'Proceedings',\n",
       "  'Conference',\n",
       "  'Empirical',\n",
       "  'Methods',\n",
       "  'Natural',\n",
       "  'Language',\n",
       "  'Processing',\n",
       "  'pages',\n",
       "  'Austin',\n",
       "  'Texas',\n",
       "  'November',\n",
       "  'Association',\n",
       "  'Computational',\n",
       "  'Linguistics',\n",
       "  'sopoulos',\n",
       "  'Turner',\n",
       "  'Generation',\n",
       "  'divided',\n",
       "  'modular',\n",
       "  'yet',\n",
       "  'highly',\n",
       "  'interdependent',\n",
       "  'decisions',\n",
       "  'content',\n",
       "  'planning',\n",
       "  'defines',\n",
       "  'parts',\n",
       "  'input',\n",
       "  'fields',\n",
       "  'meaning',\n",
       "  'representations',\n",
       "  'selected',\n",
       "  'sentence',\n",
       "  'planning',\n",
       "  'determines',\n",
       "  'selected',\n",
       "  'fields',\n",
       "  'dealt',\n",
       "  'output',\n",
       "  'sentence',\n",
       "  'surface',\n",
       "  'realization',\n",
       "  'generates',\n",
       "  'sentences',\n",
       "  'Data',\n",
       "  'driven',\n",
       "  'approaches',\n",
       "  'proposed',\n",
       "  'automatically',\n",
       "  'learn',\n",
       "  'individual',\n",
       "  'modules',\n",
       "  'One',\n",
       "  'approach',\n",
       "  'first',\n",
       "  'aligns',\n",
       "  'records',\n",
       "  'sentences',\n",
       "  'learns',\n",
       "  'content',\n",
       "  'selection',\n",
       "  'model',\n",
       "  'Duboue',\n",
       "  'McKeown',\n",
       "  'Barzilay',\n",
       "  'Lapata',\n",
       "  'Hierarchical',\n",
       "  'hidden',\n",
       "  'semi',\n",
       "  'Markov',\n",
       "  'generative',\n",
       "  'models',\n",
       "  'also',\n",
       "  'used',\n",
       "  'first',\n",
       "  'determine',\n",
       "  'facts',\n",
       "  'discuss',\n",
       "  'generate',\n",
       "  'words',\n",
       "  'predicates',\n",
       "  'arguments',\n",
       "  'chosen',\n",
       "  'facts',\n",
       "  'Liang',\n",
       "  'Sentence',\n",
       "  'planning',\n",
       "  'formulated',\n",
       "  'supervised',\n",
       "  'set',\n",
       "  'partitioning',\n",
       "  'problem',\n",
       "  'facts',\n",
       "  'partition',\n",
       "  'corresponds',\n",
       "  'sentence',\n",
       "  'Barzilay',\n",
       "  'Lapata',\n",
       "  'End',\n",
       "  'end',\n",
       "  'approaches',\n",
       "  'combined',\n",
       "  'sentence',\n",
       "  'planning',\n",
       "  'surface',\n",
       "  'realization',\n",
       "  'using',\n",
       "  'explicitly',\n",
       "  'aligned',\n",
       "  'sentence',\n",
       "  'meaning',\n",
       "  'pairs',\n",
       "  'training',\n",
       "  'data',\n",
       "  'Ratnaparkhi',\n",
       "  'Wong',\n",
       "  'Mooney',\n",
       "  'Belz',\n",
       "  'More',\n",
       "  'recently',\n",
       "  'content',\n",
       "  'selection',\n",
       "  'surface',\n",
       "  'realization',\n",
       "  'combined',\n",
       "  'Angeli',\n",
       "  'Kim',\n",
       "  'Mooney',\n",
       "  'Konstas',\n",
       "  'Lapata',\n",
       "  'intersection',\n",
       "  'rule',\n",
       "  'based',\n",
       "  'statistical',\n",
       "  'methods',\n",
       "  'hybrid',\n",
       "  'systems',\n",
       "  'aim',\n",
       "  'leveraging',\n",
       "  'human',\n",
       "  'contributed',\n",
       "  'rules',\n",
       "  'corpus',\n",
       "  'statistics',\n",
       "  'Langkilde',\n",
       "  'Knight',\n",
       "  'Soricut',\n",
       "  'Marcu',\n",
       "  'Mairesse',\n",
       "  'Walker',\n",
       "  'Our',\n",
       "  'approach',\n",
       "  'inspired',\n",
       "  'recent',\n",
       "  'success',\n",
       "  'neural',\n",
       "  'language',\n",
       "  'models',\n",
       "  'image',\n",
       "  'captioning',\n",
       "  'Kiros',\n",
       "  'Karpathy',\n",
       "  'Fei',\n",
       "  'Fei',\n",
       "  'Vinyals',\n",
       "  'Fang',\n",
       "  'machine',\n",
       "  'translation',\n",
       "  'Devlin',\n",
       "  'Bahdanau',\n",
       "  'Luong',\n",
       "  'modeling',\n",
       "  'conversations',\n",
       "  'dialogues',\n",
       "  'Shang',\n",
       "  'Wen',\n",
       "  'Yao',\n",
       "  'Our',\n",
       "  'model',\n",
       "  'similar',\n",
       "  'Mei',\n",
       "  'use',\n",
       "  'encoder',\n",
       "  'decoder',\n",
       "  'style',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'model',\n",
       "  'tackle',\n",
       "  'EATHERGOV',\n",
       "  'ROBOCUP',\n",
       "  'tasks',\n",
       "  'Their',\n",
       "  'architecture',\n",
       "  'relies',\n",
       "  'LSTM',\n",
       "  'units',\n",
       "  'attention',\n",
       "  'mechanism',\n",
       "  'reduces',\n",
       "  'scalability',\n",
       "  'compared',\n",
       "  'simpler',\n",
       "  'design',\n",
       "  'Figure',\n",
       "  'Wikipedia',\n",
       "  'infobox',\n",
       "  'Frederick',\n",
       "  'Parker',\n",
       "  'Rhodes',\n",
       "  'The',\n",
       "  'introduction',\n",
       "  'article',\n",
       "  'reads',\n",
       "  'Frederick',\n",
       "  'Parker',\n",
       "  'Rhodes',\n",
       "  'March',\n",
       "  'November',\n",
       "  'English',\n",
       "  'linguist',\n",
       "  'plant',\n",
       "  'pathologist',\n",
       "  'computer',\n",
       "  'scientist',\n",
       "  'mathematician',\n",
       "  'mystic',\n",
       "  'mycologist',\n",
       "  'Language',\n",
       "  'Modeling',\n",
       "  'Constrained',\n",
       "  'Sentence',\n",
       "  'generation',\n",
       "  'Conditional',\n",
       "  'language',\n",
       "  'models',\n",
       "  'popular',\n",
       "  'choice',\n",
       "  'generate',\n",
       "  'sentences',\n",
       "  'introduce',\n",
       "  'tableconditioned',\n",
       "  'language',\n",
       "  'model',\n",
       "  'constraining',\n",
       "  'text',\n",
       "  'generation',\n",
       "  'include',\n",
       "  'elements',\n",
       "  'fact',\n",
       "  'tables',\n",
       "  'Language',\n",
       "  'model',\n",
       "  'Given',\n",
       "  'sentence',\n",
       "  'words',\n",
       "  'vocabulary',\n",
       "  'language',\n",
       "  'model',\n",
       "  'estimates',\n",
       "  'Let',\n",
       "  'sequence',\n",
       "  'context',\n",
       "  'words',\n",
       "  'preceding',\n",
       "  'gram',\n",
       "  'language',\n",
       "  'model',\n",
       "  'makes',\n",
       "  'order',\n",
       "  'Markov',\n",
       "  'assumption',\n",
       "  'Language',\n",
       "  'model',\n",
       "  'conditioned',\n",
       "  'tables',\n",
       "  'table',\n",
       "  'set',\n",
       "  'field',\n",
       "  'value',\n",
       "  'pairs',\n",
       "  'values',\n",
       "  'sequences',\n",
       "  'words',\n",
       "  'therefore',\n",
       "  'propose',\n",
       "  'language',\n",
       "  'models',\n",
       "  'conditioned',\n",
       "  'pairs',\n",
       "  'Local',\n",
       "  'conditioning',\n",
       "  'refers',\n",
       "  'information',\n",
       "  'table',\n",
       "  'applied',\n",
       "  'description',\n",
       "  'words',\n",
       "  'already',\n",
       "  'generated',\n",
       "  'previous',\n",
       "  'words',\n",
       "  'constitute',\n",
       "  'context',\n",
       "  'language',\n",
       "  'input',\n",
       "  'text',\n",
       "  'zct',\n",
       "  'Doe',\n",
       "  'April',\n",
       "  'unk',\n",
       "  'name',\n",
       "  'Table',\n",
       "  'name',\n",
       "  'birthdate',\n",
       "  'birthplace',\n",
       "  'occupation',\n",
       "  'spouse',\n",
       "  'children',\n",
       "  'John',\n",
       "  'John',\n",
       "  'Doe',\n",
       "  'April',\n",
       "  'Oxford',\n",
       "  'placeholder',\n",
       "  'Jane',\n",
       "  'Doe',\n",
       "  'Johnnie',\n",
       "  'Doe',\n",
       "  'zct',\n",
       "  'name',\n",
       "  'spouse',\n",
       "  'children',\n",
       "  'birthd',\n",
       "  'birthd',\n",
       "  'birthd',\n",
       "  'output',\n",
       "  'candidates',\n",
       "  'april',\n",
       "  'placeholder',\n",
       "  'john',\n",
       "  'doe',\n",
       "  'unk',\n",
       "  'birthd',\n",
       "  'occupation',\n",
       "  'name',\n",
       "  'name',\n",
       "  'spouse',\n",
       "  'children',\n",
       "  'Figure',\n",
       "  'Table',\n",
       "  'features',\n",
       "  'right',\n",
       "  'example',\n",
       "  'table',\n",
       "  'left',\n",
       "  'set',\n",
       "  'output',\n",
       "  'words',\n",
       "  'defined',\n",
       "  'Section',\n",
       "  'model',\n",
       "  'The',\n",
       "  'table',\n",
       "  'allows',\n",
       "  'describe',\n",
       "  'word',\n",
       "  'string',\n",
       "  'index',\n",
       "  'vocabulary',\n",
       "  'also',\n",
       "  'descriptor',\n",
       "  'occurrence',\n",
       "  'table',\n",
       "  'Let',\n",
       "  'define',\n",
       "  'set',\n",
       "  'possible',\n",
       "  'fields',\n",
       "  'The',\n",
       "  'occurrence',\n",
       "  'word',\n",
       "  'table',\n",
       "  'described',\n",
       "  'set',\n",
       "  'field',\n",
       "  'position',\n",
       "  'pairs',\n",
       "  'number',\n",
       "  'occurrences',\n",
       "  'Each',\n",
       "  'pair',\n",
       "  'indicates',\n",
       "  'occurs',\n",
       "  'field',\n",
       "  'position',\n",
       "  'scheme',\n",
       "  'words',\n",
       "  'described',\n",
       "  'empty',\n",
       "  'set',\n",
       "  'occur',\n",
       "  'table',\n",
       "  'For',\n",
       "  'example',\n",
       "  'word',\n",
       "  'linguistics',\n",
       "  'table',\n",
       "  'Figure',\n",
       "  'described',\n",
       "  'follows',\n",
       "  'zlinguistics',\n",
       "  'fields',\n",
       "  'known',\n",
       "  'assuming',\n",
       "  'words',\n",
       "  'lower',\n",
       "  'cased',\n",
       "  'commas',\n",
       "  'treated',\n",
       "  'separate',\n",
       "  'tokens',\n",
       "  'Conditioning',\n",
       "  'field',\n",
       "  'type',\n",
       "  'position',\n",
       "  'within',\n",
       "  'field',\n",
       "  'allows',\n",
       "  'model',\n",
       "  'encode',\n",
       "  'field',\n",
       "  'specific',\n",
       "  'regularities',\n",
       "  'number',\n",
       "  'token',\n",
       "  'date',\n",
       "  'field',\n",
       "  'likely',\n",
       "  'followed',\n",
       "  'month',\n",
       "  'token',\n",
       "  'knowing',\n",
       "  'number',\n",
       "  'first',\n",
       "  'token',\n",
       "  'date',\n",
       "  'field',\n",
       "  'makes',\n",
       "  'even',\n",
       "  'likely',\n",
       "  'The',\n",
       "  'field',\n",
       "  'position',\n",
       "  'description',\n",
       "  'scheme',\n",
       "  'table',\n",
       "  'allow',\n",
       "  'express',\n",
       "  'token',\n",
       "  'terminates',\n",
       "  'field',\n",
       "  'useful',\n",
       "  'capture',\n",
       "  'field',\n",
       "  'transitions',\n",
       "  'For',\n",
       "  'biographies',\n",
       "  'last',\n",
       "  'token',\n",
       "  'name',\n",
       "  'field',\n",
       "  'often',\n",
       "  'followed',\n",
       "  'introduction',\n",
       "  'birth',\n",
       "  'date',\n",
       "  'like',\n",
       "  'born',\n",
       "  'hence',\n",
       "  'extend',\n",
       "  'descriptor',\n",
       "  'triplet',\n",
       "  'includes',\n",
       "  'position',\n",
       "  'token',\n",
       "  'counted',\n",
       "  'end',\n",
       "  'field',\n",
       "  'example',\n",
       "  'becomes',\n",
       "  'zlinguistics',\n",
       "  'fields',\n",
       "  'known',\n",
       "  'extend',\n",
       "  'Equation',\n",
       "  'use',\n",
       "  'information',\n",
       "  'additional',\n",
       "  'conditioning',\n",
       "  'context',\n",
       "  'generating',\n",
       "  'sentence',\n",
       "  'zct',\n",
       "  'zct',\n",
       "  'zwt',\n",
       "  'zwt',\n",
       "  'referred',\n",
       "  'local',\n",
       "  'conditioning',\n",
       "  'variables',\n",
       "  'since',\n",
       "  'describe',\n",
       "  'local',\n",
       "  'context',\n",
       "  'previous',\n",
       "  'word',\n",
       "  'relations',\n",
       "  'table',\n",
       "  'Global',\n",
       "  'conditioning',\n",
       "  'refers',\n",
       "  'information',\n",
       "  'tokens',\n",
       "  'fields',\n",
       "  'table',\n",
       "  'regardless',\n",
       "  'whether',\n",
       "  'appear',\n",
       "  'previous',\n",
       "  'generated',\n",
       "  'words',\n",
       "  'The',\n",
       "  'set',\n",
       "  'fields',\n",
       "  'available',\n",
       "  'table',\n",
       "  'often',\n",
       "  'impacts',\n",
       "  'structure',\n",
       "  'generation',\n",
       "  'For',\n",
       "  'biographies',\n",
       "  'fields',\n",
       "  'used',\n",
       "  'describe',\n",
       "  'politician',\n",
       "  'different',\n",
       "  'ones',\n",
       "  'actor',\n",
       "  'athlete',\n",
       "  'introduce',\n",
       "  'global',\n",
       "  'conditioning',\n",
       "  'available',\n",
       "  'fields',\n",
       "  'zct',\n",
       "  'Similarly',\n",
       "  'global',\n",
       "  'conditioning',\n",
       "  'available',\n",
       "  'words',\n",
       "  'occurring',\n",
       "  'table',\n",
       "  'introduced',\n",
       "  'zct',\n",
       "  'Tokens',\n",
       "  'provide',\n",
       "  'information',\n",
       "  'complementary',\n",
       "  'fields',\n",
       "  'For',\n",
       "  'example',\n",
       "  'may',\n",
       "  'hard',\n",
       "  'distinguish',\n",
       "  'basketball',\n",
       "  'player',\n",
       "  'hockey',\n",
       "  'player',\n",
       "  'looking',\n",
       "  'field',\n",
       "  'names',\n",
       "  'teams',\n",
       "  'league',\n",
       "  'position',\n",
       "  'weight',\n",
       "  'height',\n",
       "  'etc',\n",
       "  'However',\n",
       "  'actual',\n",
       "  'field',\n",
       "  'tokens',\n",
       "  'team',\n",
       "  'names',\n",
       "  'league',\n",
       "  'name',\n",
       "  'player',\n",
       "  'position',\n",
       "  'help',\n",
       "  'model',\n",
       "  'give',\n",
       "  'better',\n",
       "  'prediction',\n",
       "  'Here',\n",
       "  'binary',\n",
       "  'indicators',\n",
       "  'fixed',\n",
       "  'field',\n",
       "  'word',\n",
       "  'vocabularies',\n",
       "  'Figure',\n",
       "  'illustrates',\n",
       "  'model',\n",
       "  'schematic',\n",
       "  'example',\n",
       "  'For',\n",
       "  'predicting',\n",
       "  'next',\n",
       "  'word',\n",
       "  'given',\n",
       "  'context',\n",
       "  'language',\n",
       "  'model',\n",
       "  'conditioned',\n",
       "  'sets',\n",
       "  'triplets',\n",
       "  'word',\n",
       "  'occurring',\n",
       "  'table',\n",
       "  'zct',\n",
       "  'along',\n",
       "  'fields',\n",
       "  'words',\n",
       "  'table',\n",
       "  'Copy',\n",
       "  'actions',\n",
       "  'far',\n",
       "  'extended',\n",
       "  'model',\n",
       "  'conditioning',\n",
       "  'features',\n",
       "  'derived',\n",
       "  'fact',\n",
       "  'table',\n",
       "  'turn',\n",
       "  'using',\n",
       "  'table',\n",
       "  'information',\n",
       "  'scoring',\n",
       "  'output',\n",
       "  'words',\n",
       "  'particular',\n",
       "  'sentences',\n",
       "  'express',\n",
       "  'facts',\n",
       "  'given',\n",
       "  'table',\n",
       "  'often',\n",
       "  'copy',\n",
       "  'words',\n",
       "  'table',\n",
       "  'therefore',\n",
       "  'extend',\n",
       "  'model',\n",
       "  ...],\n",
       " [(-1, 5, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 5, 'B'),\n",
       "  (6, 11, 'I'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 11, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 5, 'B'),\n",
       "  (6, 14, 'I'),\n",
       "  (14, 21, 'I'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 9, 'B'),\n",
       "  (10, 16, 'I'),\n",
       "  (16, 23, 'I'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 3, 'B'),\n",
       "  (4, 11, 'I'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 5, 'B'),\n",
       "  (6, 11, 'I'),\n",
       "  (-1, 12, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 7, 'B'),\n",
       "  (8, 13, 'I'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 11, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 13, 'O'),\n",
       "  (-1, 11, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 6, 'B'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 8, 'B'),\n",
       "  (9, 13, 'I'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 11, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 11, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 11, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 10, 'B'),\n",
       "  (11, 21, 'I'),\n",
       "  (21, 27, 'I'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 6, 'B'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 12, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 13, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 5, 'B'),\n",
       "  (6, 14, 'I'),\n",
       "  (14, 20, 'I'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 12, 'B'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 8, 'B'),\n",
       "  (9, 13, 'I'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 12, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 11, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 11, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 5, 'B'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 11, 'O'),\n",
       "  (-1, 6, 'B'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 12, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 14, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 12, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 13, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 12, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 13, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 14, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 7, 'B'),\n",
       "  (8, 16, 'I'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 6, 'B'),\n",
       "  (7, 18, 'I'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 12, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 6, 'B'),\n",
       "  (7, 16, 'I'),\n",
       "  (16, 22, 'I'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 11, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 9, 'B'),\n",
       "  (10, 16, 'I'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 7, 'B'),\n",
       "  (8, 16, 'I'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 11, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 7, 'B'),\n",
       "  (8, 16, 'I'),\n",
       "  (16, 24, 'I'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 6, 'B'),\n",
       "  (7, 16, 'I'),\n",
       "  (16, 24, 'I'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 11, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 5, 'B'),\n",
       "  (6, 14, 'I'),\n",
       "  (14, 21, 'I'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 6, 'B'),\n",
       "  (7, 18, 'I'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 12, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 2, 'B'),\n",
       "  (3, 10, 'I'),\n",
       "  (-1, 6, 'B'),\n",
       "  (-1, 4, 'B'),\n",
       "  (5, 11, 'I'),\n",
       "  (11, 19, 'I'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 11, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 3, 'B'),\n",
       "  (4, 9, 'I'),\n",
       "  (-1, 8, 'B'),\n",
       "  (9, 18, 'I'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 6, 'B'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 11, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 12, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 10, 'B'),\n",
       "  (11, 19, 'I'),\n",
       "  (19, 26, 'I'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 15, 'O'),\n",
       "  (-1, 7, 'B'),\n",
       "  (8, 13, 'I'),\n",
       "  (-1, 11, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 7, 'B'),\n",
       "  (8, 13, 'I'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 9, 'B'),\n",
       "  (10, 18, 'I'),\n",
       "  (18, 24, 'I'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 3, 'B'),\n",
       "  (4, 12, 'I'),\n",
       "  (12, 18, 'I'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 9, 'B'),\n",
       "  (10, 18, 'I'),\n",
       "  (18, 24, 'I'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 7, 'B'),\n",
       "  (8, 14, 'I'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 11, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 6, 'B'),\n",
       "  (7, 12, 'I'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 11, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 5, 'B'),\n",
       "  (-1, 11, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 11, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 4, 'B'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 4, 'B'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 4, 'B'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 4, 'B'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 4, 'B'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 11, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 4, 'B'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 11, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 11, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 11, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 11, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 5, 'B'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 11, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 11, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 12, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 5, 'B'),\n",
       "  (6, 12, 'I'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 5, 'B'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 7, 'B'),\n",
       "  (8, 12, 'I'),\n",
       "  (12, 18, 'I'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 11, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 6, 'B'),\n",
       "  (7, 15, 'I'),\n",
       "  (15, 21, 'I'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 3, 'B'),\n",
       "  (4, 11, 'I'),\n",
       "  (-1, 2, 'O'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 4, 'B'),\n",
       "  (5, 17, 'I'),\n",
       "  (-1, 7, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 10, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 9, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 6, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 3, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  (-1, 8, 'O'),\n",
       "  (-1, 5, 'O'),\n",
       "  (-1, 4, 'O'),\n",
       "  ...])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iob_rule(all_corpora,silver_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting gold data from a new text (unseen text which is different form the above 20 texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = r'C:\\Users\\User\\Desktop\\Terminology project\\evaluation'  \n",
    "doc = convertpdfstotext(path1)\n",
    "# print(doc)\n",
    "cleandata = preprocessing(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N version (1)\n",
      "N hal (1)\n",
      "ADJ+N open access (1)\n",
      "N access (1)\n",
      "ADJ+N+N open access archive (1)\n",
      "N+N access archive (1)\n",
      "N archive (1)\n",
      "N+N+N access archive deposit (1)\n",
      "N+N archive deposit (1)\n",
      "N deposit (1)\n",
      "N+N+N archive deposit dissemination (1)\n",
      "N+N deposit dissemination (1)\n",
      "N dissemination (1)\n",
      "ADJ+N scientific research (1)\n",
      "N research (7)\n",
      "ADJ+N+N scientific research documents (1)\n",
      "N+N research documents (1)\n",
      "N documents (3)\n",
      "N+N research institutions (1)\n",
      "N institutions (1)\n",
      "ADJ+N private research (1)\n",
      "ADJ+N+N private research centers (1)\n",
      "N+N research centers (1)\n",
      "N centers (1)\n",
      "ADJ+N archive ouverte (1)\n",
      "N ouverte (1)\n",
      "ADJ+N+N archive ouverte pluridisciplinaire (1)\n",
      "N+N ouverte pluridisciplinaire (1)\n",
      "N pluridisciplinaire (1)\n",
      "ADJ+N destine dpt (1)\n",
      "N dpt (1)\n",
      "ADJ+N+N destine dpt diffusion (1)\n",
      "N+N dpt diffusion (1)\n",
      "N diffusion (1)\n",
      "N+N+N dpt diffusion documents (1)\n",
      "N+N diffusion documents (1)\n",
      "N publics (1)\n",
      "N+N publics privs (1)\n",
      "N privs (1)\n",
      "N CNRS (1)\n",
      "N LORIA (1)\n",
      "N MANS (1)\n",
      "ADJ+N equal contribution (1)\n",
      "N contribution (1)\n",
      "ADJ+N+N equal contribution authors (1)\n",
      "N+N contribution authors (1)\n",
      "N authors (2)\n",
      "N data (39)\n",
      "N range (4)\n",
      "ADJ+N personal information (2)\n",
      "N information (16)\n",
      "ADJ+N+N personal information speaker (1)\n",
      "N+N information speaker (2)\n",
      "N speaker (41)\n",
      "N+N+N information speaker identity (1)\n",
      "N+N speaker identity (5)\n",
      "N identity (8)\n",
      "ADJ+N emotional state (1)\n",
      "N state (1)\n",
      "N attributes (2)\n",
      "ADJ+N malicious purposes (1)\n",
      "N purposes (4)\n",
      "N development (2)\n",
      "ADJ+N virtual assistants (1)\n",
      "N assistants (1)\n",
      "ADJ+N new generation (1)\n",
      "N generation (1)\n",
      "ADJ+N+N new generation privacy (1)\n",
      "N+N generation privacy (1)\n",
      "N privacy (14)\n",
      "N+N+N generation privacy threats (1)\n",
      "N+N privacy threats (1)\n",
      "N threats (1)\n",
      "ADJ+N Current studies (1)\n",
      "N studies (3)\n",
      "N topic (2)\n",
      "N speech (76)\n",
      "N+N speech privacy (2)\n",
      "N initiative (3)\n",
      "N+N development privacy (1)\n",
      "N+N+N development privacy preservation (1)\n",
      "N+N privacy preservation (3)\n",
      "N preservation (6)\n",
      "N+N+N privacy preservation tools (1)\n",
      "N+N preservation tools (1)\n",
      "N tools (2)\n",
      "N+N+N preservation tools speech (1)\n",
      "N+N tools speech (1)\n",
      "N+N+N tools speech technology (1)\n",
      "N+N speech technology (1)\n",
      "N technology (1)\n",
      "N task (3)\n",
      "N+N speaker anonymization (7)\n",
      "N anonymization (24)\n",
      "N goal (2)\n",
      "N source (8)\n",
      "N+N source speaker (4)\n",
      "N+N+N source speaker identity (1)\n",
      "ADJ+N linguistic information (2)\n",
      "N baseline (11)\n",
      "N+N baseline VPC (1)\n",
      "N VPC (8)\n",
      "N use (2)\n",
      "N+N use voice (1)\n",
      "N voice (15)\n",
      "N+N+N use voice conversion (1)\n",
      "N+N voice conversion (5)\n",
      "N conversion (5)\n",
      "N paper (7)\n",
      "N+N paper studies (1)\n",
      "N+N+N speaker anonymization baseline (1)\n",
      "N+N anonymization baseline (2)\n",
      "N+N+N anonymization baseline system (2)\n",
      "N+N baseline system (8)\n",
      "N system (24)\n",
      "N+N+N baseline system VPC (1)\n",
      "N+N system VPC (1)\n",
      "ADJ+N emotional information (4)\n",
      "ADJ+N present speech (2)\n",
      "ADJ+N VPC rules (1)\n",
      "N rules (1)\n",
      "N attackers (5)\n",
      "N+N attackers knowledge (1)\n",
      "N knowledge (2)\n",
      "N+N+N attackers knowledge anonymization (1)\n",
      "N+N knowledge anonymization (2)\n",
      "N+N+N knowledge anonymization system (1)\n",
      "N+N anonymization system (7)\n",
      "N results (11)\n",
      "ADJ+N VPC baseline (1)\n",
      "ADJ+N+N VPC baseline system (1)\n",
      "N+N+N baseline system suppress (1)\n",
      "N+N system suppress (1)\n",
      "N suppress (2)\n",
      "N+N+N system suppress speakers (1)\n",
      "N+N suppress speakers (1)\n",
      "N speakers (5)\n",
      "N+N+N suppress speakers emotions (1)\n",
      "N+N speakers emotions (1)\n",
      "N emotions (8)\n",
      "ADJ+N original speech (8)\n",
      "ADJ+N+N original speech emotion (1)\n",
      "N+N speech emotion (4)\n",
      "N emotion (28)\n",
      "N+N+N speech emotion recognition (4)\n",
      "N+N emotion recognition (17)\n",
      "N recognition (26)\n",
      "N+N+N emotion recognition performance (5)\n",
      "N+N recognition performance (6)\n",
      "N performance (12)\n",
      "ADJ+N IEMOCAP data (1)\n",
      "ADJ+N similar degradation (2)\n",
      "N degradation (11)\n",
      "ADJ+N automatic speech (4)\n",
      "ADJ+N+N automatic speech recognition (4)\n",
      "N+N speech recognition (4)\n",
      "N evaluate (1)\n",
      "N+N evaluate preservation (1)\n",
      "N applications (2)\n",
      "ADJ+N smart speakers (1)\n",
      "ADJ+N Large amount (1)\n",
      "N amount (3)\n",
      "ADJ+N+N Large amount data (1)\n",
      "N+N amount data (2)\n",
      "N train (6)\n",
      "N+N train applications (1)\n",
      "N service (2)\n",
      "N+N service providers (1)\n",
      "N providers (1)\n",
      "N process (4)\n",
      "N+N process store (1)\n",
      "N store (1)\n",
      "ADJ+N personal data (4)\n",
      "N servers (1)\n",
      "ADJ+N sensitive modalities (1)\n",
      "N modalities (1)\n",
      "ADJ+N discernible attributes (1)\n",
      "ADJ+N+N discernible attributes speaker (1)\n",
      "N+N attributes speaker (1)\n",
      "N+N+N attributes speaker age (1)\n",
      "N+N speaker age (1)\n",
      "N age (1)\n",
      "N+N+N speaker age gender (1)\n",
      "N+N age gender (1)\n",
      "N gender (1)\n",
      "N+N+N age gender health (1)\n",
      "N+N gender health (1)\n",
      "N health (1)\n",
      "N+N+N gender health personality (1)\n",
      "N+N health personality (1)\n",
      "N personality (2)\n",
      "ADJ+N socioeconomic status (1)\n",
      "N status (1)\n",
      "ADJ+N geographical origin (1)\n",
      "N origin (1)\n",
      "ADJ+N biometric identity (1)\n",
      "ADJ+N+N biometric identity moods (1)\n",
      "N+N identity moods (1)\n",
      "N moods (1)\n",
      "N+N+N identity moods emotions (1)\n",
      "N+N moods emotions (1)\n",
      "N+N speech data (9)\n",
      "N category (1)\n",
      "ADJ+N+N personal data speech (1)\n",
      "N+N data speech (1)\n",
      "N+N+N data speech privacy (1)\n",
      "N solutions (1)\n",
      "ADJ+N recent regulations (1)\n",
      "N regulations (1)\n",
      "N+N+N privacy preservation protection (1)\n",
      "N+N preservation protection (1)\n",
      "N protection (3)\n",
      "N article (1)\n",
      "N framework (4)\n",
      "ADJ+N first attempts (1)\n",
      "N attempts (1)\n",
      "ADJ+N+N first attempts speech (1)\n",
      "N+N attempts speech (1)\n",
      "N+N+N attempts speech community (1)\n",
      "N+N speech community (1)\n",
      "N community (2)\n",
      "N+N research topic (1)\n",
      "ADJ+N dedicated protocols (1)\n",
      "N protocols (1)\n",
      "ADJ+N+N dedicated protocols metrics (1)\n",
      "N+N protocols metrics (1)\n",
      "N metrics (1)\n",
      "N+N+N protocols metrics datasets (1)\n",
      "N+N metrics datasets (1)\n",
      "N datasets (1)\n",
      "N+N+N metrics datasets baselines (1)\n",
      "N+N datasets baselines (1)\n",
      "N baselines (1)\n",
      "N+N goal VPC (1)\n",
      "N+N+N goal VPC system (1)\n",
      "N+N VPC system (1)\n",
      "N+N+N VPC system anonymize (1)\n",
      "N+N system anonymize (1)\n",
      "N anonymize (2)\n",
      "N+N+N system anonymize speaker (1)\n",
      "N+N anonymize speaker (1)\n",
      "ADJ+N paralinguistic information (1)\n",
      "ADJ+N+N paralinguistic information speaker (1)\n",
      "N+N+N information speaker speech (1)\n",
      "N+N speaker speech (3)\n",
      "N+N+N speaker speech utterance (1)\n",
      "N+N speech utterance (2)\n",
      "N utterance (6)\n",
      "ADJ+N linguistic content (2)\n",
      "N content (6)\n",
      "N+N+N speaker anonymization approach (1)\n",
      "N+N anonymization approach (1)\n",
      "N approach (3)\n",
      "N vectors (4)\n",
      "N+N vectors voice (1)\n",
      "N+N+N vectors voice conversion (1)\n",
      "N quality (2)\n",
      "N+N quality anonymization (1)\n",
      "N+N+N quality anonymization VPC (1)\n",
      "N+N anonymization VPC (1)\n",
      "N+N speaker verification (1)\n",
      "N verification (1)\n",
      "N+N+N speaker verification system (1)\n",
      "N+N verification system (1)\n",
      "N capability (1)\n",
      "N+N capability privacy (1)\n",
      "N+N+N speech recognition system (1)\n",
      "N+N recognition system (3)\n",
      "N+N preservation intelligibility (1)\n",
      "N intelligibility (1)\n",
      "N utility (6)\n",
      "ADJ+N metric work (1)\n",
      "N work (2)\n",
      "N extent (1)\n",
      "N+N extent utterance (1)\n",
      "ADJ+N emotional content (1)\n",
      "N+N recognition voice (1)\n",
      "N+N+N recognition voice privacy (1)\n",
      "N+N voice privacy (2)\n",
      "ADJ+N neutral speech (3)\n",
      "ADJ+N expressive speech (1)\n",
      "N signal (4)\n",
      "N+N speaker information (1)\n",
      "ADJ+N emotional cues (1)\n",
      "N cues (4)\n",
      "N+N anonymization process (3)\n",
      "ADJ+N emotional speech (5)\n",
      "ADJ+N+N emotional speech speech (1)\n",
      "N+N speech speech (1)\n",
      "N+N+N speech speech signal (1)\n",
      "N+N speech signal (3)\n",
      "ADJ+N Human emotion (1)\n",
      "ADJ+N psychological theories (1)\n",
      "N theories (2)\n",
      "ADJ+N complementary theories (1)\n",
      "ADJ+N long time (1)\n",
      "N time (1)\n",
      "ADJ+N+N long time collection (1)\n",
      "N+N time collection (1)\n",
      "N collection (2)\n",
      "ADJ+N emotional data (3)\n",
      "N+N data speakers (1)\n",
      "ADJ+N actual trend (1)\n",
      "N trend (1)\n",
      "ADJ+N+N actual trend capture (1)\n",
      "N+N trend capture (1)\n",
      "N capture (4)\n",
      "N+N+N trend capture diversity (1)\n",
      "N+N capture diversity (1)\n",
      "N diversity (1)\n",
      "N+N+N capture diversity humans (1)\n",
      "N+N diversity humans (1)\n",
      "N humans (1)\n",
      "ADJ+N real life (1)\n",
      "N life (1)\n",
      "ADJ+N+N real life conditions (1)\n",
      "N+N life conditions (1)\n",
      "N conditions (1)\n",
      "N+N+N life conditions order (1)\n",
      "N+N conditions order (1)\n",
      "N order (1)\n",
      "N+N+N conditions order model (1)\n",
      "N+N order model (1)\n",
      "N model (13)\n",
      "ADJ+N social aspects (1)\n",
      "N aspects (1)\n",
      "N interactions (2)\n",
      "N+N interactions laughter (1)\n",
      "N laughter (1)\n",
      "N+N+N interactions laughter disfluencies (1)\n",
      "N+N laughter disfluencies (1)\n",
      "N disfluencies (1)\n",
      "N dataset (4)\n",
      "ADJ+N spontaneous speech (1)\n",
      "ADJ+N+N spontaneous speech advantage (1)\n",
      "N+N speech advantage (1)\n",
      "N advantage (3)\n",
      "N benchmark (1)\n",
      "N+N benchmark community (1)\n",
      "ADJ+N Linguistic cues (1)\n",
      "N words (1)\n",
      "N+N speaker paralinguistic (1)\n",
      "N paralinguistic (1)\n",
      "N+N+N speaker paralinguistic cues (1)\n",
      "N+N paralinguistic cues (1)\n",
      "ADJ+N acoustic content (1)\n",
      "ADJ+N+N acoustic content speech (1)\n",
      "N+N content speech (1)\n",
      "ADJ+N prosodic features (1)\n",
      "N features (13)\n",
      "ADJ+N frequency intensity (2)\n",
      "N intensity (2)\n",
      "ADJ+N+N frequency intensity rhythm (2)\n",
      "N+N intensity rhythm (2)\n",
      "N rhythm (2)\n",
      "ADJ+N important cues (1)\n",
      "ADJ+N+N important cues field (1)\n",
      "N+N cues field (1)\n",
      "N field (1)\n",
      "N+N+N cues field speech (1)\n",
      "N+N field speech (1)\n",
      "N+N+N field speech emotion (1)\n",
      "N+N+N emotion recognition SER (1)\n",
      "N+N recognition SER (1)\n",
      "N SER (5)\n",
      "N+N SER systems (1)\n",
      "N systems (2)\n",
      "N+N capture prosody (1)\n",
      "N prosody (1)\n",
      "N+N+N capture prosody input (1)\n",
      "N+N prosody input (1)\n",
      "N input (8)\n",
      "N option (1)\n",
      "N frequency (2)\n",
      "ADJ+N cepstral coefficients (1)\n",
      "N coefficients (1)\n",
      "N association (1)\n",
      "N inventory (1)\n",
      "ADJ+N acoustic features (1)\n",
      "ADJ+N+N acoustic features CEICES (1)\n",
      "N+N features CEICES (1)\n",
      "N CEICES (2)\n",
      "N+N+N features CEICES initiative (1)\n",
      "N+N CEICES initiative (1)\n",
      "N+N+N CEICES initiative conducts (1)\n",
      "N+N initiative conducts (1)\n",
      "N conducts (1)\n",
      "N descriptors (1)\n",
      "ADJ+N several corpora (1)\n",
      "N corpora (1)\n",
      "ADJ+N various techniques (1)\n",
      "N techniques (2)\n",
      "N+N features advantage (1)\n",
      "N extraction (1)\n",
      "N signals (1)\n",
      "N+N signals error (1)\n",
      "N error (1)\n",
      "N+N use input (1)\n",
      "N+N features embeddings (1)\n",
      "N embeddings (1)\n",
      "ADJ+N neural models (1)\n",
      "N models (3)\n",
      "N+N speech processing (1)\n",
      "N processing (1)\n",
      "N+N+N speech processing tasks (1)\n",
      "N+N processing tasks (1)\n",
      "N tasks (1)\n",
      "ADJ+N different SER (1)\n",
      "N+N advantage approach (1)\n",
      "ADJ+N large amount (1)\n",
      "ADJ+N+N large amount data (1)\n",
      "ADJ+N different task (1)\n",
      "N+N speaker recognition (2)\n",
      "N problem (1)\n",
      "N+N problem privacy (1)\n",
      "N+N+N problem privacy preservation (1)\n",
      "N+N+N privacy preservation context (1)\n",
      "N+N preservation context (1)\n",
      "N context (1)\n",
      "ADJ+N+N emotional speech authors (1)\n",
      "N+N speech authors (1)\n",
      "N distance (1)\n",
      "N hashing (1)\n",
      "N+N hashing techniques (1)\n",
      "ADJ+N homomorphic encryption (1)\n",
      "N encryption (1)\n",
      "ADJ+N sensitive data (1)\n",
      "ADJ+N+N sensitive data emotions (1)\n",
      "N+N data emotions (1)\n",
      "ADJ+N adversarial networks (1)\n",
      "N networks (2)\n",
      "ADJ+N intermediate layer (1)\n",
      "N layer (1)\n",
      "ADJ+N+N intermediate layer users (1)\n",
      "N+N layer users (1)\n",
      "N users (1)\n",
      "N services (1)\n",
      "N+N input speech (2)\n",
      "N+N+N input speech aim (1)\n",
      "N+N speech aim (1)\n",
      "N aim (2)\n",
      "N+N+N speaker identity impacts (1)\n",
      "N+N identity impacts (1)\n",
      "N impacts (1)\n",
      "N+N+N identity impacts SER (1)\n",
      "N+N impacts SER (1)\n",
      "N+N+N impacts SER performance (1)\n",
      "N+N SER performance (1)\n",
      "N+N anonymization framework (1)\n",
      "N+N+N baseline system details (1)\n",
      "N+N system details (1)\n",
      "N details (2)\n",
      "N+N+N system details transformation (1)\n",
      "N+N details transformation (1)\n",
      "N transformation (12)\n",
      "N+N+N details transformation enhancements (1)\n",
      "N+N transformation enhancements (1)\n",
      "N enhancements (1)\n",
      "N attack (9)\n",
      "N+N attack scenarios (5)\n",
      "N scenarios (5)\n",
      "N+N+N attack scenarios Section (1)\n",
      "N+N scenarios Section (1)\n",
      "N Section (1)\n",
      "N+N+N scenarios Section details (1)\n",
      "N+N Section details (1)\n",
      "N+N+N Section details experiments (1)\n",
      "N+N details experiments (1)\n",
      "N experiments (3)\n",
      "N evaluation (10)\n",
      "N+N evaluation protocol (3)\n",
      "N protocol (5)\n",
      "N+N+N evaluation protocol respect (1)\n",
      "N+N protocol respect (1)\n",
      "N respect (3)\n",
      "N+N+N protocol respect emotions (1)\n",
      "N+N respect emotions (1)\n",
      "N+N results conclusion (1)\n",
      "N conclusion (1)\n",
      "N+N paper Anonymization (1)\n",
      "N Anonymization (1)\n",
      "N+N+N paper Anonymization framework (1)\n",
      "N+N Anonymization framework (1)\n",
      "N+N+N Anonymization framework fact (1)\n",
      "N+N framework fact (1)\n",
      "N fact (1)\n",
      "N+N+N framework fact emotions (1)\n",
      "N+N fact emotions (1)\n",
      "N intonation (1)\n",
      "N+N intonation propose (1)\n",
      "N propose (1)\n",
      "N values (9)\n",
      "N+N values source (1)\n",
      "N+N+N values source utterance (1)\n",
      "N+N source utterance (1)\n",
      "N+N speaker module (1)\n",
      "N module (3)\n",
      "ADJ+N linear transformation (7)\n",
      "N linear (2)\n",
      "N+N linear transformation (2)\n",
      "N+N+N linear transformation method (1)\n",
      "N+N transformation method (1)\n",
      "N method (2)\n",
      "ADJ+N synthesizer values (1)\n",
      "ADJ+N close pseudo (1)\n",
      "N pseudo (8)\n",
      "ADJ+N+N close pseudo speaker (1)\n",
      "N+N pseudo speaker (8)\n",
      "N+N features source (2)\n",
      "N+N+N features source speaker (1)\n",
      "ADJ+N+N linear transformation log (1)\n",
      "N+N transformation log (1)\n",
      "N log (3)\n",
      "N section (2)\n",
      "ADJ+N Phonetic features (1)\n",
      "ADJ+N+N Phonetic features extractor (1)\n",
      "N+N features extractor (1)\n",
      "N extractor (4)\n",
      "N+N+N features extractor Input (1)\n",
      "N+N extractor Input (1)\n",
      "N Input (1)\n",
      "N+N+N extractor Input speech (1)\n",
      "N+N Input speech (1)\n",
      "N synthesis (3)\n",
      "N vector (13)\n",
      "N+N vector extractor (1)\n",
      "N+N+N vector extractor pseudo (1)\n",
      "N+N extractor pseudo (1)\n",
      "N+N+N extractor pseudo speaker (1)\n",
      "N+N vector selection (1)\n",
      "N selection (2)\n",
      "ADJ+N standard deviation (3)\n",
      "N deviation (3)\n",
      "N utterances (3)\n",
      "ADJ+N+N standard deviation log (1)\n",
      "N+N deviation log (1)\n",
      "ADJ+N statistical calculation (1)\n",
      "N calculation (1)\n",
      "N frames (2)\n",
      "ADJ+N+N standard deviation target (1)\n",
      "N+N deviation target (1)\n",
      "N target (5)\n",
      "N+N+N deviation target pseudospeaker (1)\n",
      "N+N target pseudospeaker (1)\n",
      "N pseudospeaker (1)\n",
      "N+N frames speakers (1)\n",
      "ADJ+N derive pseudo (1)\n",
      "ADJ+N+N derive pseudo speaker (1)\n",
      "N+N+N pseudo speaker vector (2)\n",
      "N+N speaker vector (3)\n",
      "N+N+N speaker anonymization system (2)\n",
      "N+N+N anonymization system log (1)\n",
      "N+N system log (1)\n",
      "ADJ+N Anonymized speech (1)\n",
      "N+N vectors statistics (1)\n",
      "N statistics (2)\n",
      "N+N+N speaker anonymization pipeline (1)\n",
      "N+N anonymization pipeline (1)\n",
      "N pipeline (1)\n",
      "N parts (1)\n",
      "N+N parts baseline (1)\n",
      "N+N+N parts baseline model (1)\n",
      "N+N baseline model (1)\n",
      "N enhancement (1)\n",
      "N transform (2)\n",
      "N+N transform values (1)\n",
      "ADJ+N+N linguistic content input (1)\n",
      "N+N content input (1)\n",
      "N+N+N content input speech (1)\n",
      "N+N+N input speech utterance (1)\n",
      "N+N speech waveform (2)\n",
      "N waveform (2)\n",
      "N altering (1)\n",
      "ADJ+N encode speaker (1)\n",
      "ADJ+N+N encode speaker identity (1)\n",
      "N groups (1)\n",
      "N group (2)\n",
      "ADJ+N different features (1)\n",
      "ADJ+N+N different features source (1)\n",
      "ADJ+N fundamental frequency (1)\n",
      "ADJ+N phonetic features (2)\n",
      "N articulation (2)\n",
      "N+N articulation speech (1)\n",
      "ADJ+N new pseudo (1)\n",
      "ADJ+N+N new pseudo speaker (1)\n",
      "N+N+N pseudo speaker target (3)\n",
      "N+N speaker target (3)\n",
      "N+N+N speaker target vector (3)\n",
      "N+N target vector (4)\n",
      "N+N+N target vector identity (1)\n",
      "N+N vector identity (2)\n",
      "N+N vector source (1)\n",
      "N+N+N vector source input (1)\n",
      "N+N source input (1)\n",
      "N+N+N source input speaker (1)\n",
      "N+N input speaker (1)\n",
      "ADJ+N external vectors (1)\n",
      "ADJ+N furthest vectors (1)\n",
      "N+N+N speaker vector identity (1)\n",
      "N+N+N speech waveform target (1)\n",
      "N+N waveform target (1)\n",
      "N+N+N waveform target vector (1)\n",
      "N+N selection pseudo (1)\n",
      "N+N+N selection pseudo speaker (1)\n",
      "ADJ+N triphone extractor (1)\n",
      "N trainclean (1)\n",
      "N+N trainclean train (1)\n",
      "N+N+N trainclean train subsets (1)\n",
      "N+N train subsets (1)\n",
      "N subsets (1)\n",
      "N xvector (1)\n",
      "N+N xvector extractor (1)\n",
      "N+N speech synthesis (1)\n",
      "N+N+N speech synthesis system (1)\n",
      "N+N synthesis system (1)\n",
      "ADJ+N clean subset (1)\n",
      "N subset (2)\n",
      "N pool (1)\n",
      "N+N pool vector (1)\n",
      "N+N+N pool vector statistics (1)\n",
      "N+N vector statistics (1)\n",
      "N transformations (1)\n",
      "ADJ+N original VPC (1)\n",
      "ADJ+N+N original VPC anonymization (1)\n",
      "N+N VPC anonymization (1)\n",
      "N+N+N VPC anonymization system (1)\n",
      "N+N+N anonymization system values (1)\n",
      "N+N system values (1)\n",
      "N+N source speech (1)\n",
      "ADJ+N unchanged speech (1)\n",
      "ADJ+N+N unchanged speech synthesizer (1)\n",
      "N+N speech synthesizer (1)\n",
      "N synthesizer (1)\n",
      "ADJ+N different pseudo (1)\n",
      "ADJ+N+N different pseudo speaker (1)\n",
      "ADJ+N Multiple studies (1)\n",
      "N modifying (1)\n",
      "ADJ+N Motivated results (1)\n",
      "N+N method contour (1)\n",
      "N contour (2)\n",
      "N+N+N method contour values (1)\n",
      "N+N contour values (1)\n",
      "N increase (1)\n",
      "N+N increase decrease (1)\n",
      "N decrease (1)\n",
      "N+N+N increase decrease range (1)\n",
      "N+N decrease range (1)\n",
      "N+N+N decrease range variation (1)\n",
      "N+N range variation (1)\n",
      "N variation (1)\n",
      "N factor (2)\n",
      "ADJ+N uniform distribution (1)\n",
      "N distribution (3)\n",
      "N value (1)\n",
      "N+N value frame (1)\n",
      "N frame (1)\n",
      "ADJ+N mean utterance (1)\n",
      "ADJ+N+N mean utterance transform (1)\n",
      "N+N utterance transform (1)\n",
      "ADJ+N VPC attack (3)\n",
      "ADJ+N+N VPC attack scenarios (2)\n",
      "ADJ+N multiple sets (1)\n",
      "N sets (1)\n",
      "ADJ+N+N multiple sets tests (1)\n",
      "N+N sets tests (1)\n",
      "N tests (1)\n",
      "N attacker (12)\n",
      "N+N attacker knowledge (1)\n",
      "N+N+N attacker knowledge anonymization (1)\n",
      "ADJ+N Ignorant scenario (2)\n",
      "N scenario (8)\n",
      "ADJ+N+N Ignorant scenario attacker (1)\n",
      "N+N scenario attacker (1)\n",
      "ADJ+N unaware speech (1)\n",
      "N+N privacy measurement (1)\n",
      "N measurement (2)\n",
      "N+N data evaluation (1)\n",
      "N mismatch (1)\n",
      "ADJ+N good anonymization (1)\n",
      "ADJ+N+N good anonymization performance (1)\n",
      "N+N anonymization performance (1)\n",
      "N+N attacker scenario (4)\n",
      "ADJ+N aware anonymization (1)\n",
      "ADJ+N+N aware anonymization algorithm (1)\n",
      "N+N anonymization algorithm (1)\n",
      "N algorithm (1)\n",
      "ADJ+N Such attackers (1)\n",
      "ADJ+N able anonymize (1)\n",
      "ADJ+N+N able anonymize training (1)\n",
      "N+N anonymize training (1)\n",
      "N training (2)\n",
      "N+N+N anonymize training dataset (1)\n",
      "N+N training dataset (1)\n",
      "N+N+N training dataset manner (1)\n",
      "N+N dataset manner (1)\n",
      "N manner (2)\n",
      "N+N+N dataset manner service (1)\n",
      "N+N manner service (1)\n",
      "N+N+N manner service provider (1)\n",
      "N+N service provider (1)\n",
      "N provider (1)\n",
      "N+N train evaluation (1)\n",
      "N+N+N train evaluation model (1)\n",
      "N+N evaluation model (1)\n",
      "N+N+N evaluation model Experiments (1)\n",
      "N+N model Experiments (1)\n",
      "N Experiments (1)\n",
      "ADJ+N global aim (1)\n",
      "N+N+N recognition performance speaker (1)\n",
      "N+N performance speaker (1)\n",
      "N+N+N performance speaker anonymization (1)\n",
      "N voices (1)\n",
      "N sections (1)\n",
      "ADJ+N dataset evaluation (1)\n",
      "ADJ+N+N dataset evaluation protocol (1)\n",
      "N+N evaluation results (1)\n",
      "ADJ+N emotional dataset (1)\n",
      "N experiment (1)\n",
      "N+N experiment purposes (1)\n",
      "N+N+N experiment purposes emotion (1)\n",
      "N+N purposes emotion (1)\n",
      "N+N+N purposes emotion recognition (1)\n",
      "N+N+N emotion recognition speech (1)\n",
      "N+N recognition speech (1)\n",
      "N+N+N recognition speech Table (1)\n",
      "N+N speech Table (1)\n",
      "N Table (3)\n",
      "N+N results VPC (1)\n",
      "N comparison (3)\n",
      "N+N comparison purposes (2)\n",
      "ADJ+N first line (1)\n",
      "N line (2)\n",
      "ADJ+N second line (1)\n",
      "ADJ+N corresponding results (1)\n",
      "ADJ+N+N corresponding results speech (1)\n",
      "N+N results speech (1)\n",
      "ADJ+N anonymized speech (1)\n",
      "ADJ+N Informed attacker (3)\n",
      "ADJ+N+N Informed attacker scenario (1)\n",
      "ADJ+N Original speech (2)\n",
      "ADJ+N+N Original speech data (1)\n",
      "ADJ+N Original degradation (1)\n",
      "ADJ+N+N Original degradation degradation (1)\n",
      "N+N degradation degradation (2)\n",
      "N+N+N degradation degradation degradation (1)\n",
      "ADJ+N Original distribution (1)\n",
      "ADJ+N+N Original distribution emotion (1)\n",
      "N+N distribution emotion (1)\n",
      "N+N+N distribution emotion categories (1)\n",
      "N+N emotion categories (2)\n",
      "N categories (2)\n",
      "N+N respect number (1)\n",
      "N number (2)\n",
      "ADJ+N visual data (1)\n",
      "ADJ+N scripted dialogues (1)\n",
      "N dialogues (1)\n",
      "ADJ+N male actors (1)\n",
      "N actors (1)\n",
      "ADJ+N English language (1)\n",
      "N language (1)\n",
      "ADJ+N Directional microphones (1)\n",
      "N microphones (2)\n",
      "N+N capture speaker (1)\n",
      "N+N+N capture speaker speech (1)\n",
      "ADJ+N audio files (1)\n",
      "N files (1)\n",
      "N case (2)\n",
      "ADJ+N closest speaker (1)\n",
      "ADJ+N dominant speech (1)\n",
      "N reference (1)\n",
      "N+N reference transcriptions (1)\n",
      "N transcriptions (1)\n",
      "ADJ+N directional microphones (1)\n",
      "N level (2)\n",
      "ADJ+N lower level (1)\n",
      "ADJ+N dominant speaker (2)\n",
      "ADJ+N+N dominant speaker voice (1)\n",
      "N+N speaker voice (1)\n",
      "N turn (1)\n",
      "ADJ+N human annotators (1)\n",
      "N annotators (2)\n",
      "N majority (1)\n",
      "N+N majority annotators (1)\n",
      "N Figure (1)\n",
      "ADJ+N emotional labels (1)\n",
      "N labels (1)\n",
      "ADJ+N previous works (1)\n",
      "N works (1)\n",
      "ADJ+N neutral frustration (1)\n",
      "N frustration (1)\n",
      "ADJ+N+N neutral frustration sadness (1)\n",
      "N+N frustration sadness (1)\n",
      "N sadness (1)\n",
      "N+N+N frustration sadness anger (1)\n",
      "N+N sadness anger (1)\n",
      "N anger (1)\n",
      "N+N+N sadness anger happiness (1)\n",
      "N+N anger happiness (1)\n",
      "N happiness (2)\n",
      "N+N+N anger happiness Happiness (1)\n",
      "N+N happiness Happiness (1)\n",
      "N Happiness (1)\n",
      "ADJ+N original annotations (1)\n",
      "N annotations (1)\n",
      "ADJ+N+N original annotations happiness (1)\n",
      "N+N annotations happiness (1)\n",
      "N+N+N annotations happiness excitement (1)\n",
      "N+N happiness excitement (1)\n",
      "N excitement (1)\n",
      "N+N+N happiness excitement balance (1)\n",
      "N+N excitement balance (1)\n",
      "N balance (1)\n",
      "N+N+N excitement balance number (1)\n",
      "N+N balance number (1)\n",
      "N+N emotion class (1)\n",
      "N class (3)\n",
      "N+N protocol paper (1)\n",
      "N+N+N protocol paper focus (1)\n",
      "N+N paper focus (1)\n",
      "N focus (1)\n",
      "ADJ+N original utterances (1)\n",
      "N evaluations (1)\n",
      "N+N+N emotion recognition system (2)\n",
      "N+N model SER (1)\n",
      "N+N+N model SER literature (1)\n",
      "N+N SER literature (1)\n",
      "N literature (1)\n",
      "ADJ+N radial basis (1)\n",
      "N basis (1)\n",
      "ADJ+N+N radial basis function (1)\n",
      "N+N basis function (1)\n",
      "N function (1)\n",
      "N+N baseline input (1)\n",
      "N+N+N baseline input features (1)\n",
      "N+N input features (2)\n",
      "ADJ+N efficient representation (1)\n",
      "N representation (2)\n",
      "ADJ+N+N efficient representation emotion (1)\n",
      "N+N representation emotion (1)\n",
      "ADJ+N MFCCs input (1)\n",
      "ADJ+N+N MFCCs input results (1)\n",
      "N+N input results (1)\n",
      "N+N+N attack scenarios emotion (1)\n",
      "N+N scenarios emotion (1)\n",
      "N+N+N scenarios emotion recognition (1)\n",
      "N+N emotion information (3)\n",
      "ADJ+N+N original speech data (2)\n",
      "ADJ+N Informed scenario (1)\n",
      "ADJ+N metric score (1)\n",
      "N score (2)\n",
      "N measure (1)\n",
      "N+N measure emotion (1)\n",
      "N+N+N measure emotion recognition (1)\n",
      "ADJ+N good emotion (1)\n",
      "ADJ+N+N good emotion recognition (1)\n",
      "ADJ+N Regardless attack (1)\n",
      "ADJ+N+N Regardless attack scenario (1)\n",
      "N+N attack scenario (1)\n",
      "ADJ+N dataset training (1)\n",
      "ADJ+N+N dataset training evaluation (1)\n",
      "N+N training evaluation (1)\n",
      "N session (1)\n",
      "N+N session cross (1)\n",
      "N cross (2)\n",
      "N+N+N session cross validation (1)\n",
      "N+N cross validation (1)\n",
      "N validation (1)\n",
      "N+N+N cross validation protocol (1)\n",
      "N+N validation protocol (1)\n",
      "ADJ+N global performance (1)\n",
      "N test (1)\n",
      "N+N test folds (1)\n",
      "N folds (1)\n",
      "N organizers (1)\n",
      "N+N organizers Results (1)\n",
      "N Results (3)\n",
      "ADJ+N intelligible speech (1)\n",
      "ADJ+N+N intelligible speech Evaluation (1)\n",
      "N+N speech Evaluation (1)\n",
      "N Evaluation (1)\n",
      "N+N+N speech Evaluation results (1)\n",
      "N+N Evaluation results (1)\n",
      "ADJ+N clean data (1)\n",
      "N+N+N speech data comparison (1)\n",
      "N+N data comparison (1)\n",
      "N+N+N data comparison case (1)\n",
      "N+N comparison case (1)\n",
      "N+N+N comparison case ASR (1)\n",
      "N+N case ASR (1)\n",
      "N ASR (1)\n",
      "N+N+N case ASR model (1)\n",
      "N+N ASR model (1)\n",
      "ADJ+N original data (2)\n",
      "ADJ+N+N original data Results (1)\n",
      "N+N data Results (1)\n",
      "ADJ+N experimental results (1)\n",
      "ADJ+N+N experimental results evaluation (1)\n",
      "N+N results evaluation (1)\n",
      "N+N+N results evaluation protocol (1)\n",
      "N scores (3)\n",
      "N+N scores VPC (1)\n",
      "N+N+N scores VPC post (1)\n",
      "N+N VPC post (1)\n",
      "N post (1)\n",
      "N+N+N VPC post evaluation (1)\n",
      "N+N post evaluation (1)\n",
      "N+N+N post evaluation analysis (1)\n",
      "N+N evaluation analysis (2)\n",
      "N analysis (4)\n",
      "N+N utility score (1)\n",
      "ADJ+N relative degradation (1)\n",
      "ADJ+N Similar behavior (1)\n",
      "N behavior (1)\n",
      "N meaning (1)\n",
      "ADJ+N baseline anonymization (1)\n",
      "ADJ+N+N baseline anonymization system (1)\n",
      "N IEMOCAP (4)\n",
      "N presence (2)\n",
      "N confidence (2)\n",
      "N+N confidence interval (2)\n",
      "N interval (2)\n",
      "N+N+N confidence interval linear (1)\n",
      "N+N interval linear (1)\n",
      "N+N+N interval linear transformation (1)\n",
      "ADJ+N Ignorant attacker (2)\n",
      "ADJ+N Original data (1)\n",
      "N classifier (1)\n",
      "N+N+N recognition performance future (1)\n",
      "N+N performance future (1)\n",
      "N future (1)\n",
      "N modifications (6)\n",
      "N percentage (1)\n",
      "N+N percentage intervals (1)\n",
      "N intervals (1)\n",
      "N+N performance IEMOCAP (1)\n",
      "N+N+N performance IEMOCAP respect (1)\n",
      "N+N IEMOCAP respect (1)\n",
      "N+N+N IEMOCAP respect amount (1)\n",
      "N+N respect amount (1)\n",
      "N+N+N respect amount overlap (1)\n",
      "N+N amount overlap (1)\n",
      "N overlap (1)\n",
      "N+N+N amount overlap speech (1)\n",
      "N+N overlap speech (1)\n",
      "N+N+N emotion recognition scores (2)\n",
      "N+N recognition scores (2)\n",
      "ADJ+N random guessing (1)\n",
      "N guessing (1)\n",
      "N+N+N recognition performance terms (1)\n",
      "N+N performance terms (1)\n",
      "N terms (1)\n",
      "N+N manner utility (1)\n",
      "N+N+N manner utility performance (1)\n",
      "N+N utility performance (1)\n",
      "N parameters (3)\n",
      "N+N speech modification (1)\n",
      "N modification (3)\n",
      "N+N+N speech modification prosodic (1)\n",
      "N+N modification prosodic (1)\n",
      "N prosodic (1)\n",
      "N+N+N modification prosodic parameters (1)\n",
      "N+N prosodic parameters (1)\n",
      "N Note (1)\n",
      "ADJ+N mean variability (1)\n",
      "N variability (1)\n",
      "ADJ+N+N mean variability range (1)\n",
      "N+N variability range (1)\n",
      "ADJ+N experimental setup (1)\n",
      "N setup (1)\n",
      "ADJ+N available anonymization (1)\n",
      "ADJ+N+N available anonymization baseline (1)\n",
      "ADJ+N random modifications (2)\n",
      "ADJ+N+N random modifications values (2)\n",
      "N+N modifications values (3)\n",
      "ADJ+N random warping (1)\n",
      "N warping (1)\n",
      "ADJ+N similar results (1)\n",
      "N+N+N attacker scenario paper (1)\n",
      "N+N scenario paper (1)\n",
      "N application (1)\n",
      "N+N+N speech recognition performance (1)\n",
      "ADJ+N+N emotional data IEMOCAP (1)\n",
      "N+N data IEMOCAP (1)\n",
      "ADJ+N neutral data (1)\n",
      "N concerns (1)\n",
      "N+N concerns emotion (1)\n",
      "N+N+N concerns emotion information (1)\n",
      "ADJ+N fine one (1)\n",
      "N one (1)\n",
      "ADJ+N valuable information (1)\n",
      "ADJ+N+N personal information anonymization (1)\n",
      "N+N information anonymization (1)\n",
      "N+N+N information anonymization system (1)\n",
      "ADJ+N preliminary experiments (1)\n",
      "ADJ+N simple modifications (1)\n",
      "N+N+N modifications values modifications (1)\n",
      "N+N values modifications (1)\n",
      "N+N+N emotion information Acknowledgements (1)\n",
      "N+N information Acknowledgements (1)\n",
      "N Acknowledgements (1)\n",
      "N support (1)\n",
      "N+N framework project (1)\n",
      "N project (2)\n",
      "ADJ+N DEEP PRIVACY (1)\n",
      "N PRIVACY (1)\n",
      "ADJ+N+N DEEP PRIVACY gion (1)\n",
      "N+N PRIVACY gion (1)\n",
      "N gion (1)\n",
      "ADJ+N scientific interest (1)\n",
      "N interest (1)\n",
      "ADJ+N+N scientific interest group (1)\n",
      "N+N interest group (1)\n",
      "ADJ+N several Universities (1)\n",
      "N Universities (1)\n",
      "N organizations (1)\n",
      "N taz (1)\n",
      "N+N privacy speaker (1)\n",
      "N+N+N privacy speaker speech (1)\n",
      "N+N+N speaker speech characterisation (1)\n",
      "N+N speech characterisation (1)\n",
      "N characterisation (1)\n",
      "N implications (1)\n",
      "N+N implications voice (1)\n",
      "N+N+N implications voice speech (1)\n",
      "N+N voice speech (1)\n",
      "N+N+N voice speech analysis (1)\n",
      "N+N speech analysis (2)\n",
      "N+N+N speech analysis information (1)\n",
      "N+N analysis information (1)\n",
      "N+N+N analysis information disclosure (1)\n",
      "N+N information disclosure (1)\n",
      "N disclosure (1)\n",
      "N+N+N information disclosure inference (1)\n",
      "N+N disclosure inference (1)\n",
      "N inference (1)\n",
      "ADJ+N natural persons (1)\n",
      "N persons (1)\n",
      "ADJ+N+N natural persons regard (1)\n",
      "N+N persons regard (1)\n",
      "N regard (1)\n",
      "ADJ+N free movement (1)\n",
      "N movement (1)\n",
      "ADJ+N+N free movement data (1)\n",
      "N+N movement data (1)\n",
      "N End (2)\n",
      "N+N End End (1)\n",
      "N+N emotions study (1)\n",
      "N study (1)\n",
      "N+N+N emotions study dependencies (1)\n",
      "N+N study dependencies (1)\n",
      "N dependencies (1)\n",
      "N+N+N study dependencies emotion (1)\n",
      "N+N dependencies emotion (1)\n",
      "N+N+N dependencies emotion speaker (1)\n",
      "N+N emotion speaker (1)\n",
      "N+N+N emotion speaker recognition (1)\n",
      "N Trancoso (1)\n",
      "N cryptonet (1)\n",
      "N approaches (1)\n",
      "N+N approaches privacy (1)\n",
      "N+N+N speech analysis voice (1)\n",
      "N+N analysis voice (2)\n",
      "ADJ+N neural network (1)\n",
      "N network (1)\n",
      "N+N speaker identification (1)\n",
      "N identification (1)\n",
      "N+N representation disentanglement (1)\n",
      "N disentanglement (1)\n",
      "N+N cross domain (1)\n",
      "N domain (1)\n",
      "ADJ+N adversarial learning (1)\n",
      "N learning (1)\n",
      "ADJ+N variational autoencoder (1)\n",
      "N autoencoder (2)\n",
      "ADJ+N parallel voice (1)\n",
      "ADJ+N+N parallel voice conversion (1)\n",
      "ADJ+N conditional autoencoder (1)\n",
      "N circumplex (1)\n",
      "N+N circumplex model (1)\n",
      "ADJ+N social psychology (1)\n",
      "N psychology (1)\n",
      "N spectrum (1)\n",
      "N+N spectrum modification (1)\n",
      "N+N+N spectrum modification articulation (1)\n",
      "N+N modification articulation (1)\n",
      "N+N+N modification articulation disorders (1)\n",
      "N+N articulation disorders (1)\n",
      "N disorders (1)\n",
      "N phone (1)\n",
      "ADJ+N selective synthesis (1)\n",
      "ADJ+N Social science (1)\n",
      "N science (1)\n",
      "ADJ+N+N Social science information (1)\n",
      "N+N science information (1)\n",
      "N database (2)\n",
      "N chade (1)\n",
      "ADJ+N human robot (1)\n",
      "N robot (1)\n",
      "ADJ+N humorous interactions (1)\n",
      "N ACII (1)\n",
      "N Annotation (1)\n",
      "N+N Annotation tools (1)\n",
      "N+N+N Annotation tools multiparty (1)\n",
      "N+N tools multiparty (1)\n",
      "N multiparty (1)\n",
      "ADJ+N casual conversation (1)\n",
      "N conversation (1)\n",
      "N LREC (1)\n",
      "ADJ+N dyadic motion (1)\n",
      "N motion (1)\n",
      "ADJ+N+N dyadic motion capture (1)\n",
      "N+N motion capture (1)\n",
      "N+N+N motion capture database (1)\n",
      "N+N capture database (1)\n",
      "N+N CEICES Combining (1)\n",
      "N Combining (1)\n",
      "N+N+N CEICES Combining efforts (1)\n",
      "N+N Combining efforts (1)\n",
      "N efforts (1)\n",
      "ADJ+N automatic classification (1)\n",
      "N classification (1)\n",
      "ADJ+N emotional user (1)\n",
      "N user (1)\n",
      "ADJ+N+N emotional user states (1)\n",
      "N+N user states (1)\n",
      "N states (1)\n",
      "N cooperation (1)\n",
      "N+N cooperation initiative (1)\n",
      "N parameter (1)\n",
      "N gemaps (1)\n",
      "N+N gemaps voice (1)\n",
      "N+N+N gemaps voice research (1)\n",
      "N+N voice research (1)\n",
      "N transactions (1)\n",
      "N Chappell (1)\n",
      "ADJ+N specific pitch (1)\n",
      "N pitch (1)\n",
      "ADJ+N+N specific pitch contour (1)\n",
      "N+N pitch contour (1)\n",
      "N+N+N pitch contour modeling (1)\n",
      "N+N contour modeling (1)\n",
      "N modeling (1)\n",
      "N+N+N contour modeling modification (1)\n",
      "N+N modeling modification (1)\n",
      "N ICASSP (1)\n",
      "N ller (1)\n",
      "ADJ+N frequency range (1)\n",
      "ADJ+N+N frequency range voice (1)\n",
      "N+N range voice (1)\n",
      "ADJ+N fundamental speech (1)\n",
      "ADJ+N female adults (1)\n",
      "N adults (1)\n",
      "N+N privacy protection (1)\n",
      "N+N utility vector (1)\n",
      "ADJ+N neural networks (1)\n",
      "ADJ+N+N neural networks emotion (1)\n",
      "N+N networks emotion (1)\n",
      "N+N+N networks emotion recognition (1)\n",
      "ADJ+N audio transcripts (1)\n",
      "N transcripts (1)\n",
      "ADJ+N binary decision (1)\n",
      "N decision (1)\n",
      "ADJ+N+N binary decision tree (1)\n",
      "N+N decision tree (1)\n",
      "N tree (1)\n",
      "N+N+N decision tree approach (1)\n",
      "N+N tree approach (1)\n",
      "N Set (1)\n",
      "N+N+N evaluation analysis voice (1)\n",
      "N+N+N analysis voice privacy (1)\n",
      "N+N+N voice privacy challenge (1)\n",
      "N+N privacy challenge (1)\n",
      "N challenge (1)\n",
      "N+N+N speech data train (1)\n",
      "N+N data train (1)\n",
      "N+N+N data train attack (1)\n",
      "N+N train attack (1)\n",
      "N+N+N train attack models (1)\n",
      "N+N attack models (1)\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from collections import Counter\n",
    "from spacy.matcher import Matcher #import matcher\n",
    "import spacy #import library\n",
    "nlp = spacy.load('en_core_web_sm') #load model which is sm small model and en means english\n",
    "data = nlp(cleandata) \n",
    "\n",
    "\n",
    "\n",
    "matcher = Matcher(nlp.vocab) # Initialize the matcher with the shared vocab\n",
    "\n",
    "#define rules\n",
    "pattern1 = [{'POS': 'ADJ'},{'POS': 'NOUN'}]\n",
    "pattern2 = [{'POS': 'NOUN'}, {'POS': 'NOUN'}]\n",
    "pattern3 = [{'POS': 'NOUN'}, {'POS': 'NOUN'}, {'POS': 'NOUN'} ]\n",
    "pattern4 = [{'POS': 'NOUN'}]\n",
    "pattern5 = [{'POS': 'ADJ'},{'POS': 'NOUN'}, {'POS': 'NOUN'}]\n",
    "\n",
    "#add rules to matcher \n",
    "matcher.add('ADJ+N', [pattern1]) \n",
    "matcher.add('N+N', [pattern2]) \n",
    "matcher.add('N+N+N', [pattern3]) \n",
    "matcher.add('N', [pattern4]) \n",
    "matcher.add('ADJ+N+N', [pattern5]) \n",
    "\n",
    "matches = matcher(data)\n",
    "\n",
    "d=[]\n",
    "for match_id, start, end in matches:\n",
    "    rule_id = nlp.vocab.strings[match_id]  # get the unicode ID, i.e. 'COLOR'\n",
    "    span = data[start : end]  # get the matched slice of the doc\n",
    "    d.append((rule_id, span.text))\n",
    "    keyterm = span.text\n",
    "print(\"\\n\".join(f'{i[0]} {i[1]} ({j})' for i,j in Counter(d).items()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('N', 'speech'), 76),\n",
       " (('N', 'speaker'), 41),\n",
       " (('N', 'data'), 39),\n",
       " (('N', 'emotion'), 28),\n",
       " (('N', 'recognition'), 26),\n",
       " (('N', 'anonymization'), 24),\n",
       " (('N', 'system'), 24),\n",
       " (('N+N', 'emotion recognition'), 17),\n",
       " (('N', 'information'), 16),\n",
       " (('N', 'voice'), 15),\n",
       " (('N', 'privacy'), 14),\n",
       " (('N', 'model'), 13),\n",
       " (('N', 'features'), 13),\n",
       " (('N', 'vector'), 13),\n",
       " (('N', 'performance'), 12),\n",
       " (('N', 'transformation'), 12),\n",
       " (('N', 'attacker'), 12),\n",
       " (('N', 'baseline'), 11),\n",
       " (('N', 'results'), 11),\n",
       " (('N', 'degradation'), 11),\n",
       " (('N', 'evaluation'), 10),\n",
       " (('N+N', 'speech data'), 9),\n",
       " (('N', 'attack'), 9),\n",
       " (('N', 'values'), 9),\n",
       " (('N', 'identity'), 8),\n",
       " (('N', 'source'), 8),\n",
       " (('N', 'VPC'), 8),\n",
       " (('N+N', 'baseline system'), 8),\n",
       " (('N', 'emotions'), 8),\n",
       " (('ADJ+N', 'original speech'), 8),\n",
       " (('N', 'input'), 8),\n",
       " (('N', 'pseudo'), 8),\n",
       " (('N+N', 'pseudo speaker'), 8),\n",
       " (('N', 'scenario'), 8),\n",
       " (('N', 'research'), 7),\n",
       " (('N+N', 'speaker anonymization'), 7),\n",
       " (('N', 'paper'), 7),\n",
       " (('N+N', 'anonymization system'), 7),\n",
       " (('ADJ+N', 'linear transformation'), 7),\n",
       " (('N', 'preservation'), 6),\n",
       " (('N+N', 'recognition performance'), 6),\n",
       " (('N', 'train'), 6),\n",
       " (('N', 'utterance'), 6),\n",
       " (('N', 'content'), 6),\n",
       " (('N', 'utility'), 6),\n",
       " (('N', 'modifications'), 6),\n",
       " (('N+N', 'speaker identity'), 5),\n",
       " (('N+N', 'voice conversion'), 5),\n",
       " (('N', 'conversion'), 5),\n",
       " (('N', 'attackers'), 5),\n",
       " (('N', 'speakers'), 5),\n",
       " (('N+N+N', 'emotion recognition performance'), 5),\n",
       " (('ADJ+N', 'emotional speech'), 5),\n",
       " (('N', 'SER'), 5),\n",
       " (('N+N', 'attack scenarios'), 5),\n",
       " (('N', 'scenarios'), 5),\n",
       " (('N', 'protocol'), 5),\n",
       " (('N', 'target'), 5),\n",
       " (('N', 'range'), 4),\n",
       " (('N', 'purposes'), 4),\n",
       " (('N+N', 'source speaker'), 4),\n",
       " (('ADJ+N', 'emotional information'), 4),\n",
       " (('N+N', 'speech emotion'), 4),\n",
       " (('N+N+N', 'speech emotion recognition'), 4),\n",
       " (('ADJ+N', 'automatic speech'), 4),\n",
       " (('ADJ+N+N', 'automatic speech recognition'), 4),\n",
       " (('N+N', 'speech recognition'), 4),\n",
       " (('N', 'process'), 4),\n",
       " (('ADJ+N', 'personal data'), 4),\n",
       " (('N', 'framework'), 4),\n",
       " (('N', 'vectors'), 4),\n",
       " (('N', 'signal'), 4),\n",
       " (('N', 'cues'), 4),\n",
       " (('N', 'capture'), 4),\n",
       " (('N', 'dataset'), 4),\n",
       " (('N', 'extractor'), 4),\n",
       " (('N+N', 'target vector'), 4),\n",
       " (('N+N', 'attacker scenario'), 4),\n",
       " (('N', 'analysis'), 4),\n",
       " (('N', 'IEMOCAP'), 4),\n",
       " (('N', 'documents'), 3),\n",
       " (('N', 'studies'), 3),\n",
       " (('N', 'initiative'), 3),\n",
       " (('N+N', 'privacy preservation'), 3),\n",
       " (('N', 'task'), 3),\n",
       " (('N', 'amount'), 3),\n",
       " (('N', 'protection'), 3),\n",
       " (('N+N', 'speaker speech'), 3),\n",
       " (('N', 'approach'), 3),\n",
       " (('N+N', 'recognition system'), 3),\n",
       " (('ADJ+N', 'neutral speech'), 3),\n",
       " (('N+N', 'anonymization process'), 3),\n",
       " (('N+N', 'speech signal'), 3),\n",
       " (('ADJ+N', 'emotional data'), 3),\n",
       " (('N', 'advantage'), 3),\n",
       " (('N', 'models'), 3),\n",
       " (('N', 'experiments'), 3),\n",
       " (('N+N', 'evaluation protocol'), 3),\n",
       " (('N', 'respect'), 3),\n",
       " (('N', 'module'), 3),\n",
       " (('N', 'log'), 3),\n",
       " (('N', 'synthesis'), 3),\n",
       " (('ADJ+N', 'standard deviation'), 3),\n",
       " (('N', 'deviation'), 3),\n",
       " (('N', 'utterances'), 3),\n",
       " (('N+N', 'speaker vector'), 3),\n",
       " (('N+N+N', 'pseudo speaker target'), 3),\n",
       " (('N+N', 'speaker target'), 3),\n",
       " (('N+N+N', 'speaker target vector'), 3),\n",
       " (('N', 'distribution'), 3),\n",
       " (('ADJ+N', 'VPC attack'), 3),\n",
       " (('N', 'Table'), 3),\n",
       " (('N', 'comparison'), 3),\n",
       " (('ADJ+N', 'Informed attacker'), 3),\n",
       " (('N', 'class'), 3),\n",
       " (('N+N', 'emotion information'), 3),\n",
       " (('N', 'Results'), 3),\n",
       " (('N', 'scores'), 3),\n",
       " (('N', 'parameters'), 3),\n",
       " (('N', 'modification'), 3),\n",
       " (('N+N', 'modifications values'), 3),\n",
       " (('N', 'authors'), 2),\n",
       " (('ADJ+N', 'personal information'), 2),\n",
       " (('N+N', 'information speaker'), 2),\n",
       " (('N', 'attributes'), 2),\n",
       " (('N', 'development'), 2),\n",
       " (('N', 'topic'), 2),\n",
       " (('N+N', 'speech privacy'), 2),\n",
       " (('N', 'tools'), 2),\n",
       " (('N', 'goal'), 2),\n",
       " (('ADJ+N', 'linguistic information'), 2),\n",
       " (('N', 'use'), 2),\n",
       " (('N+N', 'anonymization baseline'), 2),\n",
       " (('N+N+N', 'anonymization baseline system'), 2),\n",
       " (('ADJ+N', 'present speech'), 2),\n",
       " (('N', 'knowledge'), 2),\n",
       " (('N+N', 'knowledge anonymization'), 2),\n",
       " (('N', 'suppress'), 2),\n",
       " (('ADJ+N', 'similar degradation'), 2),\n",
       " (('N', 'applications'), 2),\n",
       " (('N+N', 'amount data'), 2),\n",
       " (('N', 'service'), 2),\n",
       " (('N', 'personality'), 2),\n",
       " (('N', 'community'), 2),\n",
       " (('N', 'anonymize'), 2),\n",
       " (('N+N', 'speech utterance'), 2),\n",
       " (('ADJ+N', 'linguistic content'), 2),\n",
       " (('N', 'quality'), 2),\n",
       " (('N', 'work'), 2),\n",
       " (('N+N', 'voice privacy'), 2),\n",
       " (('N', 'theories'), 2),\n",
       " (('N', 'collection'), 2),\n",
       " (('N', 'interactions'), 2),\n",
       " (('ADJ+N', 'frequency intensity'), 2),\n",
       " (('N', 'intensity'), 2),\n",
       " (('ADJ+N+N', 'frequency intensity rhythm'), 2),\n",
       " (('N+N', 'intensity rhythm'), 2),\n",
       " (('N', 'rhythm'), 2),\n",
       " (('N', 'systems'), 2),\n",
       " (('N', 'frequency'), 2),\n",
       " (('N', 'CEICES'), 2),\n",
       " (('N', 'techniques'), 2),\n",
       " (('N+N', 'speaker recognition'), 2),\n",
       " (('N', 'networks'), 2),\n",
       " (('N+N', 'input speech'), 2),\n",
       " (('N', 'aim'), 2),\n",
       " (('N', 'details'), 2),\n",
       " (('N', 'linear'), 2),\n",
       " (('N+N', 'linear transformation'), 2),\n",
       " (('N', 'method'), 2),\n",
       " (('N+N', 'features source'), 2),\n",
       " (('N', 'section'), 2),\n",
       " (('N', 'selection'), 2),\n",
       " (('N', 'frames'), 2),\n",
       " (('N+N+N', 'pseudo speaker vector'), 2),\n",
       " (('N+N+N', 'speaker anonymization system'), 2),\n",
       " (('N', 'statistics'), 2),\n",
       " (('N', 'transform'), 2),\n",
       " (('N+N', 'speech waveform'), 2),\n",
       " (('N', 'waveform'), 2),\n",
       " (('N', 'group'), 2),\n",
       " (('ADJ+N', 'phonetic features'), 2),\n",
       " (('N', 'articulation'), 2),\n",
       " (('N+N', 'vector identity'), 2),\n",
       " (('N', 'subset'), 2),\n",
       " (('N', 'contour'), 2),\n",
       " (('N', 'factor'), 2),\n",
       " (('ADJ+N+N', 'VPC attack scenarios'), 2),\n",
       " (('ADJ+N', 'Ignorant scenario'), 2),\n",
       " (('N', 'measurement'), 2),\n",
       " (('N', 'training'), 2),\n",
       " (('N', 'manner'), 2),\n",
       " (('N+N', 'comparison purposes'), 2),\n",
       " (('N', 'line'), 2),\n",
       " (('ADJ+N', 'Original speech'), 2),\n",
       " (('N+N', 'degradation degradation'), 2),\n",
       " (('N+N', 'emotion categories'), 2),\n",
       " (('N', 'categories'), 2),\n",
       " (('N', 'number'), 2),\n",
       " (('N', 'microphones'), 2),\n",
       " (('N', 'case'), 2),\n",
       " (('N', 'level'), 2),\n",
       " (('ADJ+N', 'dominant speaker'), 2),\n",
       " (('N', 'annotators'), 2),\n",
       " (('N', 'happiness'), 2),\n",
       " (('N+N+N', 'emotion recognition system'), 2),\n",
       " (('N+N', 'input features'), 2),\n",
       " (('N', 'representation'), 2),\n",
       " (('ADJ+N+N', 'original speech data'), 2),\n",
       " (('N', 'score'), 2),\n",
       " (('N', 'cross'), 2),\n",
       " (('ADJ+N', 'original data'), 2),\n",
       " (('N+N', 'evaluation analysis'), 2),\n",
       " (('N', 'presence'), 2),\n",
       " (('N', 'confidence'), 2),\n",
       " (('N+N', 'confidence interval'), 2),\n",
       " (('N', 'interval'), 2),\n",
       " (('ADJ+N', 'Ignorant attacker'), 2),\n",
       " (('N+N+N', 'emotion recognition scores'), 2),\n",
       " (('N+N', 'recognition scores'), 2),\n",
       " (('ADJ+N', 'random modifications'), 2),\n",
       " (('ADJ+N+N', 'random modifications values'), 2),\n",
       " (('N', 'project'), 2),\n",
       " (('N+N', 'speech analysis'), 2),\n",
       " (('N', 'End'), 2),\n",
       " (('N+N', 'analysis voice'), 2),\n",
       " (('N', 'autoencoder'), 2),\n",
       " (('N', 'database'), 2),\n",
       " (('N', 'version'), 1),\n",
       " (('N', 'hal'), 1),\n",
       " (('ADJ+N', 'open access'), 1),\n",
       " (('N', 'access'), 1),\n",
       " (('ADJ+N+N', 'open access archive'), 1),\n",
       " (('N+N', 'access archive'), 1),\n",
       " (('N', 'archive'), 1),\n",
       " (('N+N+N', 'access archive deposit'), 1),\n",
       " (('N+N', 'archive deposit'), 1),\n",
       " (('N', 'deposit'), 1),\n",
       " (('N+N+N', 'archive deposit dissemination'), 1),\n",
       " (('N+N', 'deposit dissemination'), 1),\n",
       " (('N', 'dissemination'), 1),\n",
       " (('ADJ+N', 'scientific research'), 1),\n",
       " (('ADJ+N+N', 'scientific research documents'), 1),\n",
       " (('N+N', 'research documents'), 1),\n",
       " (('N+N', 'research institutions'), 1),\n",
       " (('N', 'institutions'), 1),\n",
       " (('ADJ+N', 'private research'), 1),\n",
       " (('ADJ+N+N', 'private research centers'), 1),\n",
       " (('N+N', 'research centers'), 1),\n",
       " (('N', 'centers'), 1),\n",
       " (('ADJ+N', 'archive ouverte'), 1),\n",
       " (('N', 'ouverte'), 1),\n",
       " (('ADJ+N+N', 'archive ouverte pluridisciplinaire'), 1),\n",
       " (('N+N', 'ouverte pluridisciplinaire'), 1),\n",
       " (('N', 'pluridisciplinaire'), 1),\n",
       " (('ADJ+N', 'destine dpt'), 1),\n",
       " (('N', 'dpt'), 1),\n",
       " (('ADJ+N+N', 'destine dpt diffusion'), 1),\n",
       " (('N+N', 'dpt diffusion'), 1),\n",
       " (('N', 'diffusion'), 1),\n",
       " (('N+N+N', 'dpt diffusion documents'), 1),\n",
       " (('N+N', 'diffusion documents'), 1),\n",
       " (('N', 'publics'), 1),\n",
       " (('N+N', 'publics privs'), 1),\n",
       " (('N', 'privs'), 1),\n",
       " (('N', 'CNRS'), 1),\n",
       " (('N', 'LORIA'), 1),\n",
       " (('N', 'MANS'), 1),\n",
       " (('ADJ+N', 'equal contribution'), 1),\n",
       " (('N', 'contribution'), 1),\n",
       " (('ADJ+N+N', 'equal contribution authors'), 1),\n",
       " (('N+N', 'contribution authors'), 1),\n",
       " (('ADJ+N+N', 'personal information speaker'), 1),\n",
       " (('N+N+N', 'information speaker identity'), 1),\n",
       " (('ADJ+N', 'emotional state'), 1),\n",
       " (('N', 'state'), 1),\n",
       " (('ADJ+N', 'malicious purposes'), 1),\n",
       " (('ADJ+N', 'virtual assistants'), 1),\n",
       " (('N', 'assistants'), 1),\n",
       " (('ADJ+N', 'new generation'), 1),\n",
       " (('N', 'generation'), 1),\n",
       " (('ADJ+N+N', 'new generation privacy'), 1),\n",
       " (('N+N', 'generation privacy'), 1),\n",
       " (('N+N+N', 'generation privacy threats'), 1),\n",
       " (('N+N', 'privacy threats'), 1),\n",
       " (('N', 'threats'), 1),\n",
       " (('ADJ+N', 'Current studies'), 1),\n",
       " (('N+N', 'development privacy'), 1),\n",
       " (('N+N+N', 'development privacy preservation'), 1),\n",
       " (('N+N+N', 'privacy preservation tools'), 1),\n",
       " (('N+N', 'preservation tools'), 1),\n",
       " (('N+N+N', 'preservation tools speech'), 1),\n",
       " (('N+N', 'tools speech'), 1),\n",
       " (('N+N+N', 'tools speech technology'), 1),\n",
       " (('N+N', 'speech technology'), 1),\n",
       " (('N', 'technology'), 1),\n",
       " (('N+N+N', 'source speaker identity'), 1),\n",
       " (('N+N', 'baseline VPC'), 1),\n",
       " (('N+N', 'use voice'), 1),\n",
       " (('N+N+N', 'use voice conversion'), 1),\n",
       " (('N+N', 'paper studies'), 1),\n",
       " (('N+N+N', 'speaker anonymization baseline'), 1),\n",
       " (('N+N+N', 'baseline system VPC'), 1),\n",
       " (('N+N', 'system VPC'), 1),\n",
       " (('ADJ+N', 'VPC rules'), 1),\n",
       " (('N', 'rules'), 1),\n",
       " (('N+N', 'attackers knowledge'), 1),\n",
       " (('N+N+N', 'attackers knowledge anonymization'), 1),\n",
       " (('N+N+N', 'knowledge anonymization system'), 1),\n",
       " (('ADJ+N', 'VPC baseline'), 1),\n",
       " (('ADJ+N+N', 'VPC baseline system'), 1),\n",
       " (('N+N+N', 'baseline system suppress'), 1),\n",
       " (('N+N', 'system suppress'), 1),\n",
       " (('N+N+N', 'system suppress speakers'), 1),\n",
       " (('N+N', 'suppress speakers'), 1),\n",
       " (('N+N+N', 'suppress speakers emotions'), 1),\n",
       " (('N+N', 'speakers emotions'), 1),\n",
       " (('ADJ+N+N', 'original speech emotion'), 1),\n",
       " (('ADJ+N', 'IEMOCAP data'), 1),\n",
       " (('N', 'evaluate'), 1),\n",
       " (('N+N', 'evaluate preservation'), 1),\n",
       " (('ADJ+N', 'smart speakers'), 1),\n",
       " (('ADJ+N', 'Large amount'), 1),\n",
       " (('ADJ+N+N', 'Large amount data'), 1),\n",
       " (('N+N', 'train applications'), 1),\n",
       " (('N+N', 'service providers'), 1),\n",
       " (('N', 'providers'), 1),\n",
       " (('N+N', 'process store'), 1),\n",
       " (('N', 'store'), 1),\n",
       " (('N', 'servers'), 1),\n",
       " (('ADJ+N', 'sensitive modalities'), 1),\n",
       " (('N', 'modalities'), 1),\n",
       " (('ADJ+N', 'discernible attributes'), 1),\n",
       " (('ADJ+N+N', 'discernible attributes speaker'), 1),\n",
       " (('N+N', 'attributes speaker'), 1),\n",
       " (('N+N+N', 'attributes speaker age'), 1),\n",
       " (('N+N', 'speaker age'), 1),\n",
       " (('N', 'age'), 1),\n",
       " (('N+N+N', 'speaker age gender'), 1),\n",
       " (('N+N', 'age gender'), 1),\n",
       " (('N', 'gender'), 1),\n",
       " (('N+N+N', 'age gender health'), 1),\n",
       " (('N+N', 'gender health'), 1),\n",
       " (('N', 'health'), 1),\n",
       " (('N+N+N', 'gender health personality'), 1),\n",
       " (('N+N', 'health personality'), 1),\n",
       " (('ADJ+N', 'socioeconomic status'), 1),\n",
       " (('N', 'status'), 1),\n",
       " (('ADJ+N', 'geographical origin'), 1),\n",
       " (('N', 'origin'), 1),\n",
       " (('ADJ+N', 'biometric identity'), 1),\n",
       " (('ADJ+N+N', 'biometric identity moods'), 1),\n",
       " (('N+N', 'identity moods'), 1),\n",
       " (('N', 'moods'), 1),\n",
       " (('N+N+N', 'identity moods emotions'), 1),\n",
       " (('N+N', 'moods emotions'), 1),\n",
       " (('N', 'category'), 1),\n",
       " (('ADJ+N+N', 'personal data speech'), 1),\n",
       " (('N+N', 'data speech'), 1),\n",
       " (('N+N+N', 'data speech privacy'), 1),\n",
       " (('N', 'solutions'), 1),\n",
       " (('ADJ+N', 'recent regulations'), 1),\n",
       " (('N', 'regulations'), 1),\n",
       " (('N+N+N', 'privacy preservation protection'), 1),\n",
       " (('N+N', 'preservation protection'), 1),\n",
       " (('N', 'article'), 1),\n",
       " (('ADJ+N', 'first attempts'), 1),\n",
       " (('N', 'attempts'), 1),\n",
       " (('ADJ+N+N', 'first attempts speech'), 1),\n",
       " (('N+N', 'attempts speech'), 1),\n",
       " (('N+N+N', 'attempts speech community'), 1),\n",
       " (('N+N', 'speech community'), 1),\n",
       " (('N+N', 'research topic'), 1),\n",
       " (('ADJ+N', 'dedicated protocols'), 1),\n",
       " (('N', 'protocols'), 1),\n",
       " (('ADJ+N+N', 'dedicated protocols metrics'), 1),\n",
       " (('N+N', 'protocols metrics'), 1),\n",
       " (('N', 'metrics'), 1),\n",
       " (('N+N+N', 'protocols metrics datasets'), 1),\n",
       " (('N+N', 'metrics datasets'), 1),\n",
       " (('N', 'datasets'), 1),\n",
       " (('N+N+N', 'metrics datasets baselines'), 1),\n",
       " (('N+N', 'datasets baselines'), 1),\n",
       " (('N', 'baselines'), 1),\n",
       " (('N+N', 'goal VPC'), 1),\n",
       " (('N+N+N', 'goal VPC system'), 1),\n",
       " (('N+N', 'VPC system'), 1),\n",
       " (('N+N+N', 'VPC system anonymize'), 1),\n",
       " (('N+N', 'system anonymize'), 1),\n",
       " (('N+N+N', 'system anonymize speaker'), 1),\n",
       " (('N+N', 'anonymize speaker'), 1),\n",
       " (('ADJ+N', 'paralinguistic information'), 1),\n",
       " (('ADJ+N+N', 'paralinguistic information speaker'), 1),\n",
       " (('N+N+N', 'information speaker speech'), 1),\n",
       " (('N+N+N', 'speaker speech utterance'), 1),\n",
       " (('N+N+N', 'speaker anonymization approach'), 1),\n",
       " (('N+N', 'anonymization approach'), 1),\n",
       " (('N+N', 'vectors voice'), 1),\n",
       " (('N+N+N', 'vectors voice conversion'), 1),\n",
       " (('N+N', 'quality anonymization'), 1),\n",
       " (('N+N+N', 'quality anonymization VPC'), 1),\n",
       " (('N+N', 'anonymization VPC'), 1),\n",
       " (('N+N', 'speaker verification'), 1),\n",
       " (('N', 'verification'), 1),\n",
       " (('N+N+N', 'speaker verification system'), 1),\n",
       " (('N+N', 'verification system'), 1),\n",
       " (('N', 'capability'), 1),\n",
       " (('N+N', 'capability privacy'), 1),\n",
       " (('N+N+N', 'speech recognition system'), 1),\n",
       " (('N+N', 'preservation intelligibility'), 1),\n",
       " (('N', 'intelligibility'), 1),\n",
       " (('ADJ+N', 'metric work'), 1),\n",
       " (('N', 'extent'), 1),\n",
       " (('N+N', 'extent utterance'), 1),\n",
       " (('ADJ+N', 'emotional content'), 1),\n",
       " (('N+N', 'recognition voice'), 1),\n",
       " (('N+N+N', 'recognition voice privacy'), 1),\n",
       " (('ADJ+N', 'expressive speech'), 1),\n",
       " (('N+N', 'speaker information'), 1),\n",
       " (('ADJ+N', 'emotional cues'), 1),\n",
       " (('ADJ+N+N', 'emotional speech speech'), 1),\n",
       " (('N+N', 'speech speech'), 1),\n",
       " (('N+N+N', 'speech speech signal'), 1),\n",
       " (('ADJ+N', 'Human emotion'), 1),\n",
       " (('ADJ+N', 'psychological theories'), 1),\n",
       " (('ADJ+N', 'complementary theories'), 1),\n",
       " (('ADJ+N', 'long time'), 1),\n",
       " (('N', 'time'), 1),\n",
       " (('ADJ+N+N', 'long time collection'), 1),\n",
       " (('N+N', 'time collection'), 1),\n",
       " (('N+N', 'data speakers'), 1),\n",
       " (('ADJ+N', 'actual trend'), 1),\n",
       " (('N', 'trend'), 1),\n",
       " (('ADJ+N+N', 'actual trend capture'), 1),\n",
       " (('N+N', 'trend capture'), 1),\n",
       " (('N+N+N', 'trend capture diversity'), 1),\n",
       " (('N+N', 'capture diversity'), 1),\n",
       " (('N', 'diversity'), 1),\n",
       " (('N+N+N', 'capture diversity humans'), 1),\n",
       " (('N+N', 'diversity humans'), 1),\n",
       " (('N', 'humans'), 1),\n",
       " (('ADJ+N', 'real life'), 1),\n",
       " (('N', 'life'), 1),\n",
       " (('ADJ+N+N', 'real life conditions'), 1),\n",
       " (('N+N', 'life conditions'), 1),\n",
       " (('N', 'conditions'), 1),\n",
       " (('N+N+N', 'life conditions order'), 1),\n",
       " (('N+N', 'conditions order'), 1),\n",
       " (('N', 'order'), 1),\n",
       " (('N+N+N', 'conditions order model'), 1),\n",
       " (('N+N', 'order model'), 1),\n",
       " (('ADJ+N', 'social aspects'), 1),\n",
       " (('N', 'aspects'), 1),\n",
       " (('N+N', 'interactions laughter'), 1),\n",
       " (('N', 'laughter'), 1),\n",
       " (('N+N+N', 'interactions laughter disfluencies'), 1),\n",
       " (('N+N', 'laughter disfluencies'), 1),\n",
       " (('N', 'disfluencies'), 1),\n",
       " (('ADJ+N', 'spontaneous speech'), 1),\n",
       " (('ADJ+N+N', 'spontaneous speech advantage'), 1),\n",
       " (('N+N', 'speech advantage'), 1),\n",
       " (('N', 'benchmark'), 1),\n",
       " (('N+N', 'benchmark community'), 1),\n",
       " (('ADJ+N', 'Linguistic cues'), 1),\n",
       " (('N', 'words'), 1),\n",
       " (('N+N', 'speaker paralinguistic'), 1),\n",
       " (('N', 'paralinguistic'), 1),\n",
       " (('N+N+N', 'speaker paralinguistic cues'), 1),\n",
       " (('N+N', 'paralinguistic cues'), 1),\n",
       " (('ADJ+N', 'acoustic content'), 1),\n",
       " (('ADJ+N+N', 'acoustic content speech'), 1),\n",
       " (('N+N', 'content speech'), 1),\n",
       " (('ADJ+N', 'prosodic features'), 1),\n",
       " (('ADJ+N', 'important cues'), 1),\n",
       " (('ADJ+N+N', 'important cues field'), 1),\n",
       " (('N+N', 'cues field'), 1),\n",
       " (('N', 'field'), 1),\n",
       " (('N+N+N', 'cues field speech'), 1),\n",
       " (('N+N', 'field speech'), 1),\n",
       " (('N+N+N', 'field speech emotion'), 1),\n",
       " (('N+N+N', 'emotion recognition SER'), 1),\n",
       " (('N+N', 'recognition SER'), 1),\n",
       " (('N+N', 'SER systems'), 1),\n",
       " (('N+N', 'capture prosody'), 1),\n",
       " (('N', 'prosody'), 1),\n",
       " (('N+N+N', 'capture prosody input'), 1),\n",
       " (('N+N', 'prosody input'), 1),\n",
       " (('N', 'option'), 1),\n",
       " (('ADJ+N', 'cepstral coefficients'), 1),\n",
       " (('N', 'coefficients'), 1),\n",
       " (('N', 'association'), 1),\n",
       " (('N', 'inventory'), 1),\n",
       " (('ADJ+N', 'acoustic features'), 1),\n",
       " (('ADJ+N+N', 'acoustic features CEICES'), 1),\n",
       " (('N+N', 'features CEICES'), 1),\n",
       " (('N+N+N', 'features CEICES initiative'), 1),\n",
       " (('N+N', 'CEICES initiative'), 1),\n",
       " (('N+N+N', 'CEICES initiative conducts'), 1),\n",
       " (('N+N', 'initiative conducts'), 1),\n",
       " (('N', 'conducts'), 1),\n",
       " (('N', 'descriptors'), 1),\n",
       " (('ADJ+N', 'several corpora'), 1),\n",
       " (('N', 'corpora'), 1),\n",
       " (('ADJ+N', 'various techniques'), 1),\n",
       " (('N+N', 'features advantage'), 1),\n",
       " (('N', 'extraction'), 1),\n",
       " (('N', 'signals'), 1),\n",
       " (('N+N', 'signals error'), 1),\n",
       " (('N', 'error'), 1),\n",
       " (('N+N', 'use input'), 1),\n",
       " (('N+N', 'features embeddings'), 1),\n",
       " (('N', 'embeddings'), 1),\n",
       " (('ADJ+N', 'neural models'), 1),\n",
       " (('N+N', 'speech processing'), 1),\n",
       " (('N', 'processing'), 1),\n",
       " (('N+N+N', 'speech processing tasks'), 1),\n",
       " (('N+N', 'processing tasks'), 1),\n",
       " (('N', 'tasks'), 1),\n",
       " (('ADJ+N', 'different SER'), 1),\n",
       " (('N+N', 'advantage approach'), 1),\n",
       " (('ADJ+N', 'large amount'), 1),\n",
       " (('ADJ+N+N', 'large amount data'), 1),\n",
       " (('ADJ+N', 'different task'), 1),\n",
       " (('N', 'problem'), 1),\n",
       " (('N+N', 'problem privacy'), 1),\n",
       " (('N+N+N', 'problem privacy preservation'), 1),\n",
       " (('N+N+N', 'privacy preservation context'), 1),\n",
       " (('N+N', 'preservation context'), 1),\n",
       " (('N', 'context'), 1),\n",
       " (('ADJ+N+N', 'emotional speech authors'), 1),\n",
       " (('N+N', 'speech authors'), 1),\n",
       " (('N', 'distance'), 1),\n",
       " (('N', 'hashing'), 1),\n",
       " (('N+N', 'hashing techniques'), 1),\n",
       " (('ADJ+N', 'homomorphic encryption'), 1),\n",
       " (('N', 'encryption'), 1),\n",
       " (('ADJ+N', 'sensitive data'), 1),\n",
       " (('ADJ+N+N', 'sensitive data emotions'), 1),\n",
       " (('N+N', 'data emotions'), 1),\n",
       " (('ADJ+N', 'adversarial networks'), 1),\n",
       " (('ADJ+N', 'intermediate layer'), 1),\n",
       " (('N', 'layer'), 1),\n",
       " (('ADJ+N+N', 'intermediate layer users'), 1),\n",
       " (('N+N', 'layer users'), 1),\n",
       " (('N', 'users'), 1),\n",
       " (('N', 'services'), 1),\n",
       " (('N+N+N', 'input speech aim'), 1),\n",
       " (('N+N', 'speech aim'), 1),\n",
       " (('N+N+N', 'speaker identity impacts'), 1),\n",
       " (('N+N', 'identity impacts'), 1),\n",
       " (('N', 'impacts'), 1),\n",
       " (('N+N+N', 'identity impacts SER'), 1),\n",
       " (('N+N', 'impacts SER'), 1),\n",
       " (('N+N+N', 'impacts SER performance'), 1),\n",
       " (('N+N', 'SER performance'), 1),\n",
       " (('N+N', 'anonymization framework'), 1),\n",
       " (('N+N+N', 'baseline system details'), 1),\n",
       " (('N+N', 'system details'), 1),\n",
       " (('N+N+N', 'system details transformation'), 1),\n",
       " (('N+N', 'details transformation'), 1),\n",
       " (('N+N+N', 'details transformation enhancements'), 1),\n",
       " (('N+N', 'transformation enhancements'), 1),\n",
       " (('N', 'enhancements'), 1),\n",
       " (('N+N+N', 'attack scenarios Section'), 1),\n",
       " (('N+N', 'scenarios Section'), 1),\n",
       " (('N', 'Section'), 1),\n",
       " (('N+N+N', 'scenarios Section details'), 1),\n",
       " (('N+N', 'Section details'), 1),\n",
       " (('N+N+N', 'Section details experiments'), 1),\n",
       " (('N+N', 'details experiments'), 1),\n",
       " (('N+N+N', 'evaluation protocol respect'), 1),\n",
       " (('N+N', 'protocol respect'), 1),\n",
       " (('N+N+N', 'protocol respect emotions'), 1),\n",
       " (('N+N', 'respect emotions'), 1),\n",
       " (('N+N', 'results conclusion'), 1),\n",
       " (('N', 'conclusion'), 1),\n",
       " (('N+N', 'paper Anonymization'), 1),\n",
       " (('N', 'Anonymization'), 1),\n",
       " (('N+N+N', 'paper Anonymization framework'), 1),\n",
       " (('N+N', 'Anonymization framework'), 1),\n",
       " (('N+N+N', 'Anonymization framework fact'), 1),\n",
       " (('N+N', 'framework fact'), 1),\n",
       " (('N', 'fact'), 1),\n",
       " (('N+N+N', 'framework fact emotions'), 1),\n",
       " (('N+N', 'fact emotions'), 1),\n",
       " (('N', 'intonation'), 1),\n",
       " (('N+N', 'intonation propose'), 1),\n",
       " (('N', 'propose'), 1),\n",
       " (('N+N', 'values source'), 1),\n",
       " (('N+N+N', 'values source utterance'), 1),\n",
       " (('N+N', 'source utterance'), 1),\n",
       " (('N+N', 'speaker module'), 1),\n",
       " (('N+N+N', 'linear transformation method'), 1),\n",
       " (('N+N', 'transformation method'), 1),\n",
       " (('ADJ+N', 'synthesizer values'), 1),\n",
       " (('ADJ+N', 'close pseudo'), 1),\n",
       " (('ADJ+N+N', 'close pseudo speaker'), 1),\n",
       " (('N+N+N', 'features source speaker'), 1),\n",
       " (('ADJ+N+N', 'linear transformation log'), 1),\n",
       " (('N+N', 'transformation log'), 1),\n",
       " (('ADJ+N', 'Phonetic features'), 1),\n",
       " (('ADJ+N+N', 'Phonetic features extractor'), 1),\n",
       " (('N+N', 'features extractor'), 1),\n",
       " (('N+N+N', 'features extractor Input'), 1),\n",
       " (('N+N', 'extractor Input'), 1),\n",
       " (('N', 'Input'), 1),\n",
       " (('N+N+N', 'extractor Input speech'), 1),\n",
       " (('N+N', 'Input speech'), 1),\n",
       " (('N+N', 'vector extractor'), 1),\n",
       " (('N+N+N', 'vector extractor pseudo'), 1),\n",
       " (('N+N', 'extractor pseudo'), 1),\n",
       " (('N+N+N', 'extractor pseudo speaker'), 1),\n",
       " (('N+N', 'vector selection'), 1),\n",
       " (('ADJ+N+N', 'standard deviation log'), 1),\n",
       " (('N+N', 'deviation log'), 1),\n",
       " (('ADJ+N', 'statistical calculation'), 1),\n",
       " (('N', 'calculation'), 1),\n",
       " (('ADJ+N+N', 'standard deviation target'), 1),\n",
       " (('N+N', 'deviation target'), 1),\n",
       " (('N+N+N', 'deviation target pseudospeaker'), 1),\n",
       " (('N+N', 'target pseudospeaker'), 1),\n",
       " (('N', 'pseudospeaker'), 1),\n",
       " (('N+N', 'frames speakers'), 1),\n",
       " (('ADJ+N', 'derive pseudo'), 1),\n",
       " (('ADJ+N+N', 'derive pseudo speaker'), 1),\n",
       " (('N+N+N', 'anonymization system log'), 1),\n",
       " (('N+N', 'system log'), 1),\n",
       " (('ADJ+N', 'Anonymized speech'), 1),\n",
       " (('N+N', 'vectors statistics'), 1),\n",
       " (('N+N+N', 'speaker anonymization pipeline'), 1),\n",
       " (('N+N', 'anonymization pipeline'), 1),\n",
       " (('N', 'pipeline'), 1),\n",
       " (('N', 'parts'), 1),\n",
       " (('N+N', 'parts baseline'), 1),\n",
       " (('N+N+N', 'parts baseline model'), 1),\n",
       " (('N+N', 'baseline model'), 1),\n",
       " (('N', 'enhancement'), 1),\n",
       " (('N+N', 'transform values'), 1),\n",
       " (('ADJ+N+N', 'linguistic content input'), 1),\n",
       " (('N+N', 'content input'), 1),\n",
       " (('N+N+N', 'content input speech'), 1),\n",
       " (('N+N+N', 'input speech utterance'), 1),\n",
       " (('N', 'altering'), 1),\n",
       " (('ADJ+N', 'encode speaker'), 1),\n",
       " (('ADJ+N+N', 'encode speaker identity'), 1),\n",
       " (('N', 'groups'), 1),\n",
       " (('ADJ+N', 'different features'), 1),\n",
       " (('ADJ+N+N', 'different features source'), 1),\n",
       " (('ADJ+N', 'fundamental frequency'), 1),\n",
       " (('N+N', 'articulation speech'), 1),\n",
       " (('ADJ+N', 'new pseudo'), 1),\n",
       " (('ADJ+N+N', 'new pseudo speaker'), 1),\n",
       " (('N+N+N', 'target vector identity'), 1),\n",
       " (('N+N', 'vector source'), 1),\n",
       " (('N+N+N', 'vector source input'), 1),\n",
       " (('N+N', 'source input'), 1),\n",
       " (('N+N+N', 'source input speaker'), 1),\n",
       " (('N+N', 'input speaker'), 1),\n",
       " (('ADJ+N', 'external vectors'), 1),\n",
       " (('ADJ+N', 'furthest vectors'), 1),\n",
       " (('N+N+N', 'speaker vector identity'), 1),\n",
       " (('N+N+N', 'speech waveform target'), 1),\n",
       " (('N+N', 'waveform target'), 1),\n",
       " (('N+N+N', 'waveform target vector'), 1),\n",
       " (('N+N', 'selection pseudo'), 1),\n",
       " (('N+N+N', 'selection pseudo speaker'), 1),\n",
       " (('ADJ+N', 'triphone extractor'), 1),\n",
       " (('N', 'trainclean'), 1),\n",
       " (('N+N', 'trainclean train'), 1),\n",
       " (('N+N+N', 'trainclean train subsets'), 1),\n",
       " (('N+N', 'train subsets'), 1),\n",
       " (('N', 'subsets'), 1),\n",
       " (('N', 'xvector'), 1),\n",
       " (('N+N', 'xvector extractor'), 1),\n",
       " (('N+N', 'speech synthesis'), 1),\n",
       " (('N+N+N', 'speech synthesis system'), 1),\n",
       " (('N+N', 'synthesis system'), 1),\n",
       " (('ADJ+N', 'clean subset'), 1),\n",
       " (('N', 'pool'), 1),\n",
       " (('N+N', 'pool vector'), 1),\n",
       " (('N+N+N', 'pool vector statistics'), 1),\n",
       " (('N+N', 'vector statistics'), 1),\n",
       " (('N', 'transformations'), 1),\n",
       " (('ADJ+N', 'original VPC'), 1),\n",
       " (('ADJ+N+N', 'original VPC anonymization'), 1),\n",
       " (('N+N', 'VPC anonymization'), 1),\n",
       " (('N+N+N', 'VPC anonymization system'), 1),\n",
       " (('N+N+N', 'anonymization system values'), 1),\n",
       " (('N+N', 'system values'), 1),\n",
       " (('N+N', 'source speech'), 1),\n",
       " (('ADJ+N', 'unchanged speech'), 1),\n",
       " (('ADJ+N+N', 'unchanged speech synthesizer'), 1),\n",
       " (('N+N', 'speech synthesizer'), 1),\n",
       " (('N', 'synthesizer'), 1),\n",
       " (('ADJ+N', 'different pseudo'), 1),\n",
       " (('ADJ+N+N', 'different pseudo speaker'), 1),\n",
       " (('ADJ+N', 'Multiple studies'), 1),\n",
       " (('N', 'modifying'), 1),\n",
       " (('ADJ+N', 'Motivated results'), 1),\n",
       " (('N+N', 'method contour'), 1),\n",
       " (('N+N+N', 'method contour values'), 1),\n",
       " (('N+N', 'contour values'), 1),\n",
       " (('N', 'increase'), 1),\n",
       " (('N+N', 'increase decrease'), 1),\n",
       " (('N', 'decrease'), 1),\n",
       " (('N+N+N', 'increase decrease range'), 1),\n",
       " (('N+N', 'decrease range'), 1),\n",
       " (('N+N+N', 'decrease range variation'), 1),\n",
       " (('N+N', 'range variation'), 1),\n",
       " (('N', 'variation'), 1),\n",
       " (('ADJ+N', 'uniform distribution'), 1),\n",
       " (('N', 'value'), 1),\n",
       " (('N+N', 'value frame'), 1),\n",
       " (('N', 'frame'), 1),\n",
       " (('ADJ+N', 'mean utterance'), 1),\n",
       " (('ADJ+N+N', 'mean utterance transform'), 1),\n",
       " (('N+N', 'utterance transform'), 1),\n",
       " (('ADJ+N', 'multiple sets'), 1),\n",
       " (('N', 'sets'), 1),\n",
       " (('ADJ+N+N', 'multiple sets tests'), 1),\n",
       " (('N+N', 'sets tests'), 1),\n",
       " (('N', 'tests'), 1),\n",
       " (('N+N', 'attacker knowledge'), 1),\n",
       " (('N+N+N', 'attacker knowledge anonymization'), 1),\n",
       " (('ADJ+N+N', 'Ignorant scenario attacker'), 1),\n",
       " (('N+N', 'scenario attacker'), 1),\n",
       " (('ADJ+N', 'unaware speech'), 1),\n",
       " (('N+N', 'privacy measurement'), 1),\n",
       " (('N+N', 'data evaluation'), 1),\n",
       " (('N', 'mismatch'), 1),\n",
       " (('ADJ+N', 'good anonymization'), 1),\n",
       " (('ADJ+N+N', 'good anonymization performance'), 1),\n",
       " (('N+N', 'anonymization performance'), 1),\n",
       " (('ADJ+N', 'aware anonymization'), 1),\n",
       " (('ADJ+N+N', 'aware anonymization algorithm'), 1),\n",
       " (('N+N', 'anonymization algorithm'), 1),\n",
       " (('N', 'algorithm'), 1),\n",
       " (('ADJ+N', 'Such attackers'), 1),\n",
       " (('ADJ+N', 'able anonymize'), 1),\n",
       " (('ADJ+N+N', 'able anonymize training'), 1),\n",
       " (('N+N', 'anonymize training'), 1),\n",
       " (('N+N+N', 'anonymize training dataset'), 1),\n",
       " (('N+N', 'training dataset'), 1),\n",
       " (('N+N+N', 'training dataset manner'), 1),\n",
       " (('N+N', 'dataset manner'), 1),\n",
       " (('N+N+N', 'dataset manner service'), 1),\n",
       " (('N+N', 'manner service'), 1),\n",
       " (('N+N+N', 'manner service provider'), 1),\n",
       " (('N+N', 'service provider'), 1),\n",
       " (('N', 'provider'), 1),\n",
       " (('N+N', 'train evaluation'), 1),\n",
       " (('N+N+N', 'train evaluation model'), 1),\n",
       " (('N+N', 'evaluation model'), 1),\n",
       " (('N+N+N', 'evaluation model Experiments'), 1),\n",
       " (('N+N', 'model Experiments'), 1),\n",
       " (('N', 'Experiments'), 1),\n",
       " (('ADJ+N', 'global aim'), 1),\n",
       " (('N+N+N', 'recognition performance speaker'), 1),\n",
       " (('N+N', 'performance speaker'), 1),\n",
       " (('N+N+N', 'performance speaker anonymization'), 1),\n",
       " (('N', 'voices'), 1),\n",
       " (('N', 'sections'), 1),\n",
       " (('ADJ+N', 'dataset evaluation'), 1),\n",
       " (('ADJ+N+N', 'dataset evaluation protocol'), 1),\n",
       " (('N+N', 'evaluation results'), 1),\n",
       " (('ADJ+N', 'emotional dataset'), 1),\n",
       " (('N', 'experiment'), 1),\n",
       " (('N+N', 'experiment purposes'), 1),\n",
       " (('N+N+N', 'experiment purposes emotion'), 1),\n",
       " (('N+N', 'purposes emotion'), 1),\n",
       " (('N+N+N', 'purposes emotion recognition'), 1),\n",
       " (('N+N+N', 'emotion recognition speech'), 1),\n",
       " (('N+N', 'recognition speech'), 1),\n",
       " (('N+N+N', 'recognition speech Table'), 1),\n",
       " (('N+N', 'speech Table'), 1),\n",
       " (('N+N', 'results VPC'), 1),\n",
       " (('ADJ+N', 'first line'), 1),\n",
       " (('ADJ+N', 'second line'), 1),\n",
       " (('ADJ+N', 'corresponding results'), 1),\n",
       " (('ADJ+N+N', 'corresponding results speech'), 1),\n",
       " (('N+N', 'results speech'), 1),\n",
       " (('ADJ+N', 'anonymized speech'), 1),\n",
       " (('ADJ+N+N', 'Informed attacker scenario'), 1),\n",
       " (('ADJ+N+N', 'Original speech data'), 1),\n",
       " (('ADJ+N', 'Original degradation'), 1),\n",
       " (('ADJ+N+N', 'Original degradation degradation'), 1),\n",
       " (('N+N+N', 'degradation degradation degradation'), 1),\n",
       " (('ADJ+N', 'Original distribution'), 1),\n",
       " (('ADJ+N+N', 'Original distribution emotion'), 1),\n",
       " (('N+N', 'distribution emotion'), 1),\n",
       " (('N+N+N', 'distribution emotion categories'), 1),\n",
       " (('N+N', 'respect number'), 1),\n",
       " (('ADJ+N', 'visual data'), 1),\n",
       " (('ADJ+N', 'scripted dialogues'), 1),\n",
       " (('N', 'dialogues'), 1),\n",
       " (('ADJ+N', 'male actors'), 1),\n",
       " (('N', 'actors'), 1),\n",
       " (('ADJ+N', 'English language'), 1),\n",
       " (('N', 'language'), 1),\n",
       " (('ADJ+N', 'Directional microphones'), 1),\n",
       " (('N+N', 'capture speaker'), 1),\n",
       " (('N+N+N', 'capture speaker speech'), 1),\n",
       " (('ADJ+N', 'audio files'), 1),\n",
       " (('N', 'files'), 1),\n",
       " (('ADJ+N', 'closest speaker'), 1),\n",
       " (('ADJ+N', 'dominant speech'), 1),\n",
       " (('N', 'reference'), 1),\n",
       " (('N+N', 'reference transcriptions'), 1),\n",
       " (('N', 'transcriptions'), 1),\n",
       " (('ADJ+N', 'directional microphones'), 1),\n",
       " (('ADJ+N', 'lower level'), 1),\n",
       " (('ADJ+N+N', 'dominant speaker voice'), 1),\n",
       " (('N+N', 'speaker voice'), 1),\n",
       " (('N', 'turn'), 1),\n",
       " (('ADJ+N', 'human annotators'), 1),\n",
       " (('N', 'majority'), 1),\n",
       " (('N+N', 'majority annotators'), 1),\n",
       " (('N', 'Figure'), 1),\n",
       " (('ADJ+N', 'emotional labels'), 1),\n",
       " (('N', 'labels'), 1),\n",
       " (('ADJ+N', 'previous works'), 1),\n",
       " (('N', 'works'), 1),\n",
       " (('ADJ+N', 'neutral frustration'), 1),\n",
       " (('N', 'frustration'), 1),\n",
       " (('ADJ+N+N', 'neutral frustration sadness'), 1),\n",
       " (('N+N', 'frustration sadness'), 1),\n",
       " (('N', 'sadness'), 1),\n",
       " (('N+N+N', 'frustration sadness anger'), 1),\n",
       " (('N+N', 'sadness anger'), 1),\n",
       " (('N', 'anger'), 1),\n",
       " (('N+N+N', 'sadness anger happiness'), 1),\n",
       " (('N+N', 'anger happiness'), 1),\n",
       " (('N+N+N', 'anger happiness Happiness'), 1),\n",
       " (('N+N', 'happiness Happiness'), 1),\n",
       " (('N', 'Happiness'), 1),\n",
       " (('ADJ+N', 'original annotations'), 1),\n",
       " (('N', 'annotations'), 1),\n",
       " (('ADJ+N+N', 'original annotations happiness'), 1),\n",
       " (('N+N', 'annotations happiness'), 1),\n",
       " (('N+N+N', 'annotations happiness excitement'), 1),\n",
       " (('N+N', 'happiness excitement'), 1),\n",
       " (('N', 'excitement'), 1),\n",
       " (('N+N+N', 'happiness excitement balance'), 1),\n",
       " (('N+N', 'excitement balance'), 1),\n",
       " (('N', 'balance'), 1),\n",
       " (('N+N+N', 'excitement balance number'), 1),\n",
       " (('N+N', 'balance number'), 1),\n",
       " (('N+N', 'emotion class'), 1),\n",
       " (('N+N', 'protocol paper'), 1),\n",
       " (('N+N+N', 'protocol paper focus'), 1),\n",
       " (('N+N', 'paper focus'), 1),\n",
       " (('N', 'focus'), 1),\n",
       " (('ADJ+N', 'original utterances'), 1),\n",
       " (('N', 'evaluations'), 1),\n",
       " (('N+N', 'model SER'), 1),\n",
       " (('N+N+N', 'model SER literature'), 1),\n",
       " (('N+N', 'SER literature'), 1),\n",
       " (('N', 'literature'), 1),\n",
       " (('ADJ+N', 'radial basis'), 1),\n",
       " (('N', 'basis'), 1),\n",
       " (('ADJ+N+N', 'radial basis function'), 1),\n",
       " (('N+N', 'basis function'), 1),\n",
       " (('N', 'function'), 1),\n",
       " (('N+N', 'baseline input'), 1),\n",
       " (('N+N+N', 'baseline input features'), 1),\n",
       " (('ADJ+N', 'efficient representation'), 1),\n",
       " (('ADJ+N+N', 'efficient representation emotion'), 1),\n",
       " (('N+N', 'representation emotion'), 1),\n",
       " (('ADJ+N', 'MFCCs input'), 1),\n",
       " (('ADJ+N+N', 'MFCCs input results'), 1),\n",
       " (('N+N', 'input results'), 1),\n",
       " (('N+N+N', 'attack scenarios emotion'), 1),\n",
       " (('N+N', 'scenarios emotion'), 1),\n",
       " (('N+N+N', 'scenarios emotion recognition'), 1),\n",
       " (('ADJ+N', 'Informed scenario'), 1),\n",
       " (('ADJ+N', 'metric score'), 1),\n",
       " (('N', 'measure'), 1),\n",
       " (('N+N', 'measure emotion'), 1),\n",
       " (('N+N+N', 'measure emotion recognition'), 1),\n",
       " (('ADJ+N', 'good emotion'), 1),\n",
       " (('ADJ+N+N', 'good emotion recognition'), 1),\n",
       " (('ADJ+N', 'Regardless attack'), 1),\n",
       " (('ADJ+N+N', 'Regardless attack scenario'), 1),\n",
       " (('N+N', 'attack scenario'), 1),\n",
       " (('ADJ+N', 'dataset training'), 1),\n",
       " (('ADJ+N+N', 'dataset training evaluation'), 1),\n",
       " (('N+N', 'training evaluation'), 1),\n",
       " (('N', 'session'), 1),\n",
       " (('N+N', 'session cross'), 1),\n",
       " (('N+N+N', 'session cross validation'), 1),\n",
       " (('N+N', 'cross validation'), 1),\n",
       " (('N', 'validation'), 1),\n",
       " (('N+N+N', 'cross validation protocol'), 1),\n",
       " (('N+N', 'validation protocol'), 1),\n",
       " (('ADJ+N', 'global performance'), 1),\n",
       " (('N', 'test'), 1),\n",
       " (('N+N', 'test folds'), 1),\n",
       " (('N', 'folds'), 1),\n",
       " (('N', 'organizers'), 1),\n",
       " (('N+N', 'organizers Results'), 1),\n",
       " (('ADJ+N', 'intelligible speech'), 1),\n",
       " (('ADJ+N+N', 'intelligible speech Evaluation'), 1),\n",
       " (('N+N', 'speech Evaluation'), 1),\n",
       " (('N', 'Evaluation'), 1),\n",
       " (('N+N+N', 'speech Evaluation results'), 1),\n",
       " (('N+N', 'Evaluation results'), 1),\n",
       " (('ADJ+N', 'clean data'), 1),\n",
       " (('N+N+N', 'speech data comparison'), 1),\n",
       " (('N+N', 'data comparison'), 1),\n",
       " (('N+N+N', 'data comparison case'), 1),\n",
       " (('N+N', 'comparison case'), 1),\n",
       " (('N+N+N', 'comparison case ASR'), 1),\n",
       " (('N+N', 'case ASR'), 1),\n",
       " (('N', 'ASR'), 1),\n",
       " (('N+N+N', 'case ASR model'), 1),\n",
       " (('N+N', 'ASR model'), 1),\n",
       " (('ADJ+N+N', 'original data Results'), 1),\n",
       " (('N+N', 'data Results'), 1),\n",
       " (('ADJ+N', 'experimental results'), 1),\n",
       " (('ADJ+N+N', 'experimental results evaluation'), 1),\n",
       " (('N+N', 'results evaluation'), 1),\n",
       " (('N+N+N', 'results evaluation protocol'), 1),\n",
       " (('N+N', 'scores VPC'), 1),\n",
       " (('N+N+N', 'scores VPC post'), 1),\n",
       " (('N+N', 'VPC post'), 1),\n",
       " (('N', 'post'), 1),\n",
       " (('N+N+N', 'VPC post evaluation'), 1),\n",
       " (('N+N', 'post evaluation'), 1),\n",
       " (('N+N+N', 'post evaluation analysis'), 1),\n",
       " (('N+N', 'utility score'), 1),\n",
       " (('ADJ+N', 'relative degradation'), 1),\n",
       " (('ADJ+N', 'Similar behavior'), 1),\n",
       " (('N', 'behavior'), 1),\n",
       " (('N', 'meaning'), 1),\n",
       " (('ADJ+N', 'baseline anonymization'), 1),\n",
       " (('ADJ+N+N', 'baseline anonymization system'), 1),\n",
       " (('N+N+N', 'confidence interval linear'), 1),\n",
       " (('N+N', 'interval linear'), 1),\n",
       " (('N+N+N', 'interval linear transformation'), 1),\n",
       " (('ADJ+N', 'Original data'), 1),\n",
       " (('N', 'classifier'), 1),\n",
       " (('N+N+N', 'recognition performance future'), 1),\n",
       " (('N+N', 'performance future'), 1),\n",
       " (('N', 'future'), 1),\n",
       " (('N', 'percentage'), 1),\n",
       " (('N+N', 'percentage intervals'), 1),\n",
       " (('N', 'intervals'), 1),\n",
       " (('N+N', 'performance IEMOCAP'), 1),\n",
       " (('N+N+N', 'performance IEMOCAP respect'), 1),\n",
       " (('N+N', 'IEMOCAP respect'), 1),\n",
       " (('N+N+N', 'IEMOCAP respect amount'), 1),\n",
       " (('N+N', 'respect amount'), 1),\n",
       " (('N+N+N', 'respect amount overlap'), 1),\n",
       " (('N+N', 'amount overlap'), 1),\n",
       " (('N', 'overlap'), 1),\n",
       " (('N+N+N', 'amount overlap speech'), 1),\n",
       " (('N+N', 'overlap speech'), 1),\n",
       " (('ADJ+N', 'random guessing'), 1),\n",
       " (('N', 'guessing'), 1),\n",
       " (('N+N+N', 'recognition performance terms'), 1),\n",
       " (('N+N', 'performance terms'), 1),\n",
       " (('N', 'terms'), 1),\n",
       " (('N+N', 'manner utility'), 1),\n",
       " (('N+N+N', 'manner utility performance'), 1),\n",
       " (('N+N', 'utility performance'), 1),\n",
       " (('N+N', 'speech modification'), 1),\n",
       " (('N+N+N', 'speech modification prosodic'), 1),\n",
       " (('N+N', 'modification prosodic'), 1),\n",
       " (('N', 'prosodic'), 1),\n",
       " (('N+N+N', 'modification prosodic parameters'), 1),\n",
       " (('N+N', 'prosodic parameters'), 1),\n",
       " (('N', 'Note'), 1),\n",
       " (('ADJ+N', 'mean variability'), 1),\n",
       " (('N', 'variability'), 1),\n",
       " (('ADJ+N+N', 'mean variability range'), 1),\n",
       " (('N+N', 'variability range'), 1),\n",
       " (('ADJ+N', 'experimental setup'), 1),\n",
       " (('N', 'setup'), 1),\n",
       " (('ADJ+N', 'available anonymization'), 1),\n",
       " (('ADJ+N+N', 'available anonymization baseline'), 1),\n",
       " (('ADJ+N', 'random warping'), 1),\n",
       " (('N', 'warping'), 1),\n",
       " (('ADJ+N', 'similar results'), 1),\n",
       " (('N+N+N', 'attacker scenario paper'), 1),\n",
       " (('N+N', 'scenario paper'), 1),\n",
       " (('N', 'application'), 1),\n",
       " (('N+N+N', 'speech recognition performance'), 1),\n",
       " (('ADJ+N+N', 'emotional data IEMOCAP'), 1),\n",
       " (('N+N', 'data IEMOCAP'), 1),\n",
       " (('ADJ+N', 'neutral data'), 1),\n",
       " (('N', 'concerns'), 1),\n",
       " (('N+N', 'concerns emotion'), 1),\n",
       " (('N+N+N', 'concerns emotion information'), 1),\n",
       " (('ADJ+N', 'fine one'), 1),\n",
       " (('N', 'one'), 1),\n",
       " (('ADJ+N', 'valuable information'), 1),\n",
       " (('ADJ+N+N', 'personal information anonymization'), 1),\n",
       " (('N+N', 'information anonymization'), 1),\n",
       " (('N+N+N', 'information anonymization system'), 1),\n",
       " (('ADJ+N', 'preliminary experiments'), 1),\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_terms = Counter(d)\n",
    "rank_term = total_terms.most_common() #list all\n",
    "# total_terms.most_common()[:200] #top 200\n",
    "rank_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           terms  frequency\n",
      "0                 language model         19\n",
      "1                         tokens         16\n",
      "2                          token         12\n",
      "3                language models         10\n",
      "4                        vectors         10\n",
      "..                           ...        ...\n",
      "171          Attention intention          1\n",
      "172     intention neural network          1\n",
      "173  neural network conversation          1\n",
      "174           conversation model          1\n",
      "175   network conversation model          1\n",
      "\n",
      "[176 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(rank_term, columns=['terms', 'frequency'])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('evaluation_data_before_filtering.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building IOB-tagging Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
